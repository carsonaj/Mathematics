\documentclass[twoside]{article}

\usepackage{aistats2022}
% If your paper is accepted, change the options for the package
% aistats2022 as follows:
%
%\usepackage[accepted]{aistats2022}
%
% This option will print headings for the title of your paper and
% headings for the authors names, plus a copyright note at the end of
% the first column of the first page.

% If you set papersize explicitly, activate the following three lines:
%\special{papersize = 8.5in, 11in}
%\setlength{\pdfpageheight}{11in}
%\setlength{\pdfpagewidth}{8.5in}

% If you use natbib package, activate the following three lines:
%\usepackage[round]{natbib}
%\renewcommand{\bibname}{References}
%\renewcommand{\bibsection}{\subsubsection*{\bibname}}

% If you use BibTeX in apalike style, activate the following line:
%\bibliographystyle{apalike}


\usepackage{amsmath,amsthm,amssymb,setspace, mathtools}
\usepackage{physics}

\usepackage{color}   %May be necessary if you want to color links
\usepackage{hyperref}
\hypersetup{
	colorlinks=true, %set true if you want colored links
	linktoc=all,     %set to all if you want both sections and subsections linked
	linkcolor=black,  %choose some color if you want links to stand out
	urlcolor=cyan
}

\newtheorem{thm}{Theorem}[subsection]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{conj}{Conjecture}
\newtheorem{res}[thm]{Result}
\newtheorem{ques}[thm]{Question}
\newtheorem{ans}[thm]{Answer}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[subsection]
\newtheorem{defn}[definition]{Definition}


\theoremstyle{definition}
\newtheorem{ex}[definition]{Exercise}
\newtheorem{rem}[definition]{Remark}

\newcommand{\al}{\alpha}
\newcommand{\Gam}{\Gamma}
\newcommand{\be}{\beta} 
\newcommand{\del}{\delta} 
\newcommand{\Del}{\Delta}
\newcommand{\lam}{\lambda}  
\newcommand{\Lam}{\Lambda} 
\newcommand{\ep}{\epsilon}
\newcommand{\sig}{\sigma} 
\newcommand{\om}{\omega}
\newcommand{\Om}{\Omega}
\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\T}{\mathbb{T}}
\newcommand{\Q}{\mathbb{Q}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\MA}{\mathcal{A}}
\newcommand{\MC}{\mathcal{C}}
\newcommand{\MB}{\mathcal{B}}
\newcommand{\MF}{\mathcal{F}}
\newcommand{\MG}{\mathcal{G}}
\newcommand{\ML}{\mathcal{L}}
\newcommand{\MN}{\mathcal{N}}
\newcommand{\MS}{\mathcal{S}}
\newcommand{\MP}{\mathcal{P}}
\newcommand{\ME}{\mathcal{E}}
\newcommand{\MT}{\mathcal{T}}
\newcommand{\MM}{\mathcal{M}}
\newcommand{\MI}{\mathcal{I}}

\newcommand{\ui}{[0,1]}
\newcommand{\p}{\partial}

\newcommand{\io}{\text{ i.o.}}
%\newcommand{\ev}{\text{ ev.}}
\renewcommand{\r}{\rangle}
\renewcommand{\l}{\langle}

\newcommand{\RG}{[0,\infty]}
\newcommand{\Rg}{[0,\infty)}
\newcommand{\Ru}{(\infty, \infty]}
\newcommand{\Rd}{[\infty, \infty)}
\newcommand{\Ll}{L^1_{\text{loc}}(\R^n)}

\newcommand{\limfn}{\liminf \limits_{n \rightarrow \infty}}
\newcommand{\limpn}{\limsup \limits_{n \rightarrow \infty}}
\newcommand{\limn}{\lim \limits_{n \rightarrow \infty}}
\newcommand{\convt}[1]{\xrightarrow{\text{#1}}}
\newcommand{\conv}[1]{\xrightarrow{#1}} 
\newcommand{\seq}[2]{(#1_{#2})_{#2 \in \N}}

\newcommand{\lsc}{l.s.c. }

\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\spn}{span}
\DeclareMathOperator{\iso}{Iso}
\DeclareMathOperator{\id}{id}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}







\begin{document}

% If your paper is accepted and the title of your paper is very long,
% the style will print as headings an error message. Use the following
% command to supply a shorter title of your paper so that it can be
% used as headings.
%
%\runningtitle{I use this title instead because the last one was very long}

% If your paper is accepted and the number of authors is large, the
% style will print as headings an error message. Use the following
% command to supply a shorter version of the authors names so that
% they can be used as headings (for example, use only the surnames)
%
%\runningauthor{Surname 1, Surname 2, Surname 3, ...., Surname n}

\twocolumn[

\aistatstitle{Short Application of Optimization on RKHS}

\aistatsauthor{ Carson James }

\aistatsaddress{ Texas A&M University } ]

\begin{abstract}
  This report will give an overview of various tools used to perform gradient descent in the context of reproducing kernel hilbert spaces and will present an application to Gaussian processes. 
\end{abstract}

\section{Banach Spaces}
\subsection{Bounded Linear Maps}
\begin{defn}
		Let $X$ be a normed vector space. Then $X$ is said to be a \textbf{Banach space} if $X$ is complete.  
	\end{defn}
	
	\begin{defn}
		Let $X,Y$ be a normed vector spaces and $T:X \rightarrow Y$ a linear map. Then $T$ is said to be \textbf{bounded} if there exists $C \geq 0$ such that for each $x \in X$, $$\|Tx \|\leq C \|x \|$$ We define $$L(X,Y) = \{T:X \rightarrow Y: T \text{ is linear and bounded}\}$$
	\end{defn}
	
	\begin{defn}
		Let $X_1, \dots, X_n$ and $Y$ be a normed vector spaces and $T:\prod\limits_{j=1}^n X_j \rightarrow Y$ a multilinear linear map. Then $T$ is said to be \textbf{bounded} if there exists $C \geq 0$ such that for each $(x_j)_{j=1}^n \in \prod\limits_{j=1}^n X_j$, $$\|T(x_1, \dots, x_n) \|\leq C \|x_1 \| \dots \|x_n\|$$ 
		We define $L^n(X_1, \dots, X_n; Y)$ to be the set of $T:X \rightarrow Y$ such that $T$ is multilinear and bounded. If $X_1, \dots, X_n = X$, we write $L^n(X,Y)$ in place of  $L^n(X, \dots, X; Y)$.
	\end{defn}
	
	\begin{rem}
	
	Let $X$ and $Y$ be normed vector spaces. We may identify $L(X, L(X, \dots, L(X, Y)) \dots)$ and $L^n(X, Y)$ via the isometric isomorphism given by $\phi \mapsto \psi_{\phi}$ where $$\psi_{\phi}(x_1, x_2, \dots, x_n) = \phi(x_1)(x_2) \dots (x_n)$$ 
	\end{rem}
	
	\begin{defn}
	Let $X$ be a normed vector space over $\R$. We define the \textbf{dual space of} $X$, denoted $X^*$, by $X^* = L(X, \R)$. Let $T: X \rightarrow \R$. Then $T$ is said to be a \textbf{bounded linear functional on} $X$ if $T \in X^*$.
	\end{defn}
	
	\subsection{Differentiation}
	
	\begin{defn}
	Let $X, Y$ be a banach spaces, $A \subset X$ open, $f:A \rightarrow Y$ and $x_0 \in A$. Then $f$ is said to be \textbf{\textbf{($1$-st order) Frechet differentiable} at $x_0$} if there exists $Df(x_0) \in L(X,Y)$ such that, $$f(x_0 + h) = f(x_0) + Df(x_0)(h) + o(\|h\| ) \hspace{.5cm} \text{ as } h \rightarrow 0$$  
	If $f$ is Frechet differentiable at $x_0$, we define the \textbf{Frechet derivative of $f$ at $x_0$} to be $Df(x_0)$.
	We say that $f$ is \textbf{($1$-st order) Frechet differentiable} if for each $x_0 \in A$, $f$ is Frechet differentiable at $x_0$. \\
	If $f$ is Frechet differentiable, we define the \textbf{Frechet derivative} of $f$, denoted $Df: A \rightarrow L(X,Y)$, by $$x \mapsto Df(x)$$
	Continuing inductively, if $f$ is $(n-1)$-th order Frechet differentiable, $f$ is said to be $n$-th order Frechet differentiable at $x_0$ if $D^{n-1}f$ is Frechet differentiable at $x_0$. We define $D^nf(x_0) = D(D^{n-1}f)(x_0)$.
	\end{defn}
	
	\begin{rem}
Note that $D^nf(x_0) \in L^n(X,Y)$. 
\end{rem}

\subsection{Calculus}

\begin{rem}
The tools used to obtain the following results: 

\begin{itemize}
\item Frechet Derivative

\item Bochner Integral

\item Hahn-Banach Theorem  
\end{itemize}
\end{rem}

\begin{res}
Let $X,Y$ be Banach spaces and $f \in L(X,Y)$. Then $f$ is Frechet differentiable and for each $x_0 \in X$, $Df(x_0) = f$. 
\end{res}


\begin{res}
Let $X,Y, Z$ be Banach spaces, $f:X \rightarrow Y$, $g :Y \rightarrow Z$ and $x_0 \in X$. If $f$ is Frechet differentiable at $x_0$ and $g$ is Frechet differentiable at $f(x_0)$, then $g \circ f$ is Frechet differentiable at $x_0$ and $$D(g \circ f)(x_0) = Dg(f(x_0)) \circ Df(x_0)$$
\end{res}


\begin{res}\textbf{Mean Val Thm:}\\
Let $X, Y$ be a Banach spaces, $A \subset X$ open and convex and $f:A \rightarrow Y$. If $f$ is Frechet differentiable, then for each $x,y \in A$, there exists $t \in (0,1)$ such that $$\|f(x) - f(y)\| \leq \|Df(tx + (1-t)y)\|\|x-y\|$$
\end{res}


\begin{res}
Let $X, Y$ be a Banach spaces, $A \subset X$ open and convex and $f:A \rightarrow Y$. Suppose that $f$ is Frechet differentiable. If for each $x \in A$, $Df(x) = 0$, then $f$ is constant.
\end{res}

\begin{res}
Let $X, Y$ be a Banach spaces, $A \subset X$ open and convex and $f,g:A \rightarrow Y$. Suppose that $f$ and $g$ are Frechet differentiable. If $Df = Dg$, then there exists $c \in Y$ such that $f = g+c$.
\end{res}

\begin{res}
Let $X$ be a Banach spaces, $A \subset X$ open, $f:A \rightarrow \R$ and $x_0 \in A$. Suppose that $f$ is Frechet differentiable at $x_0$. If $f$ has a local minimum at $x_0$, then $Df(x_0) = 0$. 
\end{res}
	
\begin{res}\textbf{Fundy Thm of Calc:}\\
Let $Y$ be a separable Banach space and $f \in C^1_Y(a,b)$. Then for each $x, x_0 \in (a,b)$, $x_0 < x$ implies that 
	\begin{enumerate}
	\item $f'$ is Bochner integrable on $(x_0, x]$ 
	\item  $$f(x) - f(x_0) = \int_{(x_0, x]}f'dm$$ 
	\end{enumerate}
\end{res}

\begin{res}\textbf{Taylor's Theorem:}\\
Let $Y$ be a separable Banach space, $A \subset X$ open and convex, $f\in C^n_Y(A)$ and $x_0 \in A$. Then 
\begin{align*}
f(x_0 + h) &= \sum_{k=0}^n \frac{1}{k!} D^k f(x_0)(h, \dots, h) + o(\|h\|^n)\\ \text{ as } h \rightarrow 0
\end{align*}
\end{res}

\section{Hilbert Spaces} 

\subsection{Introduction}

\begin{defn}
Let $H$ be an inner product space. Then $H$ is said to be a \textbf{Hilbert space} if $H$ is complete with respect to the norm induced by the inner product.
\end{defn}


\begin{rem}
We will be assuming the Hilbert space is real. 
\end{rem}


\begin{res}\textbf{Cauchy Schwarz Ineq:}\\
Let $H$ be an inner product space. Then for each $x,y \in H$, $|\l x, y\r| \leq \|x\| \|y\|$ with equality iff $x \in \spn(y)$.
\end{res}

\subsection{Riesz Representation Theorem}

\begin{defn}
Let $H$ be a Hilbert space. Define $\phi:H \rightarrow H^*$ by $x \mapsto x^*$ where $$x^*y = \l x ,y\r$$
\end{defn}

\begin{res}
Let $H$ be a Hilbert space. Then $\phi: H \rightarrow H^*$ defined above is an isometric isomorphism.
\end{res}

\begin{defn}
Let $H$ be a Hilbert space, $f: H \rightarrow \R$ and $x_0 \in H$. Suppose that $f$ is Frechet differentiable at $x_0$ so that $Df(x_0) \in H^*$. We define the \textbf{gradient of $f$ at $x_0$}, denoted $\nabla f(x_0) \in H$, by $$\nabla f(x_0) = \phi^{-1}Df(x_0)$$ That is, $\nabla f(x_0)$ is the unique element of $H$ such that for each $y \in H$, $$\l \nabla f(x_0), y \r = Df(x_0)(y)$$
\end{defn}

\begin{res}
Let $H$ be a Hilbert space, $f:H \rightarrow \R$ and $x_0 \in H$. If $f$ is Frechet differentiable at $x_0$, then 
$$\argmin_{\|h\| \leq 1} Df(x_0)(h) = - \|\nabla f(x_0)\|^{-1} \nabla f(x_0)$$ 
\end{res} 

\begin{rem}
In the context of Hilbert spaces, the gradient allows us generalize the gradient descent method for minimization.

The idea is as follows. If $f:H \rightarrow \R$ is Frechet differentiable. Then $$f(x_0+h) \approx f(x_0) + \l \nabla f(x_0), h \r$$ for $h$ near $0$. Taking $h = - \eta \nabla f(x_0)$ for some small $\eta >0$ insures that $h$ is close to $0$ and $h$ is in the direction of steepest descent of  $Df(x_0)(v)$ which causes $f(x_0+h) < f(x_0)$. 
\end{rem}

\section{Convex Analysis}

\subsection{Results}
\begin{res}
Let $X$ be a vector space, $A \subset X$ convex, $f:A \rightarrow \R$ convex and $x_0 \in A$. Then $f$ has a local minimum at $x_0$ iff $f$ has a global minimum at $x_0$.
\end{res}

\begin{res}
Let $X$ be a vector space, $A \subset X$ convex and $f:A \rightarrow \R$ strictly convex. If $f$ has a local minimum, then there exists a unique $x_0 \in A$ such that $f(x_0) = \min\limits_{x \in A}f(x)$.
\end{res}

\begin{res}
Let $X$ be a Banach space, $A \subset X$ open and convex, $f:A \rightarrow \R$ convex, $x_0 \in A$. Suppose that $f$ is $2$nd order Frechet differentiable. If for each $x_0 \in A$, $D^2f(x_0) \in L^2(X, \R)$ is positive semi definite (resp. pos. def.), then $f$ is convex (resp. strictly convex). 
\end{res}

\begin{rem}
By positive definite, we mean $D^2f(x_0)(h,h) > 0$ for $h \neq 0$.
\end{rem}

\section{Reproducing Kernel Hilbert Spaces}
\subsection{RKHS's}

\begin{defn}
Let $T$ be a set and $H \subset \R^T$ a hilbert space. For $t \in T$, we define the \textbf{evauluation functional at $t$}, denoted $L_t : H \rightarrow \R$, by $$L_t(f) = f(t)$$ 
The space $H$ is said to be a \textbf{reproducing kernel Hilbert space (RKHS)} if for each $t \in T$, $L_t \in H^*$ (i.e. $L_t$ is bounded). \\
If $H$ is an RKHS, the Riesz representation theorem implies that for each $t \in T$, there exists $K_t \in H$ such that for each $f \in H$, $\l K_t, f\r = f(t)$.\\
If $H$ is an RKHS, we define the \textbf{reproducing kernel} associated to $H$, denoted $K_H:T^2 \rightarrow \R$, by $$K_H(s,t) = \l K_s, K_t \r$$ 
\end{defn}

\begin{res}
Let $T$ be a set and $K : T^2 \rightarrow \R$. If $K$ is symmetric and positive definite, then there exists a unique reproducing kernel Hilbert space $H \subset \R^T$ such that $K_H = K$.
\end{res}

\begin{res}\textbf{Representer Theorem:}\\
Let $T$ be a set, $K : T^2 \rightarrow \R$ a symmetric, postivie definite kernel on $T$, $H \subset \R^T$ the corresponding RKHS, $t = (t_j)_{j=1}^n \subset T$ and $y = (y_j)_{j=1}^n \subset \R$. \\
Define $L: H \rightarrow \R$ by $$L(f) = \sum_{j=1}^n (y_j - f(t_j) )^2 + \lam \|f\|^2$$ \\
Put $\hat{f} = \argmin\limits_{f \in H}L(f)$. \\
Then there exist $(\hat{\al}_j)_{j=1}^n \subset \R$ such that $$\hat{f}(t) = \sum_{j=1}^n \hat{\al}_jK(t, t_j)$$
\end{res}

\begin{rem}
Define $A \in \R^{n \times n}$ by $A_{i,j} = K(t_i, t_j)$. Some regular calculus shows that $\hat{\al} = (A + \lam I)^{-1}y$
\end{rem}

\begin{rem}
If $(A + \lam I)^{-1}$ is too expensive to compute, we may try other methods to find a minimum of $L$. The next section explores one such method, namely gradient descent.
\end{rem}

\section{Gradient Descent}
\subsection{Update Derivation}
\begin{rem}
Using the setup from the previous section, define $Q: H \rightarrow \R$ by $$Q(f) = \sum_{j=1}^n (y_j - f(t_j) )^2$$ 
We can write rewrite $Q(f)$ as  $$Q(f) = \|L_t(f) - y\|_2^2$$ where $L_t \in L(H, \R^n)$ is given by $$L_t(f) = (f(t_j))_{j=1}^n$$ 
Writing out $Q(f_0 + h)$, we see that 
\begin{align*}
Q(f_0 + h) 
&= \|L_t(f_0) - y\|_2^2  + 2(L_t(f_0) - y)^T L_t(h) \\
& + \|L_t(h)\|_2^2 \\
&= Q(f_0) + \text{linear funct of $h$} \\
&+  \text{bilinear funct of $(h,h)$}
\end{align*}
Equating terms from Taylors theorem, we see that $D^2Q(f_0)(h,h) = 2\|L_t(h)\|_2^2$, which is p.s.d. So $Q$ is convex. Since norms are convex and $\lam \geq 0$, $L$ is convex. 
\end{rem}

\begin{rem}
Similar to before, writing out $L(f_0 + h)$, we get
\begin{align*}
L(f_0 + h) &= L(f_0) + 2(L_t(f_0) - y)^T L_t(h) + 2 \lam \l f_0 , h\r \\
&+ o(\|h\|^2) 
\end{align*}
So 
\begin{align*}
DL(f_0)(h) 
&= 2(L_t(f_0) - y)^T L_t(h) + 2 \lam \l f_0 , h\r \\
&= 2\sum_{j=1}^n (f_0(t_j)- y_j) \l K_{t_j}, h\r +  2 \lam \l f_0 , h\r \\
&= \bigg \l  2\bigg [ \sum_{j=1}^n (f_0(t_j)- y_j) K_{t_j} + \lam f_0\bigg ] , h \bigg \r 
\end{align*}
Hence $$\nabla L(f_0) = 2\bigg [ \sum_{j=1}^n (f_0(t_j)- y_j) K_{t_j} + \lam f_0\bigg ]$$
Therefore the gradient descent update reads as follows: 
\begin{align*}
f_{t+1} 
&= f_t - \eta \nabla L(f_t) \\
&= (1 - 2 \eta \lam)f_t - 2 \eta \bigg [\sum_{j=1}^n (f_0(t_j)- y_j) K_{t_j} \bigg ]
\end{align*}
\end{rem}


\section{Applications to Gaussian Processes}

\subsection{Introduction}
\begin{rem}
Let $T$ be a set and $x = (x_j)_{j=1}^n \in T^n$, $y = (y_j)_{j=1}^n \in \R^n$. Recall that if 
\begin{align*}
y_i &= f(x_i) + \ep_i \\
\ep_i &\sim N(0, \sig^2) \\
f &\sim GP(0, c)
\end{align*}
Then $$f|x, y \sim GP(\tilde{\mu}, \tilde{c})$$ where $$\tilde{\mu}(t) = c(t, x)[c(x,x) + \sig^2I]^{-1}y$$ and $$\tilde{c}(s,t) = c(s,t) - c(s,x)[c(x,x) + \sig^2 I]^{-1}c(x,t)$$

\end{rem}

\subsection{Posterior Mean and Covariance}
\begin{rem}
If $(c(x,x) + \sig^2I)^{-1}$ is too expensive to compute, we may set up the following convex optimization problems to approximate the posterior mean and posterior covariance functions via our gradient descent algorithm:
\begin{itemize}
\item $$\tilde{\mu}(t) = \argmin_{f \in H} \sum_{j=1}^n (y_j - f(t_j))^2 + \sig^2 \|h\|_H$$ 
\item Fixing $t \in T$, $$\hat{c}(\cdot, t) = \argmin_{f \in H} \sum_{j=1}^n (c(x_j, t) - f(t_j))^2 + \sig^2 \|h\|_H$$
\end{itemize}
where $H$ is the RKHS corresponding to the p.d. kernel $c$. \\
The first optimization problem lets us approximate $\tilde{\mu}$ directly by gradient descent and the second optimization problem lets us approximate $\tilde{c}(t)$ by finding $\hat{c}(\cdot, t)$ via gradient descent and the computing $\tilde{c}(s, t) = c(s, t) - \hat{c}(\cdot, t)$.
\end{rem}

\section*{Acknowledgements}
\begin{rem}
Essentially all the proofs are in the notes on my Github page listed in the references or on Wikipedia. I got the material about Gaussian processes for Dr. Pati's STAT 605 class.  
\end{rem}

\section*{References}
\begin{enumerate}
\item \href{https://github.com/carsonaj/Mathematics/blob/master/Introduction\%20to\%20Analysis/Introduction\%20to\%20Analysis.pdf}{Analysis Notes}

\item \href{https://github.com/carsonaj/Mathematics/blob/master/Introduction\%20to\%20Measure\%20and\%20Integration/Introduction\%20to\%20Measure\%20and\%20Integration.pdf}{Integration Notes}

\item \href{https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space}{RKHS Wikipedia Page}

\item \href{https://en.wikipedia.org/wiki/Representer_theorem}{Representer Theorem Wikipedia Page}
\end{enumerate}


\end{document}
