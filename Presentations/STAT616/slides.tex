\documentclass[notheorems]{beamer}

\usepackage{amsmath,amsthm,amssymb,setspace, mathtools}
\usepackage{physics}

\usepackage{color}   %May be necessary if you want to color links
\usepackage{hyperref}
\hypersetup{
	colorlinks=true, %set true if you want colored links
	linktoc=all,     %set to all if you want both sections and subsections linked
	linkcolor=black,  %choose some color if you want links to stand out
	urlcolor=cyan
}


%
%
%
\newif\ifhideproofs
%\hideproofstrue %uncomment to hide proofs
%
%
%
%
\ifhideproofs
\usepackage{environ}
\NewEnviron{hide}{}
\let\proof\hide
\let\endproof\endhide
\fi

\newtheorem{thm}{Theorem}[subsection]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{conj}{Conjecture}
\newtheorem{res}[thm]{Result}
\newtheorem{ques}[thm]{Question}
\newtheorem{ans}[thm]{Answer}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[subsection]
\newtheorem{defn}[definition]{Definition}


\theoremstyle{definition}
\newtheorem{ex}[definition]{Exercise}
\newtheorem{rem}[definition]{Remark}

\newcommand{\al}{\alpha}
\newcommand{\Gam}{\Gamma}
\newcommand{\be}{\beta} 
\newcommand{\del}{\delta} 
\newcommand{\Del}{\Delta}
\newcommand{\lam}{\lambda}  
\newcommand{\Lam}{\Lambda} 
\newcommand{\ep}{\epsilon}
\newcommand{\sig}{\sigma} 
\newcommand{\om}{\omega}
\newcommand{\Om}{\Omega}
\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\T}{\mathbb{T}}
\newcommand{\Q}{\mathbb{Q}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\MA}{\mathcal{A}}
\newcommand{\MC}{\mathcal{C}}
\newcommand{\MB}{\mathcal{B}}
\newcommand{\MF}{\mathcal{F}}
\newcommand{\MG}{\mathcal{G}}
\newcommand{\ML}{\mathcal{L}}
\newcommand{\MN}{\mathcal{N}}
\newcommand{\MS}{\mathcal{S}}
\newcommand{\MP}{\mathcal{P}}
\newcommand{\ME}{\mathcal{E}}
\newcommand{\MT}{\mathcal{T}}
\newcommand{\MM}{\mathcal{M}}
\newcommand{\MI}{\mathcal{I}}

\newcommand{\ui}{[0,1]}
\newcommand{\p}{\partial}

\newcommand{\io}{\text{ i.o.}}
%\newcommand{\ev}{\text{ ev.}}
\renewcommand{\r}{\rangle}
\renewcommand{\l}{\langle}

\newcommand{\RG}{[0,\infty]}
\newcommand{\Rg}{[0,\infty)}
\newcommand{\Ru}{(\infty, \infty]}
\newcommand{\Rd}{[\infty, \infty)}
\newcommand{\Ll}{L^1_{\text{loc}}(\R^n)}

\newcommand{\limfn}{\liminf \limits_{n \rightarrow \infty}}
\newcommand{\limpn}{\limsup \limits_{n \rightarrow \infty}}
\newcommand{\limn}{\lim \limits_{n \rightarrow \infty}}
\newcommand{\convt}[1]{\xrightarrow{\text{#1}}}
\newcommand{\conv}[1]{\xrightarrow{#1}} 
\newcommand{\seq}[2]{(#1_{#2})_{#2 \in \N}}

\newcommand{\lsc}{l.s.c. }

\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\spn}{span}
\DeclareMathOperator{\iso}{Iso}
\DeclareMathOperator{\id}{id}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}


%Information to be included in the title page:
\title{Gradient Descent in Hilbert Space}
\author{Carson James}

\begin{document}

\frame{\titlepage}

\begin{frame}
\frametitle{Outline}
\tableofcontents
	
\end{frame}

%\begin{enumerate}
%	\item Banach Spaces
%		\begin{itemize}
%		\item bounded linear maps
%		\item Frechet differentiation
%		\end{itemize}
%	\item Calculus 
%		\begin{itemize}
%		\item tools
%		\item results
%		\end{itemize}
%	\item Hilbert Spaces
%		\begin{itemize}
%		\item Riesz representation theorem
%		\item gradients
%		\end{itemize}
%	\item Convex Analysis 
%		\begin{itemize}
%		\item results for Frechet differentiable functions
%		\end{itemize}
%	\item Reproducing Kernel Hilbert Spaces
%		\begin{itemize}
%		\item representer theorem
%		\end{itemize}
%	\item Applications to Gaussian Processes
%	\begin{itemize}
%		\item predictive posterior 
%		\end{itemize}
%	
%	\end{enumerate}

\section{Banach Spaces}
\subsection{Bounded Linear Maps}
\begin{frame}
\frametitle{Banach Spaces}
	
	\begin{defn}
		Let $X$ be a normed vector space. Then $X$ is said to be a \textbf{Banach space} if $X$ is complete.  
	\end{defn}
	
	\pause
	
	\begin{defn}
		Let $X,Y$ be a normed vector spaces and $T:X \rightarrow Y$ a linear map. Then $T$ is said to be \textbf{bounded} if there exists $C \geq 0$ such that for each $x \in X$, $$\|Tx \|\leq C \|x \|$$ We define $$L(X,Y) = \{T:X \rightarrow Y: T \text{ is linear and bounded}\}$$
	\end{defn}
	\end{frame}
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	\begin{frame}
	
	\begin{defn}
		Let $X_1, \dots, X_n$ and $Y$ be a normed vector spaces and $T:\prod\limits_{j=1}^n X_j \rightarrow Y$ a multilinear linear map. Then $T$ is said to be \textbf{bounded} if there exists $C \geq 0$ such that for each $(x_j)_{j=1}^n \in \prod\limits_{j=1}^n X_j$, $$\|T(x_1, \dots, x_n) \|\leq C \|x_1 \| \dots \|x_n\|$$ 
		We define $$L^n(X_1, \dots, X_n; Y) = \{T:X \rightarrow Y: T \text{ is multilinear and bounded}\}$$ 
		If $X_1, \dots, X_n = X$, we write $L^n(X,Y)$ in place of  $L^n(X, \dots, X; Y)$.
	\end{defn}
	
	\end{frame}
	
	
	
	
	
	

	
	
	
	
	
	
	
	
	
	
	
	
	
	\begin{frame}
	
	\begin{rem}
	
	Let $X$ and $Y$ be normed vector spaces. We may identify $L(X, L(X, \dots, L(X, Y)) \dots)$ and $L^n(X, Y)$ via the isometric isomorphism given by $\phi \mapsto \psi_{\phi}$ where $$\psi_{\phi}(x_1, x_2, \dots, x_n) = \phi(x_1)(x_2) \dots (x_n)$$ 
	\end{rem}
	\pause

	\begin{defn}
	Let $X$ be a normed vector space over $\R$. We define the \textbf{dual space of} $X$, denoted $X^*$, by $X^* = L(X, \R)$. Let $T: X \rightarrow \R$. Then $T$ is said to be a \textbf{bounded linear functional on} $X$ if $T \in X^*$.
	\end{defn}
	
\end{frame}
























\subsection{Frechet Differentiation}
\begin{frame}
	
	

	\begin{defn}
	Let $X, Y$ be a banach spaces, $A \subset X$ open, $f:A \rightarrow Y$ and $x_0 \in A$. Then $f$ is said to be \textbf{\textbf{($1$-st order) Frechet differentiable} at $x_0$} if there exists $Df(x_0) \in L(X,Y)$ such that, $$f(x_0 + h) = f(x_0) + Df(x_0)(h) + o(\|h\| ) \hspace{.5cm} \text{ as } h \rightarrow 0$$  
	If $f$ is Frechet differentiable at $x_0$, we define the \textbf{Frechet derivative of $f$ at $x_0$} to be $Df(x_0)$.
	We say that $f$ is \textbf{($1$-st order) Frechet differentiable} if for each $x_0 \in A$, $f$ is Frechet differentiable at $x_0$. \\
	If $f$ is Frechet differentiable, we define the \textbf{Frechet derivative} of $f$, denoted $Df: A \rightarrow L(X,Y)$, by $$x \mapsto Df(x)$$
	Continuing inductively, 
	\end{defn}

\end{frame}











\begin{frame}
\begin{defn}
Let $X, Y$ be a banach spaces, $A \subset X$ open, $f:A \rightarrow Y$. We define \textbf{$n$-th} order Frechet differentiablility inductively. \\
If $f$ is $n-1$-th order Frechet differentiable, $f$ is said to be $n$-th order Frechet differentiable at $x_0$ if $D^{n-1}f$ is Frechet differentiable at $x_0$. We define $D^nf(x_0) = D(D^{n-1}f)(x_0)$.   
\end{defn}
\pause

\begin{rem}
Note that $D^nf(x_0) \in L^n(X,Y)$. 
\end{rem}

\end{frame}







\section{Calculus} 

\subsection{Tools}
\begin{frame}
\frametitle{Calculus}

\begin{rem}
The tools used to obtain the following results: 
\pause
\begin{itemize}
\item Frechet Derivative
\pause
\item Bochner Integral
\pause
\item Hahn-Banach Theorem  
\end{itemize}
\end{rem}
\end{frame}







\subsection{Results}
\begin{frame}


\begin{res}
Let $X,Y$ be Banach spaces and $f \in L(X,Y)$. Then $f$ is Frechet differentiable and for each $x_0 \in X$, $Df(x_0) = f$. 
\end{res}
\pause

\begin{res}
Let $X,Y, Z$ be Banach spaces, $f:X \rightarrow Y$, $g :Y \rightarrow Z$ and $x_0 \in X$. If $f$ is Frechet differentiable at $x_0$ and $g$ is Frechet differentiable at $f(x_0)$, then $g \circ f$ is Frechet differentiable at $x_0$ and $$D(g \circ f)(x_0) = Dg(f(x_0)) \circ Df(x_0)$$
\end{res}
\pause

\begin{res}
Let $X, Y$ be a Banach spaces, $A \subset X$ open and convex and $f:A \rightarrow Y$. If $f$ is Frechet differentiable, then for each $x,y \in A$, there exists $t \in (0,1)$ such that $$\|f(x) - f(y)\| \leq \|Df(tx + (1-t)y)\|\|x-y\|$$
\end{res}
\pause

\end{frame}















\begin{frame}
\begin{res}
Let $X, Y$ be a Banach spaces, $A \subset X$ open and convex and $f:A \rightarrow Y$. Suppose that $f$ is Frechet differentiable. If for each $x \in A$, $Df(x) = 0$, then $f$ is constant.
\end{res}
\pause

\begin{res}
Let $X, Y$ be a Banach spaces, $A \subset X$ open and convex and $f,g:A \rightarrow Y$. Suppose that $f$ and $g$ are Frechet differentiable. If $Df = Dg$, then there exists $c \in Y$ such that $f = g+c$.
\end{res}
\pause

\begin{res}
Let $X$ be a Banach spaces, $A \subset X$ open, $f:A \rightarrow \R$ and $x_0 \in A$. Suppose that $f$ is Frechet differentiable at $x_0$. If $f$ has a local minimum at $x_0$, then $Df(x_0) = 0$. 
\end{res}
\end{frame}












\begin{frame}
\begin{res}
Let $Y$ be a separable Banach space and $f \in C^1_Y(a,b)$. Then for each $x, x_0 \in (a,b)$, $x_0 < x$ implies that 
	\begin{enumerate}
	\item $f'$ is Bochner integrable on $(x_0, x]$ 
	\item  $$f(x) - f(x_0) = \int_{(x_0, x]}f'dm$$ 
	\end{enumerate}
\end{res}
\pause

\begin{res}
Let $Y$ be a separable Banach space, $A \subset X$ open and convex, $f\in C^n_Y(A)$ and $x_0 \in A$. Then $$f(x_0 + h) = \sum_{k=0}^n D^k f(x_0)(h, \dots, h) + o(\|h\|^n) \hspace{1cm }\text{ as } h \rightarrow 0$$
\end{res}
\end{frame}










\section{Hilbert Spaces}
\begin{frame}
\frametitle{Hilbert Spaces}


\begin{defn}
Let $H$ be an inner product space. Then $H$ is said to be a \textbf{Hilbert space} if $H$ is complete with respect to the norm induced by the inner product.
\end{defn}
\pause

\begin{rem}
We will be assuming the Hilbert space is real. 
\end{rem}
\pause

\begin{res}
Let $H$ be an inner product space. Then for each $x,y \in H$, $|\l x, y\r| \leq \|x\| \|y\|$ with equality iff $x \in \spn(y)$.
\end{res}
\end{frame}












\subsection{Riesz Representation Theorem}
\begin{frame}


\begin{defn}
Let $H$ be a Hilbert space. Define $\phi:H \rightarrow H^*$ by $x \mapsto x^*$ where $$x^*y = \l x ,y\r$$
\end{defn}
\pause

\begin{res}
Let $H$ be a Hilbert space. Then $\phi: H \rightarrow H^*$ defined above is an isometric isomorphism.
\end{res}

\end{frame}














\begin{frame}

\begin{defn}
Let $H$ be a Hilbert space, $f: H \rightarrow \R$ and $x_0 \in H$. Suppose that $f$ is Frechet differentiable at $x_0$ so that $Df(x_0) \in H^*$. We define the \textbf{gradient of $f$ at $x_0$}, denoted $\nabla f(x_0) \in H$, by $$\nabla f(x_0) = \phi^{-1}Df(x_0)$$ That is, $\nabla f(x_0)$ is the unique element of $H$ such that for each $y \in H$, $$\l \nabla f(x_0), y \r = Df(x_0)(y)$$
\end{defn}
\pause

\begin{res}
Let $H$ be a Hilbert space, $f:H \rightarrow \R$ and $x_0 \in H$. If $f$ is Frechet differentiable at $x_0$, then 
$$\argmin_{\|h\| \leq 1} Df(x_0)(h) = - \|\nabla f(x_0)\|^{-1} \nabla f(x_0)$$ 
\end{res} 
\end{frame}











\begin{frame}
\begin{rem}
In the context of Hilbert spaces, the gradient allows us generalize the gradient descent method for minimization.

The idea is as follows. If $f:H \rightarrow \R$ is Frechet differentiable. Then $$f(x_0+h) \approx f(x_0) + \l \nabla f(x_0), h \r$$ for $h$ near $0$. Taking $h = - \eta \nabla f(x_0)$ for some small $\eta >0$ insures that $h$ is close to $0$ and $h$ is in the direction of steepest descent of  $Df(x_0)(v)$ which causes $f(x_0+h) < f(x_0)$. 
\end{rem}

\end{frame}















\section{Convex Analysis}

\subsection{Results}
\begin{frame}
\frametitle{Convex Analysis}


\begin{res}
Let $X$ be a vector space, $A \subset X$ convex, $f:A \rightarrow \R$ convex and $x_0 \in A$. Then $f$ has a local minimum at $x_0$ iff $f$ has a global minimum at $x_0$.
\end{res}
\pause

\begin{res}
Let $X$ be a vector space, $A \subset X$ convex and $f:A \rightarrow \R$ strictly convex. If $f$ has a local minimum, then there exists a unique $x_0 \in A$ such that $f(x_0) = \min\limits_{x \in A}f(x)$.
\end{res}
\pause

\begin{res}
Let $X$ be a Banach space, $A \subset X$ open and convex, $f:A \rightarrow \R$ convex, $x_0 \in A$. Suppose that $f$ is $2$nd order Frechet differentiable at $x_0$. If $D^2f(x_0) \in L^2(X, \R)$ is positive semi definite (resp. pos. def.), then $f$ is convex (resp. strictly convex). 
\end{res}
\pause

\begin{rem}
By positive definite, we mean $D^2f(x_0)(h,h) > 0$ for $h \neq 0$.
\end{rem}
\end{frame}













\section{Reproducing Kernel Hilbert Spaces}
\subsection{RKHS's}
\begin{frame}

\frametitle{Reproducing Kernel Hilbert Spaces}



\begin{defn}
Let $T$ be a set and $H \subset \R^T$ a hilbert space. For $t \in T$, we define the \textbf{evauluation functional at $t$}, denoted $L_t : H \rightarrow \R$, by $$L_t(f) = f(t)$$ 
\pause
The space $H$ is said to be a \textbf{reproducing kernel Hilbert space (RKHS)} if for each $t \in T$, $L_t \in H^*$ (i.e. $L_t$ is bounded). \\
\pause
If $H$ is an RKHS, the Riesz representation theorem implies that for each $t \in T$, there exists $K_t \in H$ such that for each $f \in H$, $\l K_t, f\r = f(t)$. \\
\pause
If $H$ is an RKHS, we define the \textbf{reproducing kernel} associated to $H$, denoted $K_H:T^2 \rightarrow \R$, by $$K_H(s,t) = \l K_s, K_t \r$$ 
\end{defn}
\end{frame}















\begin{frame}
\begin{res}
Let $T$ be a set and $K : T^2 \rightarrow \R$. If $K$ is symmetric and positive definite, then there exists a unique reproducing kernel Hilbert space $H \subset \R^T$ such that $K_H = K$.
\end{res}

\end{frame}









\begin{frame}
\begin{res}
Let $T$ be a set, $K : T^2 \rightarrow \R$ a symmetric, postivie definite kernel on $T$, $H \subset \R^T$ the corresponding RKHS, $t = (t_j)_{j=1}^n \subset T$ and $y = (y_j)_{j=1}^n \subset \R$. \\
\pause
Define $L: H \rightarrow \R$ by $$L(f) = \sum_{j=1}^n (y_j - f(t_j) )^2 + \lam \|f\|^2$$ \\
\pause 
Put $\hat{f} = \argmin\limits_{f \in H}L(f)$. 
\\
\pause 
Then there exist $(\hat{\al}_j)_{j=1}^n \subset \R$ such that $$\hat{f}(t) = \sum_{j=1}^n \hat{\al}_jK(t, t_j)$$
\end{res}
\end{frame}















\begin{frame}
\begin{rem}
Define $A \in \R^{n \times n}$ by $A_{i,j} = K(t_i, t_j)$. Some regular calculus shows that $\hat{\al} = (A + \lam I)^{-1}y$
\end{rem}
\pause

\begin{ques}
What if $(A + \lam I)^{-1}$ is hard to compute? 
\pause
\end{ques}

\begin{ans}
gradient descent
\end{ans}
\end{frame}















\begin{frame}
\begin{rem}
Define $Q: H \rightarrow \R$ by $$Q(f) = \sum_{j=1}^n (y_j - f(t_j) )^2$$ 
\pause
We can write rewrite $Q(f)$ as  $$Q(f) = \|L_t(f) - y\|_2^2$$ where $L_t \in L(H, \R^n)$ is given by $$L_t(f) = (f(t_j))_{j=1}^n$$ 
\end{rem}
\end{frame}












\begin{frame}
Writing this out, we see that 
\begin{align*}
Q(f_0 + h) 
&= \|L_t(f_0) - y\|_2^2  + 2(L_t(f_0) - y)^T L_t(h) + \|L_t(h)\|_2^2 \\
&= Q(f_0) + [\text{lin funct of $h$}] +  [\text{bilin funct of $(h,h)$}]
\end{align*}
\pause
Equating terms from Taylors theorem, we see that $D^2Q(f_0)(h,h) = 2\|L_t(h)\|_2^2$, which is p.s.d. So $Q$ is convex. Since norms are convex and $\lam \geq 0$, $L$ is convex. 
\end{frame}














\begin{frame}
\begin{rem}
Similar to before, writing out $L(f_0 + h)$, we get
$$L(f_0 + h) = L(f_0) + 2(L_t(f_0) - y)^T L_t(h) + 2 \lam \l f_0 , h\r + o(\|h\|^2) $$
\pause
So 
\begin{align*}
DL(f_0)(h) 
&= 2(L_t(f_0) - y)^T L_t(h) + 2 \lam \l f_0 , h\r \\
&= 2\sum_{j=1}^n (f_0(t_j)- y_j) \l K_{t_j}, h\r +  2 \lam \l f_0 , h\r \\
&= \bigg \l  2\bigg [ \sum_{j=1}^n (f_0(t_j)- y_j) K_{t_j} + \lam f_0\bigg ] , h \bigg \r 
\end{align*}
\pause
Hence $$\nabla L(f_0) = 2\bigg [ \sum_{j=1}^n (f_0(t_j)- y_j) K_{t_j} + \lam f_0\bigg ]$$
\end{rem}
\end{frame}








\end{document}