\documentclass[notheorems]{beamer}

\usepackage{amsmath,amsthm,amssymb,setspace, mathtools}
\usepackage{physics}

\usepackage{color}   %May be necessary if you want to color links
\usepackage{hyperref}
\hypersetup{
	colorlinks=true, %set true if you want colored links
	linktoc=all,     %set to all if you want both sections and subsections linked
	linkcolor=black,  %choose some color if you want links to stand out
	urlcolor=cyan
}


% graphics
\usepackage{graphicx}
\graphicspath{/home/carson/Desktop/Github/NetworkIntegration/Figures}
\usepackage{subfig}


%
%
%
\newif\ifhideproofs
%\hideproofstrue %uncomment to hide proofs
%
%
%
%
\ifhideproofs
\usepackage{environ}
\NewEnviron{hide}{}
\let\proof\hide
\let\endproof\endhide
\fi



% math setup
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{defn}[definition]{Definition}
\newtheorem{nota}[definition]{Note}
\newtheorem{thm}[definition]{Theorem}
\newtheorem{lem}[definition]{Lemma}
\newtheorem{prop}[definition]{Proposition}
\newtheorem{cor}[definition]{Corollary}
\newtheorem{conj}[definition]{Conjecture}
\newtheorem{ex}[definition]{Exercise}
\newtheorem{ques}[definition]{Question}
\newtheorem{ans}[definition]{Answer}
\newtheorem{fact}[definition]{Fact}
\newtheorem{rem}[definition]{Remark}
\newtheorem{model}{Model}
\newtheorem{modl}[model]{Model}

\newcommand{\al}{\alpha}
\newcommand{\Gam}{\Gamma}
\newcommand{\be}{\beta} 
\newcommand{\del}{\delta} 
\newcommand{\Del}{\Delta}
\newcommand{\lam}{\lambda}  
\newcommand{\Lam}{\Lambda} 
\newcommand{\ep}{\epsilon}
\newcommand{\Sig}{\Sigma} 
\newcommand{\sig}{\sigma} 
\newcommand{\om}{\omega}
\newcommand{\Om}{\Omega}
\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\T}{\mathbb{T}}
\newcommand{\Q}{\mathbb{Q}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\MA}{\mathcal{A}}
\newcommand{\MC}{\mathcal{C}}
\newcommand{\MB}{\mathcal{B}}
\newcommand{\MF}{\mathcal{F}}
\newcommand{\MG}{\mathcal{G}}
\newcommand{\ML}{\mathcal{L}}
\newcommand{\MN}{\mathcal{N}}
\newcommand{\MS}{\mathcal{S}}
\newcommand{\MP}{\mathcal{P}}
\newcommand{\ME}{\mathcal{E}}
\newcommand{\MT}{\mathcal{T}}
\newcommand{\MM}{\mathcal{M}}
\newcommand{\MI}{\mathcal{I}}

\newcommand{\ui}{[0,1]}
\newcommand{\p}{\partial}

\newcommand{\io}{\text{ i.o.}}
%\newcommand{\ev}{\text{ ev.}}
\renewcommand{\r}{\rangle}
\renewcommand{\l}{\langle}

\newcommand{\RG}{[0,\infty]}
\newcommand{\Rg}{[0,\infty)}
\newcommand{\Ru}{(\infty, \infty]}
\newcommand{\Rd}{[\infty, \infty)}
\newcommand{\Ll}{L^1_{\text{loc}}(\R^n)}

\newcommand{\limfn}{\liminf \limits_{n \rightarrow \infty}}
\newcommand{\limpn}{\limsup \limits_{n \rightarrow \infty}}
\newcommand{\limn}{\lim \limits_{n \rightarrow \infty}}
\newcommand{\convt}[1]{\xrightarrow{\text{#1}}}
\newcommand{\conv}[1]{\xrightarrow{#1}} 
\newcommand{\seq}[2]{(#1_{#2})_{#2 \in \N}}

\newcommand{\lsc}{l.s.c. }

\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\spn}{span}
\DeclareMathOperator{\iso}{Iso}
\DeclareMathOperator{\id}{id}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}


%Information to be included in the title page:
\title{Presentation}
\author{Carson James}

\begin{document}

\frame{\titlepage}

%\begin{frame}
%\frametitle{Outline}
%\tableofcontents	
%\end{frame}

%\begin{enumerate}
%	\item Banach Spaces
%		\begin{itemize}
%		\item bounded linear maps
%		\item Frechet differentiation
%		\end{itemize}
%	\item Calculus 
%		\begin{itemize}
%		\item tools
%		\item results
%		\end{itemize}
%	\item Hilbert Spaces
%		\begin{itemize}
%		\item Riesz representation theorem
%		\item gradients
%		\end{itemize}
%	\item Convex Analysis 
%		\begin{itemize}
%		\item results for Frechet differentiable functions
%		\end{itemize}
%	\item Reproducing Kernel Hilbert Spaces
%		\begin{itemize}
%		\item representer theorem
%		\end{itemize}
%	\item Applications to Gaussian Processes
%	\begin{itemize}
%		\item predictive posterior 
%		\end{itemize}
%	
%	\end{enumerate}


\section{Models}

\begin{frame}
\begin{defn}
We define $\Lam^{n \times r}_+ = \{\Sig \in \R^{n \times r}: \Sig \text{ is diagonal and positive semi-definite}\}$ and $O_n = \{U \in \R^{n \times n}: U \text{ is orthogonal}\}$. 
\end{defn}
\end{frame}































\begin{frame}
\textbf{Model 1:}
\begin{enumerate}
    \item Fix $M \in \R^{n \times n_M}$columns of $M$ are orthogonal and set $P_M = M(M^TM)^{-1}M^T $?
    \item Choose $\Sig_Z \in \Lam^{n_M \times r}_+$, $\Sig_X \in \Lam^{n \times p}_+$,  $U_Z \in O_{r}$ and $U_X \in O_p$. 
    \item Set $V_Z^T = \Sig_Z U_Z$ and $V_X^T = \Sig_X U_X$. 
    \item Set $J_Z = MV_Z^T$ and $J_X = MV_X^T$. 
    \item Choose $I_Z \in \R^{n \times r}$, $I_X \in \R^{n \times p}$ such that $\MC(I_Z) \cup \MC(I_X) \subset \MC(I -P_M)$. 
    \item Choose $E_X \in \R_{n \times p}$ with $(E_X)_{i,j} \sim N(0, \sig^2)$
    \item Set $Z = J_Z + I_Z$ and $X = J_X + I_X + E_X$ 
\end{enumerate}
Then $\MC(M) \perp \MC(I_Z), \MC(I_X)$.

\end{frame}


\begin{frame}
\textbf{Model 2:}
We consider a modification of the planted partition model which is a submodel of the stochastic block model with $n$ nodes and $r$ blocks (for now $r=2$).
\begin{itemize}
    \item Choose $U \in \R^{n \times 2}$ such that for each $i \in \{1, \ldots, n\}$, 
    \[
    U_{i,j} = 
    \begin{cases}
    1 & \text{node $i$ is in block $j$} \\
    0 & \text{else}
    \end{cases}
    \] 
    as in the stochastic block model. We choose
    \[
    U = 
    \begin{pmatrix}
    1 & 0 \\
    \vdots & \vdots \\
    1 & 0 \\
    0 & 1 \\
    \vdots & \vdots \\
    0 & 1
    \end{pmatrix}
    \]
\end{itemize}
\end{frame}

\begin{frame}
\begin{itemize}
\item Choose $W \in \R^{n \times 1}$, with $W = (1, -1, \ldots, 1, -1)^T$ and $\MC(W) \perp \MC(U)$. 
    \item Choose $0<b<a<1$ and set
    \[\
    Q = 
    \begin{pmatrix}
    a & b \\
    b & a
    \end{pmatrix}
    \]
    to be the block probability matrix from the planted partition model.
    \item Set $B = Q^{1/2}$.
    \item Choose $\al \in (0,1)$ and set $Z = (1-\al) (0,W) + \al (UB, 0)$
    \item Set $X = (0,W)$
\end{itemize}
\end{frame}

\begin{frame}
Then
\begin{itemize}
    \item $U^TW = 0$
    \item $BB^T \in [0,1]^{2 \times 2}$ and $(UB)(UB)^T \in [0,1]^{n \times n}$
    \item if $\al$ is close enough to 1, then $Z \in  [0,1]^{n \times n}$
    \item here $J_Z = J_X = (0, W)$ and $I_Z = (UB, 0)$, $I_X = (0,0)$.
\end{itemize}
\end{frame}









\begin{frame}
\textbf{Model 3:}
We consider another modification of the planted partition model which is a submodel of the stochastic block model with $n$ nodes and $r$ blocks (for now $r=2$).
\begin{itemize}
    \item Choose $U \in \R^{n \times 2}$ such that for each $i \in \{1, \ldots, n\}$, 
    \[
    U_{i,j} = 
    \begin{cases}
    1 & \text{node $i$ is in block $j$} \\
    0 & \text{else}
    \end{cases}
    \] 
    as in the stochastic block model. We choose
    \[
    U = 
    \begin{pmatrix}
    1 & 0 \\
    \vdots & \vdots \\
    1 & 0 \\
    0 & 1 \\
    \vdots & \vdots \\
    0 & 1
    \end{pmatrix}
    \]
    \end{itemize}
    
    \end{frame}
    
    \begin{frame}
    
    \begin{itemize}
    \item Choose $W \in \R^{n \times 1}$, with $W = (1, -1, \ldots, 1, -1)^T$ and $\MC(W) \perp \MC(U)$. 
    \item Choose $0<b<a<1$ and set
    \[\
    Q = 
    \begin{pmatrix}
    a & b \\
    b & a
    \end{pmatrix}
    \]
    to be the block probability matrix from the planted partition model.
    \item Set $B = Q^{1/2}$.
    \item Choose $K^T \in O(p)$ and $I_X \in \R^{n \times p}$ such that $\MC(I_X) \perp \MC(W)$. 
    \item Choose $E_X \in \R^{n \times p}$ with $(E_X)_{i,j} \sim N(0, \sig^2)$
    \item Define $J_Z = W
    \begin{pmatrix}
    \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}
    \end{pmatrix}$, $I_Z = U B$, $J_X = 
    \begin{pmatrix}
    W & 0 
    \end{pmatrix}
    K^T$
     \item Choose $\alpha \in (0,1)$ and set $Z = (1 - \alpha) J_Z + \alpha I_Z$ 
    \item Set $X = 
     J_X + I_X + E_X$
\end{itemize}

\end{frame}
    
\begin{frame}
Then
\begin{itemize}
    \item $U^TW = 0$
    \item $BB^T \in [0,1]^{2 \times 2}$ and $(UB)(UB)^T \in [0,1]^{n \times n}$
    \item if $\al$ is close enough to 1, then $Z \in  [0,1]^{n \times n}$
    \end{itemize}

\end{frame}















\begin{frame}
\frametitle{Analysis of Initial Solution:} 
Consider the following model for the data:
$$A_{ij}\sim \text{Ber}(ZZ^T)_{ij}), \quad i>j,i,j\in[n],$$
$$X_{uv}= (W)_{uv} + \epsilon_{uv}, \quad u\in[n], v \in [p].$$
Assume $\epsilon_{uv} \overset{iid}{\sim} N(0, \sigma^2)$. Write 
$$Z = [M, R_Z] \Gamma,$$
$$W = [M, R_W] U^T,$$
where $M\in\R^{n\times r_M}, R_Z\in\R^{n\times r_Z}$ and $R_W\in\R^{n\times r_w}$ are matrices with orthogonal columns, and $\Gamma\in\R^{(r_M + r_Z)\times (r_M + r_Z)}$ and $U\in\R^{p\times (r_M + r_W)}$ some other matrices.

\end{frame}






\begin{frame}

Define $\hat{V}^{(1)}$ as the matrix of $(r_M + r_Z)$ leading eigenvectors of $A$, and $\hat{V}^{(2)}$ as the matrix of $(r_M + r_W)$ left leading singular vectors of $X$. Then define $\hat{M}$ as the matrix of $r_M$ left leading singular vectors of $[\hat{V}^{(1)}, \hat{V}^{(2)}]$. Set $$\epsilon = \sqrt{\frac{1}{2}\bigg(\frac{\del(ZZ^T)}{\lam^2_{\min}(\Gam \Gam^T)} + \frac{n}{\lam^2_{\min}(U^TU)}  \bigg)}$$

\begin{conj}
With the assumptions as above, and some regularity conditions (TBD) (maybe if $\ep = o(1)$ as $n \rightarrow \infty$), there exists some orthogonal matrix $U$ such that
$$\mathbb{E}\|\hat{M} - MU\|_F = O\left(\frac{r_M^{1/2}}{\sqrt{n}}\right).$$
\end{conj}
\end{frame}















\begin{frame}
\frametitle{idea}
\begin{align*}
    & U^{(1)} = A & U^{(2)} = X \\
    &\hat{\Pi}^{(i)} =  \hat{V}^{(i)}(\hat{V}^{(i)})^T  &\hat{\Pi} = \frac{1}{2}(\hat{\Pi}^{(1)} + \hat{\Pi}^{(2)}) \\
    &\tilde{\Pi}^{(i)} = \mathbb{E}\hat{\Pi}^{(i)}   &\tilde{\Pi} = \frac{1}{2}(\tilde{\Pi}^{(1)} + \tilde{\Pi}^{(2)}) \\ 
    &  & \Pi = MM^T \\
\end{align*}
\end{frame}














\begin{frame}
Define $\tilde{M}$ to be the matrix consisting of the $r_M$ left leading singular vectors of $\tilde{\Pi}$. Then 
\begin{align*}
    \min_{W \in O_{r_M}} \|\hat{M} - MW\|_F 
    & \leq \|\hat{M}\hat{M}^T - MM^T\|_F \\
    & \leq \|\hat{M}\hat{M}^T - \tilde{M}\tilde{M}^T\|_F + \|\tilde{M}\tilde{M}^T - MM^T\|_F
\end{align*}
Then we control both of these errors.
\end{frame}










\begin{frame}
To control the first error, we need the following lemma:
\begin{lem}
Let $X \in \mathbb{R}^{n \times p}$ with $X_{i,j} \sim N(0, \sigma^2)$ and $a > 1$. Set $C_{n,p} = \frac{a}{a-1}\frac{3}{2}\bigg[(\sqrt{n} + \sqrt{p}) + \frac{5}{\log{(3/2)}} \sqrt{\log{(n \wedge p)}} \bigg]\sigma$. Then for each $t \geq  C_{n,p}$, 
\begin{equation*}
    P(\|X\| \geq t) \leq \exp \bigg(-\frac{t^2}{2(a \sigma)^2} \bigg)
\end{equation*}
and for each $q \geq 1$,
\begin{equation*}
    \mathbb{E}(\|X\|^q) = O(\sigma^q(\sqrt{n} + \sqrt{p})^q)
\end{equation*}
\end{lem} 
\end{frame}
















\begin{frame}
Focusing on the first error, the Davis-Kahan theorem for rectangular matrices tells us that 
\begin{align*}
\|\hat{M}\hat{M}^T - \tilde{M}\tilde{M}^T\|_F
& \leq \frac{2^{3/2}(2 \sigma_1(\tilde{\Pi}) +  \|\hat{\Pi} - \tilde{\Pi}\|_{op})\|\hat{\Pi} - \tilde{\Pi}\|_F}{\sigma_{r_M}(\tilde{\Pi})^2 - \sigma_{r_M + 1}(\tilde{\Pi})^2} \\
& \leq \frac{2^{3/2}(2 \sigma_1(\tilde{\Pi}) +  \|\hat{\Pi} - \tilde{\Pi}\|_F)\|\hat{\Pi} - \tilde{\Pi}\|_F}{\sigma_{r_M}(\tilde{\Pi})^2 - \sigma_{r_M + 1}(\tilde{\Pi})^2}
\end{align*}
\end{frame}
















\begin{frame}
To bound $ \|\hat{\Pi} - \tilde{\Pi}\|_F$, we note that the triangle inequality implies that 
\begin{align*}
    \|\hat{\Pi} - \tilde{\Pi}\|_F
    &= \frac{1}{2} \bigg \|\sum_{i=1}^2 \hat{\Pi}^{(i)} - \tilde{\Pi}^{(i)}\bigg \|_F \\
    & \leq  \frac{1}{2}\sum_{i=1}^2 \|\hat{\Pi}^{(i)} - \tilde{\Pi}^{(i)}\|_F \\
\end{align*}
\end{frame}

















\begin{frame}
Since $\|\cdot\|_1 \leq \|\cdot \|_p$ on a probability space, we may bound 
\begin{align*}
    \mathbb{E} \bigg( \|\hat{\Pi}^{(i)} - \tilde{\Pi}^{(i)}\|_F^q \bigg)^{1/q}
    & \leq \mathbb{E} \bigg( \|\hat{\Pi}^{(i)} - \Pi \|_F^q \bigg)^{1/q}+ \mathbb{E} \bigg(\| \Pi - \tilde{\Pi}^{(i)}\|_F^q \bigg)^{1/q} \\
    & = \mathbb{E} \bigg( \|\hat{\Pi}^{(i)} - \Pi \|_F^q \bigg)^{1/q}+ \| \mathbb{E} (\Pi - \hat{\Pi}^{(i)})\|_F  \\
    & \leq \mathbb{E} \bigg( \|\hat{\Pi}^{(i)} - \Pi \|_F^q \bigg)^{1/q}+   \mathbb{E}\|(\Pi - \hat{\Pi}^{(i)})\|_F \\
    & \leq \mathbb{E} \bigg( \|\hat{\Pi}^{(i)} - \Pi \|_F^q \bigg)^{1/q}+   \mathbb{E}\bigg(\|\Pi - \hat{\Pi}^{(i)}\|_F^q \bigg)^{1/q}\\
    & = 2 \mathbb{E} \bigg( \|\hat{\Pi}^{(i)} - \Pi \|_F^q \bigg)^{1/q}
\end{align*}
\end{frame}


















\begin{frame}
To bound $\|\hat{\Pi}^{(i)} - \Pi \|_F^q$, we again apply the Davis-Kahan theorem to obtain 
\begin{align}
    \|\hat{\Pi}^{(i)} - \Pi \|_F
    & \leq \frac{2^{3/2}(2 \sigma_1(\Pi) +  \|U^{(i)} - \mathbb{E}U^{(i)}\|_{op}) r_M^{1/2}\|U^{(i)} - \mathbb{E}U^{(i)}\|_{op}}{\sigma_{r_M}(\mathbb{E}U^{(i)})^2 - \sigma_{r_M + 1}(\mathbb{E}U^{(i)})^2} 
\end{align}
\end{frame}

















\begin{frame}
Set $c_i = \frac{2^{3/2}r_M^{1/2}}{\sigma_{r_M}(\mathbb{E}U^{(i)})^2 - \sigma_{r_M + 1}(\mathbb{E}U^{(i)})^2}$. The previous lemma then implies that
\begin{align*}
    \mathbb{E}\|\hat{\Pi}^{(2)} - \Pi \|_F^q
    &\leq c_2^q \mathbb{E}\bigg[ (2 \sigma_1(\Pi) +  \|U^{(2)} - \mathbb{E}U^{(2)}\|_{op})^q \|U^{(2)} - \mathbb{E}U^{(2)}\|_{op}^q \bigg] \\
    &= c_2^q \mathbb{E}\bigg[\text{Binomial}(q, 2 \sigma_1(\Pi), \|U^{(2)} - \mathbb{E}U^{(2)}\|_{op}^{q} ) \|U^{(2)} - \mathbb{E}U^{(2)}\|_{op}^{q}  \bigg]\\
    &= c_2^q O(\sigma^{2q}(\sqrt{n} + \sqrt{r_M})^{2q})\\
\end{align*}
and 
$$(\mathbb{E}\|\hat{\Pi}^{(2)} - \Pi \|_F^q)^{1/q} = c_2 O(\sigma^2 (\sqrt{n} + \sqrt{r_M})^2)$$
\end{frame}















\begin{frame}
Now we need to use a lemma from the paper \emph{Distributed estimation of principal eigenspaces} (Fan et al) to combine these bounds as well as to be used in bounding the second error term $\|\tilde{M}\tilde{M}^T - MM^T\|_F$ 
\end{frame}

















\begin{frame}[fragile]
\frametitle{Data}
Data was obtained from 
\begin{itemize}
\item World Bank API via the 
\verb|wbstats| package. 
\item Github: \href{https://github.com/lukes/ISO-3166-Countries-with-Regional-Codes}{lukes/ISO-3166} 
\item Kaggle: \href{https://www.kaggle.com/datasets/yasirtariq/tradenetwork}{Trade Network} (I think I need to pull the data myself from the UN comtrade database using the \verb|comtradr| package)
\end{itemize}

\begin{align*}
&\text{GDP} & \text{regionAfrica} && \text{regionAmericas} \\
& 3127.891         &   1         &&  0  \\
& 3952.803         &   0         &&  0
\end{align*}
          
\end{frame}

















\begin{frame}
The first network, with adjacency matrix $A$, is defined so that two countries have a connection if the at least one of the countries receives at least a certain percentage of the other country's exports. Here the threshold is $0.15$\\
% trim=left bottom right top
%\hspace*{-5cm} 
\includegraphics[scale=1.5, trim={1cm 1cm 1cm 2cm}, clip]{/home/carson/Desktop/Github/NetworkIntegration/Figures/thresholded_network.pdf}

\end{frame}


















\begin{frame}
The second network, with weighted adjacency matrix $B$, is defined so that two countries have a connection if they had some nonnegligible trade. The weight of a connection is the $\log$ of the total trade between the two countries. \\
% trim=left bottom right top
%\hspace*{-5cm} 
\includegraphics[scale=2, trim={1cm 1cm 1cm 1.5cm}, clip]{/home/carson/Desktop/Github/NetworkIntegration/Figures/log_total_network.pdf}
\end{frame}





















\begin{frame}
Scree plots for $A$ and $B$:\\

\begin{figure}
   \includegraphics[width=0.475\textwidth]{/home/carson/Desktop/Github/NetworkIntegration/Figures/scree_A.pdf}
   \hfill
   \includegraphics[width=0.475\textwidth]{/home/carson/Desktop/Github/NetworkIntegration/Figures/scree_B.pdf}
\end{figure}
\end{frame}



















\begin{frame}
Some $\text{ASE}$ plots for $A$:
\begin{figure}
   \includegraphics[width=0.475\textwidth]{/home/carson/Desktop/Github/NetworkIntegration/Figures/ase_A_12.pdf}
   \hfill
   \includegraphics[width=0.475\textwidth]{/home/carson/Desktop/Github/NetworkIntegration/Figures/ase_A_34.pdf}
\end{figure}
\end{frame}


















\begin{frame}
$\text{ASE}$ plots for $B$:\\
\includegraphics[scale = 1.25]{/home/carson/Desktop/Github/NetworkIntegration/Figures/ase_B_12.pdf}
\end{frame}





















\begin{frame}
Scree plot for the covariates:\\
\includegraphics[scale = 1.25]{/home/carson/Desktop/Github/NetworkIntegration/Figures/scree_covars.pdf}
\end{frame}

















\begin{frame}
I added a snippet of code to Dongbangs algorithm to let me initialize $M$ as the COSIE estimate, $$\hat{M} = r_M \text{-l.s.v}(\hat{V}^{(1)}, \hat{V}^{(2)})$$ where 
\begin{itemize}
\item $\hat{V}^{(1)} = \text{$(r_M + r_Z)$-l.s.v}(A)$ 
\item $\hat{V}^{(2)} = \text{$(r_M + r_W)$-l.s.v}(X)$
\end{itemize}
\end{frame}

















\begin{frame}
Initializing Dongbangs algorithm for $A$ by averaging principal vectors and by the COSIE estimate yields a loss of  $7718508$ and $221727943$ respectively.\\

Initializing the algorithm for $B$ by averaging principal vectors and by the COSIE estimate yields a loss of  $140245606$ and $1743031541$ respectively. Maybe I made a mistake in my code since we were expecting the COSIE estimate to yield a better starting estimate. 
\end{frame}















%\begin{frame}
%
%\end{frame}




















\end{document}