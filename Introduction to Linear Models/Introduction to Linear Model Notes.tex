\documentclass[12pt]{amsart}
 \usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb,amsfonts,setspace}
\usepackage[shortlabels]{enumitem}
\usepackage{exercise, chngcntr}
\usepackage{physics}
\usepackage{mathtools}
%
%
%
\newif\ifhideproofs
%\hideproofstrue %uncomment to hide proofs
%
%
%
%
\ifhideproofs
\usepackage{environ}
\NewEnviron{hide}{}
\let\proof\hide
\let\endproof\endhide
\fi



\newcommand\Item[1][]{%
  \ifx\relax#1\relax  \item \else \item[#1] \fi
  \abovedisplayskip=0pt\abovedisplayshortskip=0pt~\vspace*{-\baselineskip}}



\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{conj}{Conjecture}
\newtheorem{defn}[thm]{Definition}
\newtheorem{note}[thm]{Note}
\newtheorem{ex}[thm]{Exercise}


\newcommand{\al}{\alpha}
\newcommand{\Gam}{\Gamma}
\newcommand{\be}{\beta} 
\newcommand{\del}{\delta} 
\newcommand{\Del}{\Delta}
\newcommand{\lam}{\lambda}  
\newcommand{\Lam}{\Lambda} 
\newcommand{\ep}{\epsilon}
\newcommand{\sig}{\sigma} 
\newcommand{\Sig}{\Sigma}
\newcommand{\om}{\omega}
\newcommand{\Om}{\Omega}
\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\MA}{\mathcal{A}}
\newcommand{\MC}{\mathcal{C}}
\newcommand{\MB}{\mathcal{B}}
\newcommand{\MF}{\mathcal{F}}
\newcommand{\MG}{\mathcal{G}}
\newcommand{\ML}{\mathcal{L}}
\newcommand{\MN}{\mathcal{N}}
\newcommand{\MS}{\mathcal{S}}
\newcommand{\MP}{\mathcal{P}}
\newcommand{\ME}{\mathcal{E}}
\newcommand{\MT}{\mathcal{T}}
\newcommand{\MM}{\mathcal{M}}

\newcommand{\z}[1]{Let ${#1} \in \MM_{m,n}$}

\newcommand{\RG}{[0,\infty]}
\newcommand{\Rg}{[0,\infty)}
\newcommand{\limfn}{\liminf \limits_{n \rightarrow \infty}}
\newcommand{\limpn}{\limsup \limits_{n \rightarrow \infty}}
\newcommand{\limn}{\lim \limits_{n \rightarrow \infty}}
\newcommand{\convt}[1]{\xrightarrow{\text{#1}}}
\newcommand{\conv}[1]{\xrightarrow{#1}} 

\newcommand{\Ll}{L^1_{\text{loc}}(\R^n)}
\newcommand{\seq}[1]{(x_{#1})_{#1 \in \N}}

\newcommand{\n}{\Vert}
\DeclareMathOperator*{\diag}{diag}
\DeclareMathOperator*{\ke}{ker}

 
\begin{document}

\title{Linear Model Notes}
\author[James]{Carson James}
\maketitle

\tableofcontents

\section{Matrix Algebra}

\subsection{Column and Null Space}

\begin{ex}
Let $X \in \MM_{m,n}$. Then $\MN(X) = \MN(X^TX)$.
\end{ex}

\begin{proof}
Let $a \in \MN(X)$. Then $Xa = 0$. So $X^TXa = 0$. Thus $a \in \MN(X^TX)$. Conversely, suppose that $a \in \MN(X^TX)$. Then $X^TXa = 0$. So 
\begin{align*}
0 
&= a^TX^TXa \\
&= (Xa)^T(Xa) \\
&= \n Xa \n^2 
\end{align*}
Hence $Xa = 0$ and $a \in \MN(X)$.
\end{proof}

\begin{ex}
Let $X \in \MM_{m,n}$. Then $\MC(X^T) = \MC(X^TX)$.
\end{ex}

\begin{proof}
\begin{align*}
\MC(X^T) 
&= \MN(X)^{\perp} \\
&= \MN(X^TX)^{\perp} \\
&= \MC(X^T X)
\end{align*}
\end{proof}

\begin{ex}
\z{X}. If $X^TX = 0$, then $X = 0$.
\end{ex}

\begin{proof}
Suppose that $X^TX = 0$. Then 
\begin{align*}
rank(X^T)
&= \dim \MC(X^T) \\
&= \dim \MC(X^TX) \\
&= rank(X^TX) \\
&= 0
\end{align*}
So $X^T = X = 0$.
\end{proof}

\begin{ex}
Let $X \in \MM_{m,n}$ and $A,B \in \MM_{n,p}$. Then $X^TXA = X^TXB$ iff $XA = XB$. 
\end{ex}

\begin{proof}
Clearly if $XA = XB$, then $X^TXA = X^TXB$. Conversely, suppose that $X^TXA = X^TXB$. Then $X^TX(A-B) = 0$. So for  each $i =1, \cdots, p$, $X^TX(A-B)e_i = 0$. Thus for each $i=1, \cdots, p$ $X(A-B)e_i \in \MN(X^T) \cap \MC(X) = \{0\}$. Hence $X(A-B) = 0$ and $XA = XB$. 
\end{proof}

\begin{thm}
Let $X \in \MM_{m,n}$. Then $$nullity(X) + rank(X) = n$$.
\end{thm}

\begin{ex} 
Let $X \in \MM_{m,n}$. Then
$$rank(X^T) = rank(X)$$
\end{ex}

\begin{proof}
We have that 
\begin{align*}
rank(X^T)
&= rank(X^TX) \\
&= n - nullity(X^TX) \\
&= n - nullity(X) \\
&= rank(X)
\end{align*}
\end{proof}

\begin{defn}
Let $X \in \MM_{m,n}$. Then $X$ is said to have \textbf{full column rank} if $rank(X) = n$
\end{defn}

\begin{ex}
Let $X \in \MM_{m,n}$. If $X$ has full column rank, then $$\MN(X) = \{0\}$$
\end{ex}

\begin{proof}
Suppose that $X$ has full column rank. Then $rank(X) = n$ Hence $nullity(X) = 0$ and $\MN(X) = \{0\}$.
\end{proof}


\subsection{Generalized Inverses}

\begin{defn}
Let $A \in \MM_{m,n}$ and $G \in \MM_{n,m}$. Then $G$ is said to be a \textbf{generalized inverse} of $A$ if $AGA = A$. 
\end{defn}

\begin{thm}
Let $A \in \MM_{m,n}$. Suppose that $rank(A) = r$. Then there exists $P \in \MM_{m,m}, Q \in \MM_{n,n}, C \in \MM_{r,r}$ such that $P,Q,C$ are non-singular, $rank(C) = r$ and 
\[
A = P
\begin{pmatrix}
C & 0 \\
0 & 0
\end{pmatrix}
Q
\]
\end{thm}

\begin{ex}
Let 
\[
A = P
\begin{pmatrix}
C & 0 \\
0 & 0
\end{pmatrix}
Q
\]
as in the previous theorem and $D \in \MM_{r,m-r}, E \in \MM_{n-r, r}, F \in \MM_{n-r, m-r}$. Put 
\[
G = Q^{-1}
\begin{pmatrix}
C^{-1} & D \\
E & F
\end{pmatrix}
P^{-1}
\]

Then $G$ is a generalized inverse of $A$.
 
\end{ex}

\begin{proof}\
\begin{align*}
AGA 
&= \bigg[P
\begin{pmatrix}
C &0 \\
0 & 0
\end{pmatrix}
Q \bigg]\bigg[Q^{-1}
\begin{pmatrix}
C ^{-1}&D \\
E & F
\end{pmatrix}
P^{-1} \bigg] \bigg[P
\begin{pmatrix}
C &0 \\
0 & 0
\end{pmatrix}
Q\bigg] \\
&= P
\begin{pmatrix}
C &0 \\
0 & 0
\end{pmatrix}
\begin{pmatrix}
C ^{-1}&D \\
E & F
\end{pmatrix}
\begin{pmatrix}
C &0 \\
0 & 0
\end{pmatrix}
Q\\
&= P 
\begin{pmatrix}
I &CD \\
0 & 0
\end{pmatrix}
\begin{pmatrix}
C &0 \\
0 & 0
\end{pmatrix}
Q \\
&= P
\begin{pmatrix}
C &0 \\
0 & 0
\end{pmatrix}
Q \\
&= A
\end{align*}
\end{proof}


\begin{note}
The previous exercise and theorem guarantee the existence of a generealized inverse for all matrices.
We will take $G = A^-$ to mean that $G$ is a generalized inverse of $A$. Unless otherwise specified, $A^-$ will refer to a generic generalized inverse of $A$, that is, unless otherwise specified, any statement about $A^-$ will apply to all generalized inverses of $A$.
\end{note}

\begin{thm}
Let $A \in \MM_{m,n}$. Suppose that $rank(A) = r$. Let $P \in \MM_{mm}$, $Q \in \MM_{n,n}$ permutation matrices and $C \in \MM_{r,r}$. Suppose that $rank(C) = r$ and $PAQ = \begin{pmatrix}
C &D \\
E &F
\end{pmatrix}$ Then $Q\begin{pmatrix}
C^{-1} &0 \\
0 &0
\end{pmatrix}P = A^-$. 
\end{thm}

\begin{ex}
\z{X}. Then $(X^T)^- = (X^-)^T$.
\end{ex}

\begin{proof}
\begin{align*}
X^T(X^-)^TX^T
&= (X X^- X)^T\\
&= X^T
\end{align*}
\end{proof}

\begin{ex}
\z{X}. Then $\MC(XX^-) = \MC(X)$. 
\end{ex}

\begin{proof}
Clearly $\MC(XX^-) \subset \MC(X)$. Let $b \in \MC(X)$. Then there exists $a \in \R^n$ such that $Xa = b$. Then 
\begin{align*}
XX^-b 
&= XX^-Xa \\
&= Xa \\
&= b
\end{align*}

So $b \in \MC(XX^-)$. Thus $\MC(X) \subset \MC(XX^-)$ and $\MC(X) = \MC(XX^-)$
\end{proof}

\begin{ex}
\z{X}. Then $\MN(X) = \MN(X^-X)$
\end{ex}

\begin{proof}
From the previous exercise, we have that 
\begin{align*}
\MN(X) 
&= \MC(X^T)^{\perp} \\
&= \MC(X^T(X^T)^-)^{\perp} \\
&= \MC(X^T(X^-)^T)^{\perp} \\
&= \MC((X^-X)^T)^{\perp} \\
&= \MN(X^-X)
\end{align*}
\end{proof}
\vspace{2mm}

\begin{ex}
\z{X}. Then $X^- = (X^TX)^-X^T$. 
\end{ex}

\begin{proof}
By definition, $X^TX (X^TX)^- X^TX = X^TX$. A previous exercise implies that $X(X^TX)^-X^TX = X$. Thus $X^- = (X^TX)^-X^T$.
\end{proof}

\subsection{Projections}

\begin{defn}
Let $A \in \MM_{m,m}$. Then $X$ is said to be \textbf{idempotent} if $A^2 = A$.
\end{defn}

\begin{ex}
Let $X \in \MM_{m.n}$. Then $XX^-$ and $X^-X$ are idempotent
\end{ex}

\begin{proof}
\begin{align*}
(XX^-)(XX^-) 
&= (XX^-X)X^- \\
&= XX^-\\
\end{align*} The case is similar for $X^-X$.
\end{proof}

\begin{ex}
Let $A \in \MM_{m.m}$. If $X$ is idempotent, then $I-A$ is idempotent.
\end{ex}

\begin{proof} 
Suppose that $A$ is idempotent. Then
\begin{align*}
(I-A)(I-A) 
&= I^2 -IA -AI + A^2 \\
&= I -2A +A \\
&= I -A
\end{align*}
\end{proof}

\begin{thm}
Let $A \in \MM_{m,m}$. If $A$ is idempotent, then $rank(A) = tr(A)$.
\end{thm}

\begin{defn}
Let $P \in \MM_{m,m}$ and $S \subset \R^m$ a subspace. Then $P$ is said to be a \textbf{projection matrix} onto $S$ if 
\begin{enumerate}
\item $P$ is idempotent
\item $\MC(P) \subset S$ 
\item for each $x \in S$, $Px = x$
\end{enumerate} 
\end{defn}

\begin{note}
In the previous definition, (2) and (3) imply that $\MC(X) = S$, so to say that $X$ projects ``onto" S is accurate.
\end{note}

\begin{ex}
Let $S \subset \R^m$ and $P,Q$ projection matrices onto $S$. Then $PQ = Q$. 
\end{ex}

\begin{proof}
Let $x \in \R^m$. Then $Qx \in \MC(Q) = S$. So $PQx = Qx$. Thus $PQ = Q$.
\end{proof}

\begin{ex}
\z{X}. Then $XX^-$ is a projection onto $\MC(X)$.
\end{ex}

\begin{proof}
A previous exercises tells us that $XX^-$ is idempotent. Another previous exercise tells us that $\MC(XX^-) = \MC(X)$. Let $b \in \MC(X)$. Then there exists $a \in \R^n$ such that $Xa =b$. So 
\begin{align*}
XX^-b
&= XX^- Xa\\ 
&= Xa\\
&= b
\end{align*}
\end{proof}

\begin{ex}
\z{X}. Then $I-X^-X$ is a projection onto $\MN(X)$
\end{ex}

\begin{proof}
Since $X^-X$ is idempotent, so is $I-X^-X$. Let $b \in \MC(I-X^-X)$. Then there exists $a \in \R^n$ such that $(I-X^-X)a = b$. Then 
\begin{align*}
Xb 
&= X(I-X^-X)a \\
&= (X-XX^-X)a \\
&= (X-X)a \\
&= 0a \\ 
&= 0
\end{align*}
So $\MC(I-X^-X) \subset \MN(X)$. Let $a \in \MN(X)$. Then $Xa =0$ and 
\begin{align*}
(I-X^-X)a 
&= a - X^-Xa \\
&= a
\end{align*} 
So for each $a \in \MN(X)$, $(I-X^-X)a = a$.
\end{proof}

\begin{ex}
Let $S \subset \R^m$ be a subspace and $P \in \MM_{m,m}$ be a symmetric projection matrix onto $S$. Then $P$ is unique. 
\end{ex}

\begin{proof}
Let $Q \in \MM_{m,m}$ be a symmetric projection matrix onto $S$. Then 
\begin{align*}
(P-Q)^T(P-Q) 
&= P^TP - P^TQ - Q^TP + Q^TQ \\
&= P^2 - PQ - QP +Q^2 \\
&= P - Q - P + Q \\
&= 0
\end{align*}
Thus $P-Q = 0$ and $P =Q$.
\end{proof}

\begin{defn}
\z{X}. We define $P_X$ by $$P_X = X (X^TX)^-X^T$$
\end{defn}

\begin{ex}
\z{X}. Then $P_X$ is well defined, that is, $P_X$ is independent of the choice of $(X^TX)^-$.
\end{ex}

\begin{proof}
Suppose that $G, H$ are generalized inverses of $X^TX$. By definition, we have 
\begin{align*}
X^TXGX^TX = X^TXHX^TX 
& \Rightarrow XGX^TX = XHX^TX \\
& \Rightarrow X^TXG^TX^T = X^TXHX^T \\
& \Rightarrow XG^TX^T = XHX^T \\
& \Rightarrow XGX^T = XHX^T = P_X
\end{align*}
\end{proof}

\begin{note}
Recall that $X^- = (X^TX)^-X^T$. So that $P_X = XX^-$ is indeed a projection onto $\MC(X)$. Recall that $[(X^TX)^-]^T$ is a generalized inverse of $(X^TX)^T = (X^TX)$. Hence $P_X^T = X[(X^TX)^-]^TX^T = P_X$. Since $P_X$ is symmetric, it is the unique symmetric projection onto $\MC(X)$. 
\end{note}

\begin{ex}
\z{X}. Then $(X^T)^- = X(X^TX)^-$. 
\end{ex}

\begin{proof}
We know that $P_X X = X$. Transposing both sides, we get that 
\begin{align*}
X^T 
&= X^TP_X \\
&= X^TX(X^TX)^-X^T
\end{align*} 
So $$(X^T)^- = X(X^TX)^-$$
\end{proof}

\begin{note}
Recall that $(X^T)^- = X(X^TX)^-$. So that $P_X = (X^T)^-X^T$. A previous exercises tells us that $I-P_X$ is a projection on $\MN(X^T)$. Since $I-P_X$ is symmetric, it is the unique symmetric projection onto $\MN(X^T)$. 
\end{note}

\begin{ex}
Let $X_1, X_2 \in \MM_{m,n}$. Suppose that $\MC(X_1) = \MC(X_2)^{\perp}$. Then $P_{X_1}P_{X_2} = P_{X_2}P_{X_1} = 0$.  
\end{ex}

\begin{proof}
Since $I-P_{X_1}$ is the unique symmetric projection onto $\MN(X_1^T) = \MC(X_1)^{\perp} = \MC(X_2)$, we have that $I-P_{X_1} = P_{X_2}$. Thus $P_{X_1}P_{X_2} = P_{X_1}(I-P_{X_1}) = 0$. Similarly, $P_{X_2}P_{X_1} = 0$.
\end{proof}

\begin{ex}
Let $X \in \MM_{m,n}$. For each $z \in \MN(X^T)$, $P_Xz = 0$.
\end{ex}

\begin{proof}
Let $z \in \MN(X^T)$. Then $P_Xz = X(X^TX)^-(X^Tz) = 0$.
\end{proof}

\begin{ex}
Let $X_1,X_2 \in \MM_{m,n}$. If $\MC(X_1) \subset \C(X_2)$, then $P_{X_2} - P_{X_1}$ is the unique projection onto $\MC((I-P_{X_1})X_2)$.
\end{ex}

\begin{proof}
Clearly $P_{X_2} - P_{X_1}$ is symmetric. Since $\MC(X_1) \subset \C(X_2)$, we have that $P_{X_2}P_{X_1} = P_{X_1}$. Also, by symmetry, 
\begin{align*}
(P_{X_1}P_{X_2})^T 
&= P_{X_2}^TP_{X_1}^T \\
&= P_{X_2}P_{X_1} \\
&= P_{X_1} 
\end{align*} 
So $P_{X_1}P_{X_2} = P_{X_1}^T = P_{X_1}$. Now we have that
\begin{enumerate}
\item 
\begin{align*}
(P_{X_2} - P_{X_1})^2
&= (P_{X_2} - P_{X_1})(P_{X_2} - P_{X_1}) \\
&= P_{X_2}^2 + P_{X_1}^2 - P_{X_2}P_{X_1} - P_{X_1}P_{X_2} \\
&= P_{X_2} + P_{X_1} - P_{X_1} - P_{X_1} \\
&= P_{X_2} - P_{X_1}
\end{align*}
So $P_{X_2} - P_{X_1}$ is idempotent.
\item Let $x \in \R^m$. Then there exist unique $y \in \MC(X_2)$ and $z \in \MC(X_2)^{\perp} = \MN(X_2^T)$ such that $x = y+z$. So there exists $e \in \R^n$ such that $y = X_2e$ Since $z \in \MN(X_2^T)$, $P_{X_2}z = 0$. Then 
\begin{align*}
(P_{X_2} - P_{X_1})x
&= P_{X_2}x- P_{X_1}x \\
&= P_{X_2}x - P_{X_1}P_{X_2}x \\
&= y - P_{X_1}y \\
&= X_2e - P_{X_1}X_2e \\
&= (I - P_{X_1})X_2e \\
& \in \MC((I - P_{X_1})X_2)
\end{align*}
\item Let $x \in \MC((I - P_{X_1})X_2)$. Then there existe $e \in \R^n$ such that $x = (I - P_{X_1})X_2e$. So 
\begin{align*}
(P_{X_2} - P_{X_1})x
&= P_{X_2}(I - P_{X_1})x \\
&= P_{X_2}(I - P_{X_1})(I - P_{X_1})X_2e \\
&= P_{X_2}(I - P_{X_1})X_2e \\
&= (P_{X_2} - P_{X_1})X_2e \\
&= P_{X_2}X_2e - P_{X_1}X_2e \\
&= X_2e - P_{X_1}X_2e \\
&= (I-P_{X_1})X_2e \\
&= x
\end{align*}
\end{enumerate}
\end{proof}

\subsection{Solving Linear Equations}

\begin{defn}
Let $A \in \MM_{m,n}$ and $b \in \R^m$. Then the system $Ax=b$ is said to be \textbf{consistent} if $b \in \MC(A)$.
\end{defn}

\begin{ex}
Let $A \in \MM_{m,n}$ and $G \in \MM_{n,m}$. Then $G = A^-$ iff for each $b \in \MC(A)$, $Gb$ solves $Ax = b$. 
\end{ex}

\begin{proof}
Suppose that $G = A^-$. Let $b \in \MC(A)$. Then there exists $x^* \in \R^n$ such that $Ax^* = b$. So
\begin{align*}
A(Gb)
&= AG(Ax^*) \\
&= (AGA)x^* \\
&= Ax^* \\
&= b
\end{align*}
So $Gb$ solves $Ax =b$. Conversely, Suppose that for each $b \in \MC(A)$, $Gb$ solves $Ax = b$. Let $z \in \R^n$. So $Az \in \MC(A)$. Then  
\begin{align*}
(AGA)z 
&= A[G(Az)] \\
&= Az
\end{align*}
Since for each $z \in \R^n AGAz = Az$, $AGA = A$ and $G = A^-$.
\end{proof}
\vspace{2mm}

\begin{ex}
Let $b \in \MC(A)$. Then $$\{x \in \R^n: Ax = b\} = \{A^-b+(I-A^-A)z: z \in \R^n \}$$.
\end{ex}

\begin{proof}
Let $x \in \{A^-b+(I-A^-A)z: z \in \R^n \}$. Then there exists $z \in \R^n$ such that $x = A^-b+(I-A^-A)z$. Since $(I-A^-A)$ is a projection onto $\MN(A)$, 
\begin{align*}
Ax
&= AA^-b \\
&= b
\end{align*}
So $x \in \{x \in \R^n: Ax = b\}$. Conversely, let $x \in \{x \in \R^n: Ax = b\}$. Then 
\begin{align*}
x 
&= A^-(Ax) + (x - A^-Ax) \\
&= A^-(b) + (I-A^-A)x \\
& \in \{A^-b+(I-A^-A)z: z \in \R^n \}
\end{align*}
\end{proof}

\subsection{Moore-Penrose Pseudoinverse}

\begin{thm} \textbf{Singular Value Decomposition:} \\
Let $A \in \MM_{m,n}$. Suppose that $rank(A) = r$. Then there exist $U \in \MM_{m,m} V \in \MM_{n,n}$, and $D_0 \in \MM_{r,r}$ such that
\begin{enumerate}
\item $A = 
U
\begin{pmatrix}
D_0 & 0 \\
0 & 0
\end{pmatrix}
V^T
$
\item  
$U^T
U = I$
\item $V^T
V = I$ 
\item $D_0 = diagonal(d_1, d_2, \cdots, d_r)$ with $d_1 \geq d_2 \geq \cdots \geq d_r >0$
\end{enumerate}

\end{thm}

\begin{note}
Put $D = 
\begin{pmatrix}
D_0 & 0 \\
0 & 0
\end{pmatrix} \in \MM_{m,n}$ 
\begin{enumerate}
\item Since $D_0$ is symmetric, $D^T = 
\begin{pmatrix}
D_0 & 0 \\
0 & 0
\end{pmatrix}  \in \MM_{n,m}$
\item Since $D_0$ is diagonal, $D_0^{-1}$ is also diagonal and symmetric
\end{enumerate}
\end{note}

\begin{defn}
Let $A \in \MM_{m,m}$ and $A^+ \in \MM_{n,m}$. Then $A^+$ is said to be a  \textbf{Moore-Penrose pseudoinverse} of $A$ if 
\begin{enumerate}
\item $AA^+A = A$
\item $A^+AA^+ = A^+$
\item $AA^+$ is symmetric
\item $A^+A$ is symmetric
\end{enumerate} 
\end{defn}

\begin{note}
We have that $P_X = XX^+ = X(X^TX)^-X^T$. 
\end{note}

\begin{ex}
Let $A \in \MM_{m,n}$ and $S,T \in \MM_{n,m}$. If $S$ and $T$ are m-p pseudoinverses of $A$, then $S=T$. 
\end{ex}

\begin{proof}
Suppose that $S,T$ satisfy properties (1)-(4). Then 
\begin{align*}
S
&= SAS \\
&= (SA)^TS \\
&= A^TS^TS \\
&= (ATA)^TS^TS \\
&= A^TT^TA^TS^TS \\
&= (TA)^T(SA)^TS \\
&= (TA)(SA)S \\
&= TA(SAS) \\
&= TAS
\end{align*}
and 
\begin{align*}
T 
&= TAT \\
&= T(AT)^T \\
&= TT^TA^T \\
&= TT^T(ASA)^T \\
&= TT^TA^TS^TA^T \\
&= T(AT)^T (AS)^T \\
&= T(AT) (AS) \\
&= (TAT)AS \\
&= TSA 
\end{align*} 

So $S=T$
\end{proof}

\begin{ex}
Let $A \in \MM_{m, n}$ have singular value decomposition $A = UDV^T = U
\begin{pmatrix}
D_0 & 0 \\
0 & 0
\end{pmatrix}
V^T$. Define $D^+ = \begin{pmatrix}
D_0^{-1} & 0 \\
0 & 0
\end{pmatrix} \in \MM_{n,m}$. Then $D^+$ is the m-p pseudoinverse of $D$.
\end{ex}

\begin{proof}\
\begin{enumerate}
\item 
\begin{align*}
DD^+D 
&= \begin{pmatrix}
D_0 & 0 \\
0 & 0
\end{pmatrix} 
\begin{pmatrix}
D_0^{-1} & 0 \\
0 & 0
\end{pmatrix} 
\begin{pmatrix}
D_0 & 0 \\
0 & 0
\end{pmatrix} \\
&= \begin{pmatrix}
I & 0 \\
0 & 0
\end{pmatrix}
\begin{pmatrix}
D_0 & 0 \\
0 & 0
\end{pmatrix} \\ 
&= \begin{pmatrix}
D_0 & 0 \\
0 & 0
\end{pmatrix} \\
&= D
\end{align*}
\item Similar to (1).
\item 
\begin{align*}
(DD^{+})^T
&=\begin{pmatrix}
I & 0 \\
0 & 0
\end{pmatrix}^T \\
&= \begin{pmatrix}
I & 0 \\
0 & 0
\end{pmatrix} \\
&= DD^{+}
\end{align*}
\item Similar to (3).
\end{enumerate}

\end{proof}

\begin{ex}
Let $A \in \MM_{m, n}$ have singular value decomposition $A = UDV^T$. So $A^T \in \MM_{n,m}$ has singular value decomposition $A^T = VD^TU^T$.
Then $(D^T)^+ = (D^+)^T$
\end{ex}

\begin{proof}
Since $D^T = \begin{pmatrix}
D_0 & 0 \\
0 & 0
\end{pmatrix} \in \MM_{n,m}$, we have that $(D^T)^+ = 
\begin{pmatrix}
D_0^{-1} & 0 \\
0 & 0
\end{pmatrix}
= (D^+)^T$
\end{proof}

\begin{ex}
Let $A \in \MM_{m, n}$ have singular value decomposition $A = UDV^T$. Define $A^+ = VD^+U^T$. Then $A^+$ is the m-p pseudoinverse of $A$.
\end{ex}

\begin{proof}
\begin{enumerate}
\item 
\begin{align*}
AA^+A 
&= (UDV^T)(VD^+U^T)(UDV^T) \\
&= UDD^+DV^T \\
&= UDV^T \\
&= A
\end{align*}
\item Similar to (1)
\item 
\begin{align*}
(AA^+)^T
&= \big[(UDV^T)(VD^+U^T)\big]^T\\
&= (UDD^+U^T)^T \\
&= U(DD^+)^TU^T \\
&= U DD^+ U^T \\
&= (UDV^T)(VD^+U^T) \\
&= AA^+
\end{align*}
\item Similar to (3).
\end{enumerate}
\end{proof}

\begin{ex}
Let $A \in \MM_{m,n}$ have singular value decomposition $A = UDV^T$. Then $(A^T)^+ = (A^+)^T$.
\end{ex}

\begin{proof} \
\begin{align*}
(A^T)^+ 
&= \big[(UDV^T)^T\big]^+ \\
&= (VD^TU^T)^+ \\
&= U(D^T)^+V^T \\
&= U(D^+)^TV^T \\
&= (VD^+U^T)^T \\
&= (A^+)^T
\end{align*}
\end{proof}
\vspace{3mm}


\begin{ex} Let $A \in \MM_{m,n}$. Then there exists a unique matrix $A^+ \in \MM_{n,m}$ such that $A^+$ is the m-p pseudoinverse of $A$. 
\end{ex}

\begin{proof}
The existence of and uniqueness of $A^+$ are shown in the previous exercises. 
\end{proof}

\begin{ex}
Let $A \in \MM_{m,m}$. Then $(A^+)^+ = A$.
\end{ex}

\begin{proof}
We observe that $A$ satisfies properties $(1)-(4)$ for $A^+$. By uniqueness, $(A^+)^+ = A$.
\end{proof}

\begin{ex}
Let $A \in \MM_{m,n}$ and $b \in \MC(A)$. Pur $S = \{x \in \R^n: Ax= b\}$. Then $$\n A^+b \n = \min_{x \in S}\n x \n$$.
\end{ex}

\begin{proof} Let $x \in S$. A previous exercise tells us that there exists $z \in \R^n$ such that $x = A^+b + (I-A^+A)z$. Then 
\begin{align*} 
\n x \n^2
&= \n A^+b + (I-A^+A)z \n^2 \\
&= (A^+b + (I-A^+A)z)^T(A^+b + (I-A^+A)z) \\
&= \n A^+b \n^2 - 2z^T(I-A^+A)^T(A^+b) + \n (I-A^+A)z \n^2 \\
&= \n A^+b \n^2 - 2z^T(I-A^+A)A^+b + \n (I-A^+A)z \n^2 \\
&= \n A^+b \n^2 + \n (I-A^+A)z \n^2 \\
& \geq \n A^+b \n^2
\end{align*}
\end{proof}

\subsection{Differentiation}

\begin{defn}
Let $Q:\R^n \rightarrow \R$ given by $b \mapsto Q(b)$. Suppose that $Q \in C^1(\R^n)$.  We define 
\[ 
\pdv{Q}{b}= 
\begin{pmatrix} 
\pdv{Q}{b_1}  \\
\vdots\\
\pdv{Q}{b_n}
\end{pmatrix}  
\]
\end{defn}

\begin{ex}
Let $a, b  \in \R_n$ and $A \in \MM_{n, n}$. Then \vspace{2mm}
\begin{enumerate}
\Item $$\pdv{a^Tb}{b} = a$$ \vspace{2mm}
\Item $$\pdv{b^T A b}{b} = (A+A^T)b$$
\end{enumerate} 
\end{ex}

\begin{proof}\
\begin{enumerate}
\Item Since $$a^Tb = \sum_{i=1}^n a_ib_i$$ We have that $$\pdv{a^Tb}{b_i} = a_i$$ and therefore $$\frac{\partial a^Tb}{\partial b} = a$$ \vspace{3mm}
\Item Since 
\begin{align*}
b^T A b 
&= \sum_{i = 1}^n b_i \sum_{j=1}^n A_{i,j}b_j \\
&= \sum_{i = 1}^n \sum_{j=1}^n b_iA_{i,j}b_j \\
\end{align*}
The terms containing $b_i$ are $$A_{i,i}b_i^2 + \sum_{\substack{ j=1 \\ j \neq i}}^n (A_{i,j} + A_{j,i})b_ib_j$$
This implies that 
\begin{align*}
\frac{\partial b^TAb}{\partial b_i} 
&= 2A_{i,i}b_i + \sum_{\substack{j=1 \\ j\neq i}}^n (A_{i,j}+A_{j,i})b_j\\
&= \sum_{j=1}^n (A_{i,j}+A^T_{i,j})b_j\\
&= [(A+A^T)b]_i
\end{align*}
So $$\pdv{b^T A b}{b} = (A+A^T)b$$
\end{enumerate}
\end{proof} 

\subsection{Quadratic Forms and Eigendecomposition}

\begin{defn}
Let $A \in \MM_{n, n}$. Then $A$ is said to be \textbf{postive semi-definite} if for each $x \in \R^n$, $$x^TAx \geq 0$$ 
\end{defn}

\begin{defn}
Let $A \in \MM_{n, n}$. Then $A$ is said to be \textbf{postive-definite} if for each $x \in \R^n$, $x \neq 0$ implies that $$x^TAx > 0$$ 
\end{defn}

\begin{ex}
Let $A \in \MM_{n,n}$. If $A$ is positive-definite, then $A$ is invertible.
\end{ex}

\begin{ex}
Let $A \in \MM_{n,n}$. Then $A$ is invertible iff for each eigenvalue $\lam$ of $A$, $\lam \neq 0$.
\end{ex}

\begin{proof}
Suppose that $A$ is invertible. Let $x \in \R^n$. Suppose that $Ax = 0$. Then $x =0$. So $x$ is not an eigenvector of $A$. So $0$ is not an eigenvalue of $A$. Conversely. Suppose that $A$ is not invertible. Then there exists $x \in \ker A$ such that $x \neq 0$. Then $Ax = 0 = 0x$. So $0$ is an eigenvalue of $A$.  
\end{proof}

\begin{proof}
Suppose that $A$ is positive definite. Let $x \in \ker A$. Suppose that $x \neq 0$. Then $x^TAx = x^T0 = 0$, which is a contradiction. Hence $\ke A = \{0\}$. So $\rank(A) = n$ and $A$ is invertible.
\end{proof}

\begin{ex}
Let $A \in \MM_{n,n}$. If $A$ is positive semi-definite (respectively positive definite), then the eigenvalues of $A$ are nonnegative (respectively positive). 
\end{ex}



\begin{proof}
Let $\lambda \in \C$ be an eigenvalue of $A$ and $x\ in \R^n$ a corresponding eigenvector. Then $x \neq 0$. So $x^Tx \geq 0$. If $A$ is positive semi-definite, then
\begin{align*}
0 
& \leq x^TAx \\
&= \lambda x^T x \\
\end{align*}  
Hence $\lam \geq 0$. The case is similar for $A$ positive definite.  
\end{proof}

\begin{defn}
Let $U \in \MM_{n,n}$. Then $U$ is said to be \textbf{orthogonal} if $U^TU = UU^T = I$
\end{defn}

\begin{thm}
Let $A \in \MM_{n,n}$ be a symmetric matrix and $\lam_1, \cdots, \lambda_n$ the eigenvalues of $A$. Then
\begin{enumerate}
\item $\lam_1, \cdots, \lambda_n \in \R$ 
\item for $i,j \in \{1, \cdots, n\}$ if $i \neq j$ and $x_i$, $x_j$ are eigenvectors corresponding to $\lam_i, \lam_j$ respectively, then $x_i^Tx_j = 0$.
\item there exist $U, D \in \MM_{n,n}$ such that $U$ is orthogonal, $D = \diag(\lam_1, \cdots, \lam_n)$ and  $A = UDU^T$. 
\end{enumerate} 
\end{thm}

\begin{note}
We will be dealing with covariance matrices which are positive semi-definite symmetric matrices and thus have nonnegative eigenvalues.
\end{note}

\section{The Linear Model}
\subsection{Model Description}
\begin{defn}
Given $y \in \R_m$ a vector of observed responses to the matrix $X \in \MM_{m,n}$ of observed inputs, we will consider the model $$y = Xb +e$$ where $b \in \R_n$ is a vector of unknown parameters and $e \in \R^m$ is a random vector of unobserved errors with zero mean. 
\end{defn}

\begin{defn}
For a parameter vector $b \in \R^n$, we have that  $e = y-Xb$. For this reason, $e$ is called the \textbf{residual vector} or simply the ``residuals''.
\end{defn}

\begin{note}
The goal will be to find a parameter vector $b \in \R^n$ that makes the causes the residuals to be as small as possible.  
\end{note}

\subsection{Least Squares Optimization}

\begin{defn}
We define the \textbf{cost function}, $Q: \R^n \rightarrow \R$ by 
\begin{align*}
Q(b) 
&= \n y - Xb \n ^2 \\
&= (y - Xb )^T(y - Xb )
\end{align*}
\end{defn}

\begin{defn}
Let $b \in \R^n$. Then $b$ is said to be a \textbf{least squares solution} for the model if $$Q(b) = \inf_{c \in R^{n}} Q(c)$$ 
\end{defn}

\begin{ex}
If $b$ is a least squares solution for the model, then $X^TXb = X^Ty$.
\end{ex}

\begin{proof}
Suppose that $b$ is a least squares solution for the model, then $Q$ has a global minimum at $b$. Since $Q$ is convex in $b$, this global minimum is also a local minimum. Thus $$\frac{\partial Q}{\partial b}(b) = 0$$ By definition, 
\begin{align*}
Q(b) 
&= y^Ty -y^TXb - b^TX^Ty + b^TX^TXb \\
&= y^Ty -2y^TXb + b^TX^TXb \\ 
\end{align*}
Thus 
\begin{align*}
0
&= \frac{\partial Q}{\partial b}(b) \\
&= -2X^Ty + 2X^TXb
\end{align*}
Hence $X^TXb = X^Ty$.
\end{proof}

\begin{defn}
For $y \in \R^m$ and $X \in \MM_{m,n}$, we define the \textbf{normal equation} to be $$X^TXb = X^Ty$$
\end{defn}

\begin{ex}
The normal equation is consistent.
\end{ex}

\begin{proof}
We have that $X^Ty \in \MC(X^T) = \MC(X^TX)$. 
\end{proof}

\begin{ex}
Let $b \in \R^n$. Then $b$ is a least squares solution for the model iff $b$ satisfies the normal equation.
\end{ex}

\begin{proof}
The previous exercises tells us that if $b$ is a least squares solution for the model, then $b$ satisfies the normal equation. Conversely, suppose that $b$ satisfies the normal equation. Then 
\begin{align*}
Q(c) 
&= (y - Xc )^T(y - Xc ) \\
&= (y - Xb +Xb -Xc )^T(y - Xb +Xb -Xc ) \\
&= (y - Xb)^T (y - Xb) - (y - Xb)^T(X(b-c)) - (b-c)^TX^T(y - Xb) + (b-c)^TX^T(X(b-c)) \\
&= Q(b) -2(b-c)^TX^T(y-Xb) + \n X(b-c) \n^2 \\
&= Q(b)+ \n X(b-c) \n^2 
\end{align*}
Thus $b$ minimizes $Q$.
\end{proof}

\begin{ex}
Let $b \in \R^n$ be a least squares solution for the model. Then $\n y \n^2 = \n Xb \n^2 + \n e \n^2$
\end{ex}

\begin{proof}
Since $b$ satisfies the normal equation, we have that $X^T(y - Xb) = 0$. Thus 
\begin{align*}
Xb \cdot e
&= b^TX^Te \\
&= b^TX^T(y - Xb) \\
&= b^T0 \\
&=0
\end{align*}
So $Xb$ and $e$ are orthogonal. Therefore 
\begin{align*}
\n y\n^2 
&= \n Xb+ e\n^2 \\
&= \n Xb \n^2 + \n e \n^2
\end{align*}
\end{proof}

\subsection{Estimation}
\begin{note}
In what follows we are considering the model $y = Xb +e$ with $y, e \in \R^n$, $b \in \R^p$, $X \in \MM_{n,p}$ and $\E[e] = 0$.
\end{note}

\begin{defn}
Let Then $\lam \in \R^p$. The function $t(y)$ is said to be a linear unbiased estimator for the function $f(b) = \lam^T b$ if there exists $ a \in \R^n$, $c \in \R$ such that $t(y) = c+ a^Ty$ and for each $b \in \R^p$, $\E[t(y)] = \lam^T b$. 
\end{defn}

\begin{ex}
Let Then $\lam \in \R^p$ and $ a \in \R^n$, $c \in \R$. Suppose that $t(y) = c+a^Ty$ is an unbiased linear estimator for $f(b) = \lam^T b$. Then $c = 0$ and $\lam = X^Ta$. 
\end{ex}

\begin{proof}
We have that for each $b \in \R^p$, 
\begin{align*}
\lam^Tb 
&= \E[c+a^Ty] \\
&= c + a^T\E[y] \\
&= c + a^T Xb
\end{align*}
Taking $b = 0$, we get that $c =0$. So for each $b \in \R^p$, $\lam^Tb = a^TXb$. This implies that $\lam^T = a^TX$ and $\lam = X^Ta$.
\end{proof}

\begin{defn}
Let $\lam \in R^p$. Then the function $f(b) = \lam^T b$ is said to be \textbf{linearly estimable} if there exists a linear, unbiased estimator for $f(b)$. Equivalently, $f(b) = \lam^T b$ is linearly estimable if there exists $a \in \R^n$ such that for each $b \in \R^p$ $\E[a^Ty] = \lambda^Tb$
\end{defn}

\begin{ex}
Let $\lam \in \R^p$. Then the following are equivalent: 
\begin{enumerate}
\item $f(b) = \lam^T b$ is linearly estimable
\item $\lam \in \MC(X^T)$
\item for each $G  \in X^-$ of $X$, $\lam^T = \lam^TGX$
\item there exists $G \in X^-$  of $X$ such that $\lam^T = \lam^TGX$
\end{enumerate}
$f(b) = \lam^T b$ is linearly estimable iff $\lam \in \MC(X^T)$. 
\end{ex}

\begin{proof} $(1) \Rightarrow (2)$\\
Suppose that $f(b)$ is linearly estimable. Then there exists $a \in \R^n$ such that for each $b \in \R^p$ $\E[a^Ty] = \lambda^Tb$. Then for each $b \in \R^p$, $$\lambda^Tb = a^T\E[y] = a^TXb$$ Hence $\lam^T = a^TX$ and $X^Ta = \lam$. So $\lam \in \MC(X^T)$. \vspace{4mm} \\
$(2) \Rightarrow (3)$\\ 
Suppose that $\lam \in \MC(X^T)$. Let $G \in X^-$. Then $G^T \in (X^T)^-$ Since $\lam \in \MC(X^T)$, there exists $a \in \R^n$ such that $ X^Ta = \lam$. A previous exercise tells us that there exists $z \in \R^n$ such that $$a = G^T \lam + (I-G^TX^T)z$$
So 
\begin{align*}
\lam 
&= X^Ta \\
&= X^T \big[ G^T \lam + (I-G^TX^T)z \big] \\
&= X^TG^T \lam
\end{align*}
Hence $\lam^T = \lam^T G X$.\vspace{4mm}\\
$(3) \Rightarrow (4)$\\
Trivial.\vspace{4mm}\\
$(4) \Rightarrow (1)$\\
Suppose that there exists $G \in X^-$ such that $\lam^T = \lam^T G X$. Choose $a = G^T\lam \in \R^n$. Let $b \in \R^p$. Then 
\begin{align*}
E[a^Ty] 
&= a^T\E[y] \\
&= \lam^TG\E[y] \\ 
&= \lam^TGXb \\
&= \lam^Tb
\end{align*}
So $f(b) = \lam^Tb$ is linearly estimable.
\end{proof}

\begin{defn}
Let $\hat{b} \in \R^p$ be a least squares solution and $\lam \in \R^n$. Then $\hat{f} = \lam^T \hat{b}$ is said to be a least squares estimator of $f(b) = \lam^T b$. 
\end{defn}

\begin{ex}
Let $\hat{b} \in \R^p$ be a least squares solution and $\lam \in \R^p$. Then $\hat{f} = \lam^T \hat{b}$ is the unique least squares estimator of $f(b) = \lam^Tb$ iff $f(b)$ is linearly estimable. 
\end{ex}

\begin{proof} Suppose that $f(b) = \lam^T b$ is linearly estimable. Then $\lam \in \MC(X^T)$. So there exists $a \in \R^n$ such that $\lam^T = a^TX$. Let $b'$ be a least squares solution. Then there exists $z \in \R^p$ such that $$b' = (X^TX)^-X^Ty + (I-(X^TX)^-(X^TX))z$$ Then 
\begin{align*}
\lam^Tb' 
&= \lam^T\bigg[ (X^TX)^-X^Ty + (I-(X^TX)^-(X^TX))z \bigg] \\
&= a^TX(X^TX)^-X^Ty + a^TX(I-(X^TX)^-X^TX)z \\
&= a^TP_Xy + a^T(X-P_XX)z \\
&= a^TP_Xy
\end{align*} 
In particular, $\lam^Tb' = a^TP_Xy = \lam^T\hat{b}$. \vspace{2mm}\\ Conversely, suppose that $\hat{f} = \lambda^T \hat{b}$ is the unique least squares estimator of $f(b) = \lam^T b$. Then for each $z \in \R^p$, $$\lam^T\hat{b} 
= \lam^T(X^TX)^-X^Ty + \lam^T(I-(X^TX)^-X^TX)z$$
So for each $z \in \R^p$, $$\lam^T(X^TX)^-X^Ty + \lam^T(I-(X^TX)^-X^TX)z = 0$$ and thus $$\lam^T(I-(X^TX)^-X^TX) = 0$$ Therefore $$ \lam^T = \lam^T(X^TX)^-X^TX$$ Transposing both sides, we obtain that $$\lam = X^TX[(X^TX)^-]^T\lam \in \MC(X^T)$$ So $f(b) = \lam^T b$ is linearly estimable
\end{proof}

\begin{ex}
Let $\lam \in \R^p$ and $\hat{b} \in \R^p$ a least squares solution. If $f(b) = \lam^Tb$ is linearly estimable, then the unique least squares estimator $\hat{f} = \lam^T\hat{b}$ of $f(b)$ is a linear unbiased estimator of $f(b)$.
\end{ex}

\begin{proof}
Suppose that $f(b) = \lam^Tb$ is linearly estimable. Then there exists $a \in \R^n$ such that $\lam^T = a^TX$. The previous exercise tells us that $$\lam^T\hat{b} = \lam^T(X^TX)^-X^Ty$$ Thus for each $b \in \R^p$,
\begin{align*}
\E[\lam^T\hat{b}] 
&= \E[\lam^T(X^TX)^-X^Ty] \\
&= \lam^T(X^TX)^-X^T\E[y] \\
&= \lam^T(X^TX)^-X^TXb \\ 
&= a^TX(X^TX)^-X^TXb \\
&= a^TP_XXb \\
&= a^TXb \\
&= \lam^T b
\end{align*}
\end{proof}

\subsection{Imposing Restictions for a Unique Solution}

\begin{defn} Let $X \in \MM_{n,p}$ with $rank(X) = r$, let $s = p - r$ and $C \in \MM_{s, p}$ with $rank(C) = s$ and $\MC(X^T) \cap \MC(C^T) = \{0\}$ and let $y \in \R^n$. We consider the system 
$$\begin{pmatrix}
X^TX \\
C
\end{pmatrix} b = 
\begin{pmatrix}
X^Ty \\
0
\end{pmatrix} $$
or equivalently the system
$$\begin{pmatrix}
X \\
C
\end{pmatrix} b = 
\begin{pmatrix}
P_Xy \\
0
\end{pmatrix} $$
These systems are the \textbf{ restricted normal equations with restrictions $C$}.
\end{defn}

\begin{note}
Requiring $rank(C) = s$ means that the rows of $C$ (i.e. the restrictions) are linearly independent. To have a unique solution to $$\begin{pmatrix}
X \\
C
\end{pmatrix} b = 
\begin{pmatrix}
P_Xy \\
0
\end{pmatrix} $$ we must have $$\MN\big( \begin{pmatrix}
X \\
C
\end{pmatrix}  \big) = \{0\}$$ or equivalently, $$\MC\big( \begin{pmatrix}
X^T & C^T
\end{pmatrix}\big) = \R^p$$
Since $rank(X^T) = rank(X) = r$, we have that $\MC\big( \begin{pmatrix}
X^T & C^T
\end{pmatrix} \big) = \R^p$ iff $\MC(X^T) \cap \MC(C^T) = \{0\}$. 
\end{note}

\begin{ex}
Under the assumptions for the restricted normal equations, the following systems are equivalent:
\begin{enumerate}
\item $$\begin{pmatrix}
X^TX \\
C
\end{pmatrix} b = 
\begin{pmatrix}
X^Ty \\
0
\end{pmatrix} $$
\item $$\begin{pmatrix}
X^TX \\
C^TC
\end{pmatrix} b = 
\begin{pmatrix}
X^Ty \\
0
\end{pmatrix} $$
\item $$(X^TX + C^TC)b = X^Ty$$
\end{enumerate}
\end{ex}

\begin{proof}\
$(1) \Rightarrow (2)$ We need to show that for each $b \in \R^p$ $Cb = 0 $ implies that $C^TCb = 0 $. This is immediate since $\MN(C^TC) = \MN(C)$.\vspace{3mm}\\ 
$(2) \Rightarrow (3)$ Let $b \in \R^p$ be a solution to system (1). Then we have that $$(X^TX + C^TC)b = X^TXb + C^TCb = X^Ty + 0 = X^Ty$$ \vspace{3mm}\\ 
$(3) \Rightarrow (1)$ Suppose that $(X^TX + C^TC)b = X^Ty$. This implies that $C^TCb = X^T(y - Xb)$. So $$C^TCb \in \MC(C^TC) \cap \MC(X^T) =  \MC(C^T) \cap \MC(X^T)= \{0\}$$ Hence $b \in \MN(C^TC) = \MN(C)$. So $Cb = 0$ and $X^TXb =(X^TX+C^TC)b= X^Ty$, orquivalently, $$\begin{pmatrix}
X^TX \\
C
\end{pmatrix} b = 
\begin{pmatrix}
X^Ty \\
0
\end{pmatrix} $$

\end{proof}

\begin{ex}
Under the assumptions for the restricted normal equations, we have the following: 
\begin{enumerate}
\item $X^TX +C^TC$ is invertible
\item $(X^TX +C^TC)^{-1}X^Ty$ is the unique solution to $X^TXb = X^Ty$ and $Cb = 0$.
\item $(X^TX +C^TC)^{-1}$ is a generalized inverse of $X^TX$
\item $C(X^TX +C^TC)^{-1}X^T = 0$
\item $C(X^TX +C^TC)^{-1}C^T = I$
\end{enumerate}
\end{ex}

\begin{proof}\
\begin{enumerate}
\item \begin{align*}
\R^p 
&= \MC\big(\begin{pmatrix}
X^T & C^T
\end{pmatrix}\big) \\
&= \MC\bigg(\begin{pmatrix}
X^T & C^T
\end{pmatrix} \begin{pmatrix}
X \\ C
\end{pmatrix}\bigg) \\
&= \MC(X^TX + C^TC)
\end{align*}
Since $X^TX + C^TC \in \MM_{p,p}$ and $rank(X^TX + C^TC) = p$, we have that $X^TX + C^TC$ is invertible.
\item Put $b = (X^TX +C^TC)^{-1}X^Ty$. Then $(X^TX +C^TC)b = X^Ty$. A previous exercise tells us that $b$ is a solution to the system $$\begin{pmatrix}
X^TX \\
C
\end{pmatrix} b = 
\begin{pmatrix}
X^Ty \\
0
\end{pmatrix} $$ which implies that $X^TXb = X^Ty$ and $Cb=0$.
\item From (2), we know that $$X^TX\big[ (X^TX +C^TC)^{-1}X^Ty\big] = X^Ty$$ Since $y \in \R^n$ is arbitrary, we have $$X^TX(X^TX +C^TC)^{-1}X^T = X^T$$ Multiplying both sides on the right by $X$ tells us that $(X^TX +C^TC)^{-1}$ is a generalized inverse of $X^TX$. 
\item From (2), we know that $$C(X^TX +C^TC)^{-1}X^Ty = 0$$ Since $y \in \R^n$ is arbitrary, $$C(X^TX +C^TC)^{-1}X^T = 0$$ 
\item 
\end{enumerate}
\end{proof}

\subsection{Constrained Parameter Space}

\begin{defn}
Let $P \in \MM_{p, q}$ and $\del \in \R^q$. Suppose that $P$ has full column rank. We define the \textbf{constrained parameter space} $\MT = \{b \in \R^p: P^Tb = \del\}$.
\end{defn}

\begin{note}
Since $P$ has full column rank, $\MC(P^T) = \R^q$ and for each $\del \in \R^q$, $P^Tb = \del$ is consistent. We now fix $P, \del$ so that $\MT$ is fixed.
\end{note}

\begin{defn}
Let $\lam \in \R^p$. The function $t(y)$ is said to be a \textbf{linear unbiased estimator in $\MT$} for $f(b) = \lam^Tb$ if there exists $a \in \R^n$, $c \in \R$ such that $t(y) = c + a^Ty$ and for each $b \in \MT$, $\E[t(y)] = \lam^Tb$
\end{defn}

\begin{defn}
Let $\lam \in \R^p$. The function $f(b) = \lam^Tb$ is said to be \textbf{linearly estimable in $\MT$} if there exists a linear unbiased estimator in $\MT$ for $f(b) = \lam^Tb$. Equivalently $\lam^Tb$ is linearly estimable in $\MT$ if there exist $a \in \R^n$, $c \in \R$ such that for each $b \in \MT$, $\E[c + a^Ty] = \lam^Tb$. 
\end{defn}

\begin{thm}
Let $\lam \in \R^p$ and $a \in \R^n$. Then $t(y) = c+a^Ty$ is a linear unbiased estimator for $f(b) = \lam^Tb$ iff if there exists $d \in \R^q$ such that $\lam = X^Ta + Pd$ and $c = d^T \del$. 
\end{thm}

\begin{defn}
We define the \textbf{normal equations with restrictions $\MT$} to be 
$$\begin{pmatrix}
X^TX & P \\ P^T & 0
\end{pmatrix} 
\begin{pmatrix}
b \\ \theta 
\end{pmatrix} 
=
\begin{pmatrix}
X^Ty \\ \del
\end{pmatrix} $$
\end{defn}

\begin{thm}
We have the following:
\begin{enumerate}
\item The restricted normal equations are consistent. 
\item Let $\hat{b}$ be the first component of a solution to the restricted normal equations. Then $Q(\hat{b}) = \min\limits_{b \in \MT}Q(b)$.
\item Let $\hat{b}$ be the first component of a solution to the restricted normal equations and $b \in \MT$. Then $Q(b) = Q(\hat{b})$ iff $b$ is the first component of a solution of to the restricted normal equations.
\end{enumerate}
\end{thm}

\subsection{The Gauss-Markov Model}
\begin{defn}
Let $X \in \MM_{n,p}$, $y \in \R^n$. We consider the model $y = Xb + e$ where $\E[e] = 0$, $Var(e) = \sig^2I_n$. This model is called the \textbf{Gauss-Markov model}. Note that $E[y] = Xb$ and $Var(y) = \sig^2 I$.
\end{defn}

\begin{thm}
Let $a, c \in \R^n$, $A \in \MM_{p,n}$ and $y$ a random vector in $\R^n$. 
Then 
\begin{enumerate}
\item $\E[a^Ty] = a^T\E[y]$ 
\item $Var(a^Ty) = a^TVar(y)a$
\item $Cov(a^Ty, c^Ty) = a^TVar(y)c$
\item $Var(Ay) = A^TVar(y)A $
\end{enumerate}
\end{thm}

\begin{ex}
Let $\lam^T \in \R^p$ and $\hat{b} \in \R^p$ a least squares solution. Suppose that $f(b) = \lam^Tb$ is linearly estimable. Then the unique least squares estimator $\hat{f} = \lam^T\hat{b}$ satisfies $$Var(\hat{f}) = \sig^2 \lam^T(X^TX)^- \lam$$  
\end{ex}

\begin{proof}
Uniqueness of $\hat{f}$ tells us that $\hat{f} = \lam^T(X^TX)^-X^Ty$. A previous exercise tells us that for each gen. inv. $X^-$ of $X$, $\lam^T = \lam^TX^-X$. Recall that $(X^TX)^-X^T$ is a gen. inv. of $X$. Then 
\begin{align*}
Var(\hat{f}) 
&= Var(\lam^T(X^TX)^-X^Ty) \\
&= \lam^T(X^TX)^-X^T Var(y)(\lam^T(X^TX)^-X^T)^T \\
&= \sig^2\lam^T(X^TX)^-X^T(\lam^T(X^TX)^-X^T)^T \\
&= \sig^2\lam^T(X^TX)^-\bigg(\lam^T(X^TX)^-X^TX\bigg)^T\\
&= \sig^2 \lam^T(X^TX)^- (\lam^T)^T\\
&= \sig^2 \lam^T(X^TX)^-\lam
\end{align*}
\end{proof}

\begin{ex}
Let $\lam \in \R^p$. Suppose that $f(b) = \lam^Tb$ is linearly estimable. Then $\hat{f} = \lam^T\hat{b}$ is the minimum variance linear unbiased estimator for $f(b)$. 
\end{ex}

\begin{proof}
Let $t(y) = c+ a^Ty$ be a linear unbiased estimator for $f(b) = \lam^Tb$. Recall that $c = 0$ and $\lam = X^Ta$, $\hat{f} = \lam^T (X^TX)^-X^Ty$ and for each generalized inverse $X^-$ of $X$, $\lam^TX^-X = \lam^T$. Then 
\begin{align*}
Var(t(y)) 
&= Var(a^Ty) \\
&= Var(\hat{f} + (a^Ty - \hat{f})) \\
&= Var(\hat{f}) + Var(a^Ty - \hat{f}) + 2Cov(\hat{f}, a^Ty - \hat{f}) 
\end{align*}
Now \begin{align*}
Cov(\hat{f}, a^Ty - \hat{f})
&= Cov(\lam^T (X^TX)^-X^Ty, a^Ty - \lam^T (X^TX)^-X^Ty) \\
&= \lam^T (X^TX)^-X^TVar(y)\bigg[a^T - \lam^T (X^TX)^-X^T \bigg]^T \\
&= \sig^2 \lam^T (X^TX)^-X^T\bigg[a^T - \lam^T (X^TX)^-X^T \bigg]^T \\
&=\sig^2 \lam^T (X^TX)^-\bigg[a^TX - \lam^T (X^TX)^-X^TX \bigg]^T \\
&= \sig^2 \lam^T (X^TX)^-\bigg[a^TX - \lam^T (X^TX)^-X^TX \bigg]^T \\
&= \sig^2 \lam^T (X^TX)^-\bigg[a^TX - \lam^T \bigg]^T \\
&= \sig^2 \lam^T (X^TX)^-(X^Ta - \lam) \\
&= 0
\end{align*}
Hence $Var(t(y)) = Var(\hat{f}) + Var(a^Ty - \hat{f}) \geq Var(\hat{f})$
\end{proof}

\begin{thm}\
\begin{enumerate}
\item For each $A,B \in \MM_{n,n}$ and $\al \in \R$, $\trace(\al A+B) = \al \trace(A) + \trace(B)$.
\item For each $A \in \MM_{n,p}$ and $B \in \MM_{p,n}$, $\trace(AB) = \trace(BA)$.
\item For each random matrix $Z \in \MM_{n,n}$, $\E[\trace(Z)] = \trace(\E[Z])$.
\end{enumerate} 
\end{thm}

\begin{ex}
Let $z \in \R^p$ be a random vector. Suppose that $\E[z] = \mu$ and $Var(z) = \Sigma$. Then for each $A \in \MM_{p,p}$, $$\E[z^TAz] = \mu^T A \mu + \trace(A\Sig)$$
\end{ex}

\begin{proof}
Note that $$\E[z^TAz] =  \E[(z-\mu)^TA(z-\mu)] + \E[\mu^TA(z - \mu)] + \E[z^TA\mu] $$
Observe that
\begin{align*}
\E[(z-\mu)^TA(z-\mu)] 
&= \E[\trace((z-\mu)^TA(z-\mu))] \\
&= \E[\trace((A(z-\mu)(z-\mu)^T)] \\
&= \trace(\E[(A(z-\mu)(z-\mu)^T]) \\
&= \trace (A \E[(z-\mu)(z-\mu)^T]) \\
&= \trace (A \Sig) \\
\end{align*}
and that 
\begin{align*}
\E[\mu^TA(z - \mu)]
&= \E[\mu^TA(z-\mu)] \\
&= \mu^TA \E[z-\mu] \\
&= 0 \\
\end{align*}
and that 
\begin{align*}
\E[z^TA\mu]
&= \E[z^T]A \mu \\
&= \mu^T A \mu
\end{align*}
Thus $\E[z^TAz] = \mu^T A \mu + \trace(A\Sig)$.
\end{proof}

\begin{defn}
Put $\hat{e} = y - \hat{y} = (I-P_X)y$. Then the \textbf{sum of squares error}, $SSE$, is defined to be $SSE = \hat{e}^T\hat{e} = y^T(I-P_X)y$.
\end{defn}

\begin{ex}
Let $r = rank(X)$. Define $$\hat{\sig}^2 = \frac{SSE}{n-r}$$ Then $\hat{\sig}^2$ is an unbiased estimator for $\sig^2$. 
\end{ex}

\begin{proof}
The previous exercise tells us that 
\begin{align*}
\E[SSE] 
&= \E[y^T(I-P_X)y] \\
&= b^TX^T(I-P_X)Xb + \sig^2 \trace(I-P_X) \\
&= \sig^2 \trace(I-P_X) \\
&= \sig^2 \rank(I-P_X) \\
&= \sig^2 nullity(X^T) \\
&= \sig^2(n-r)
\end{align*}

So $\E[\hat{\sig}^2] = \sig^2$.
\end{proof}

\subsection{The Aitken Model}

\begin{defn}
Let $X \in \MM_{n,p}$, $y \in \R^n$ and $V \in \MM_{n,n}$. We consider the model $y = Xb + e$ where $\E[e] = 0$, $Var(e) = \sig^2V$. This model is called the \textbf{Aitken model}. Note that $E[y] = Xb$ and $Var(y) = \sig^2 V$.
\end{defn}

\begin{defn}
Let $R \in \MM_{n,n}$. Suppose that $R$ is invertible and $RVR^T = I$ or equivalently, $V = (R^TR)^{-1}$. We define the \textbf{transformed Aitken model} by $z = Ry$, $U = RX$, $f = Re$ so that $$z = Ub + f$$ Note that $$E[z] = RXb = Ub$$ and $$Var(f) = RVar(e)R^T = \sig^2RVR^T = \sig^2I$$ 
\end{defn}

\begin{defn}
Under the transformed Aitken model, we can can look for solutions $b \in \R^p$ to the normal equations $$U^TUb = U^Tz$$ When we transform back to the Aitken model, we have the \textbf{Aitken equations} $$X^TV^{-1}Xb = X^TV^{-1}y$$ We denote a solution to the Aitken equations by $\hat{b}_{GLS}$ and a solution to the normal equations by $\hat{b}_{OLS}$
\end{defn}

\section{Distribution Theory}

\subsection{Introduction}

\begin{defn}
Let $x \in \R^p$ be a random vector. Define $m_x: \R^p \rightarrow \RG$ by $$m_x(t) = E[e^{t^Tx}]$$ We call $m_X$ the \textbf{moment generating function of X}.
\end{defn}

\begin{thm}
Let $x_1, x_2 \in \R^p$ be random vectors. Then $F_{x_1} = F_{x_2}$ iff $m_{x_1} = m_{x_2}$.
\end{thm}

\begin{thm}
Let $x_1 \in \R^{p_1}, \cdots, x_n \in \R^{p_n}$ be random vectors. Put $p = \sum\limits_{i=1}^n p_i$. For $t \in \R^{p}$, we can partition $t$ as $t = (t_1^T, \cdots, t_n^T)^T$ where $t_1 \in \R^{p_1}, \cdots, t_n \in \R^{p_n}$. Put $x = (x_1^T, \cdots, x_n^T)^T$. Then $x_1, \cdots, x_n$ are independent iff for each $t \in \R^p$, $m_x(t) = \prod\limits_{i=1}^nm_{x_i}(t_i)$. 
\end{thm}

\subsection{Multivariate Normal}



\begin{defn}
Let $x \in \R^p$ be a random vector, $\mu \in \R^p$ and $V \in \MM_{p, p}$ be symmetric and positive semi-definite. Then $x$ is said to have a \textbf{multivariate normal distribution with mean $\mu$ and covariance matrix $V$}, denoted $x \sim N_{p}(\mu, V)$, if for each $t \in \R^p$, $m_x(t) = e^{t^T\mu + \frac{1}{2} t^T V t}$.
\end{defn}

\begin{ex}
Let $x \sim N_p(\mu, V)$, $a \in \R^q$ and $B \in \MM_{q, p}$. Define the random vector $y \in \R^q$ by  $y = a +Bx$. Then $y \sim N_q(a + B\mu, BVB^T)$. 
\end{ex}

\begin{proof} 
for $t \in \R^q$, we have that
\begin{align*}
m_y(t)
&= E[e^{t^Ty}] \\
&= E[e^{t^T(a+Bx)}] \\
&= E[e^{t^Ta} + t^TBx] \\
&= e^{t^Ta}E[e^{t^TBx}] \\
&= e^{t^Ta}m_x(B^Tt) \\
&= e^{t^Ta}e^{t^TB\mu + \frac{1}{2} t^TBVB^Tt} \\
&= e^{t^Ta + t^TB\mu + \frac{1}{2} t^TBVB^Tt} \\
&= e^{t^T(a + B\mu) + \frac{1}{2} t^TBVB^Tt} \\
\end{align*}
So $y \sim N_q(a+B\mu, BVB^T)$.
\end{proof}

\begin{ex}
Let $x \in \R^p$ be a multivariate normal random vector. Then any subvector of $x$ is a multivariate normal random vector.
\end{ex}

\begin{proof}
Let $x = (x_1, \cdots, x_p)^T$. Suppose that $x \sim N_p(\mu, V)$. Let $x' = (x_{i_1}, \cdots, x_{i_k})$ be a subvector of $x$. So $i_1 < \cdots < i_q$ and $i_1, \cdots, i_q\in \{1, \cdots, p\}$. Choose a matrix $B \in \MM_{q,p}$ such that $x' = Bx$. Then $x' \sim N_q(B\mu, BVB^T)$.
\end{proof}

\begin{thm}
Let $x = (x_1^T, \cdots, x_n^T)^T$, $\mu = (\mu_1^T \cdots, \mu_n^T)^T \in \R^p$ and $V = 
\begin{pmatrix}
V_{1,1} & \cdots & V_{1,n} \\
  & \vdots&  \\
V_{n,1} & \cdots & V_{n,n} 
\end{pmatrix} \in \MM_{p,p}$ 
where $x_i, \mu_i \in \R^{p_i}$, $V_{i,j} \in \MM{p_i, p_j}$ and $\sum\limits_{i=1}^n p_i = p$. If $x \sim N_{p}(\mu, V)$ then $x_1, \cdots, x_n$ are independent iff for each $i,j \in \{1, \cdots, n\}$, $i \neq j$ implies that $V_{i,j} = 0$.
\end{thm}

\begin{ex}
Let $z = (z_1, \cdots, z_p) \in \R^p$ be a random vector. Then $z \sim N_p(0,I)$ iff for each $i = 1, \cdots, p$, $z_i \sim N_1(0, 1)$ and $z_1, \cdots, z_n$ are independent.
\end{ex}

\begin{proof}
Suppose that $z \sim N_p(0,I)$. Since $z_i = e_i^Tz$, the previous results tells us that for each $i = 1, \cdots, p$, $z_i \sim N_1(0, 1)$ and $z_1, \cdots, z_n$ are independent. Conversely, suppose that for each $i = 1, \cdots, p$, $z_i \sim N_1(0, 1)$ and $z_1, \cdots, z_n$ are independent. Then for each $t \in \R^p$, 
\begin{align*}
m_z(t) 
&= \prod_{i=1}^pm_{z_i}(t_i) \\
&= \prod_{i=1}^p e^{\frac{1}{2}t_i^2} \\
&= e^{\frac{1}{2}t^Tt}
\end{align*}
Thus $z \sim N_p(0, I)$.
\end{proof}

\begin{ex}
Let $x \sim N_p(\mu, V), a1, a_2 \in \R^q$, $B_1, B_2 \in \MM_{q,p}, y_1 = a_1 +B_1x$ and $y_2 = a_2 + B_2x$. Then $y_1, y_2$ are independent iff $B_1VB_2^T = 0$. 
\end{ex}

\begin{proof}
Put $y = 
\begin{pmatrix}
y_1 \\ y_2
\end{pmatrix}
$, $a = 
\begin{pmatrix}
a_1 \\ a_2
\end{pmatrix}$ 
and $B = 
\begin{pmatrix}
B_1 \\B_2
\end{pmatrix}$. Since 
$y
= 
a
+
B
x$
We know that $y \sim N_{2q}(a+B\mu, BVB^T)$. Observe that $BVB^T = 
\begin{pmatrix}
B_1VB_1^T & B_1VB_2^T \\
B_2VB_1^T & B_2VB_2^T
\end{pmatrix}
$ A previous result tells us that $y_1, y_2$ are independent iff $B_1VB_2^T = 0$
\end{proof}


\subsection{Chi-Square}
\begin{defn} Let $u$ be a random variable, $p \in \N$ and $\phi \geq 0$. Then $u$ is said to have a \textbf{$\chi^2$ distribution with $p$ degrees of freedom and noncentrality parameter $\phi$}, denoted $u \sim \chi^2_p(\phi)$, if for each $t \in \R$, $$m_u(t) = (1-2t)^{p/2}e^{2\phi t/(1-2t)}$$ If $\phi=0$, we say that $u$ has a (central) chi-square distribution with $p$ degrees of freedom, denoted $u \sim \chi^2_p$.
\end{defn}

\begin{ex}
Let $z \sim N_p(0,I)$. Then $z^Tz \sim \chi^2_p$.
\end{ex}

\begin{proof}
One can show that $m_{z_i^2}(t) = (1-2t)^{1/2}$. By independence $$m_{z^Tz}(t) = m_{\sum_{i=1}^p z_i^2}(t) = (1-2t)^{p/2}$$
\end{proof}

\begin{ex}
Let $u_1 \sim \chi^2_{p_1}(\phi_1), \cdots, u_n \sim \chi^2_{p_n}(\phi_n)$. Put $u = \sum\limits_{i=1}^n u_i$, $p = \sum\limits_{i=1}^np_i$ and $\phi = \sum\limits_{i=1}^n\phi_i$. If $u_1, \cdots, u_n$ are independent, then $u \sim \chi^2_p(\phi)$. 
\end{ex}

\begin{proof}
By independence, we have that for each $t \in \R$, 
\begin{align*}
m_u(t) 
&= \prod\limits_{i=1}^nm_{u_i}(t) \\
&= \prod\limits_{i=1}^n (1-2t)^{p_i/2}e^{2\phi_i t/(1-2t)} \\
&= (1-2t)^{p/2}e^{2\phi t/(1-2t)} 
\end{align*}
\end{proof}

\begin{thm}
Let $x \in \R^p$ be a random vector, $\mu \in \R^p$ and $V \in \MM_{p,p}$ positive definite. If $x \sim N_p(\mu, V)$, then $$x^TV^{-1}x \sim \chi^2_p(\mu^TV^{-1}\mu /2)$$
\end{thm}

\begin{defn}
Let $p_1, p_2 \in \N$, $\phi \geq 0$, $u_1 \sim \chi^2_{p_1}(\phi)$ and $u_2 \sim \chi^2_{p_2}$. Then $\frac{u_1/p_1}{u_2/p_2}$ is said to have a \textbf{$F$ distribution with noncentrality parameter $\phi$ and $p_1, p_2$ degrees of freedom}, denoted $\frac{u_1/p_1}{u_2/p_2} \sim F_{p_1, p_2}(\phi)$. 
\end{defn}

\begin{defn}
Let $\mu \in \R$, $k \in \N$, $u \sim N_1(\mu, 1)$ and $v \sim \chi^2_k$. Then $u/\sqrt{v/k}$ is said to have a \textbf{$t$ distribution with noncentrality parameter $\mu$ and $k$ degrees of freedom}, denoted $u/\sqrt{v/k} \sim t_k(\mu)$. 
\end{defn}

\subsection{Quadratic Forms}

\begin{ex}
Let $\mu \in \R^p$ and $A, V \in \MM_{p,p}$. Suppose that $A$ is symmetric, $V$ is nonsingular, $AV$ is idempotent and $rank(AV) = s$. Let $x \sim N_p(\mu, V)$. Then $x^TAx \sim \chi^2_s(\mu A \mu^T/2)$
\end{ex}

\begin{ex}

\end{ex}

























\end{document}
