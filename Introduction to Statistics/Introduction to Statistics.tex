%% filename: amsbook-template.tex
%% version: 1.1
%% date: 2014/07/24
%%
%% American Mathematical Society
%% Technical Support
%% Publications Technical Group
%% 201 Charles Street
%% Providence, RI 02904
%% USA
%% tel: (401) 455-4080
%%      (800) 321-4267 (USA and Canada only)
%% fax: (401) 331-3842
%% email: tech-support@ams.org
%% 
%% Copyright 2006, 2008-2010, 2014 American Mathematical Society.
%% 
%% This work may be distributed and/or modified under the
%% conditions of the LaTeX Project Public License, either version 1.3c
%% of this license or (at your option) any later version.
%% The latest version of this license is in
%%   http://www.latex-project.org/lppl.txt
%% and version 1.3c or later is part of all distributions of LaTeX
%% version 2005/12/01 or later.
%% 
%% This work has the LPPL maintenance status `maintained'.
%% 
%% The Current Maintainer of this work is the American Mathematical
%% Society.
%%
%% ====================================================================

%    AMS-LaTeX v.2 driver file template for use with amsbook
%
%    Remove any commented or uncommented macros you do not use.

\documentclass{book}

%    For use when working on individual chapters
%\includeonly{}

\input{"../Book Preamble/Book Preamble.tex"}


% Glossary - Notation
\glsxtrnewsymbol[description={finite measures on $(X, \MA)$}]{n000001}{$\MM_+(X, \MA)$}
\glsxtrnewsymbol[description={velocity}]{v}{\ensuremath{v}}


\makeindex

\begin{document}
	
	\frontmatter
	
	\title{Introduction to Statistics}
	
	%    Remove any unused author tags.
	
	%    author one information
	\author{Carson James}
	\thanks{}
	
	\date{}
	
	\maketitle
	
	%    Dedication.  If the dedication is longer than a line or two,
	%    remove the centering instructions and the line break.
	%\cleardoublepage
	%\thispagestyle{empty}
	%\vspace*{13.5pc}
	%\begin{center}
	%  Dedication text (use \\[2pt] for line break if necessary)
	%\end{center}
	%\cleardoublepage
	
	%    Change page number to 6 if a dedication is present.
	\setcounter{page}{4}
	
	\tableofcontents
	\printunsrtglossary[type=symbols,style=long,title={Notation}]
	
	%    Include unnumbered chapters (preface, acknowledgments, etc.) here.
	%\include{}
	
	\mainmatter
	% Include main chapters here.
	%\include{}
	
	\chapter*{Preface}
	\addcontentsline{toc}{chapter}{Preface}
	
	\begin{flushleft}
		\href{https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode.txt}{cc-by-nc-sa}
	\end{flushleft}



































	
	\section{Introduction}
	\begin{defn}
		Let $A \in \MB(R^d)$ and $\Theta \neq \varnothing$. Suppose that $m(A) > 0$. We define 
		$$\MD(A) = \{f \in L^1(A) : f \geq 0 \text{ and } \|f\|_1 = 1\}$$ 
		and for $\theta \in \Theta$, we define
		$$\MD(A|\theta) = \{f: A \times \Theta \rightarrow \R : f(\cdot| \theta) \in \MD(A)\}$$
	\end{defn}
	
	
	
	
	
	
	\newpage
	\section{Sampling}
	
	\subsection{Inverse CDF Sampling}
	
	
	













	\newpage
	
	\subsection{Importance Sampling}
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	\newpage
	\subsection{Rejection Sampling}
	
	\begin{ex}
		Let $f, g \in \MD(\R^d)$ and $A \in \MB(\R^d)$. Suppose that $m^d(A) > 0$. If $X \sim f$, then $X|X \in A \sim \|fI_A\|_1^{-1}fI_A$. 
	\end{ex}

	\begin{proof}
		Let $C \in \MB(\R^d)$. Then
		\begin{align*}
			P(X \in C|X \in A)
			&= P(X \in C \cap A) P(X \in A)^{-1} \\
			&= \|fI_A\|_1^{-1} \int_C fI_A dm^d \\
		\end{align*}
		So $f_{X|X \in A} = \|fI_A\|_1^{-1}fI_A$.
	\end{proof}
	
	\begin{ex}
		Let $A, B \in \MB(\R^d)$. Suppose that $A \subset B$ and $0 < m^d(A)$ and $ m^{d}(B) < \infty$. If $X \sim \uni(B)$, then $X|X \in A \sim \uni(A)$. 
	\end{ex}

	\begin{proof}
		Clear using the previous exercise with $f = I_B$.
	\end{proof}
	
	\begin{ex}\textbf{(Fundamental Theorem of Simulation):} \\
		Let $f \in \MD(\R^d)$ and $c > 0$. Define $$G_c = \{(x,v) \in \R^{d+1}: 0< v < cf(x)\}$$ 
		\begin{enumerate}
			\item If $X \sim f$ and $U \sim \uni(0,1)$  are independent, then $(X, cUf(X)) \sim \uni(G_c)$.
			\item If $(X, V) \sim \uni(G_c)$, then $X \sim f$.
		\end{enumerate}
	\end{ex}

	\begin{proof} First we note that $m^{d+1}(G_c) = c$. 
		\begin{enumerate}
			\item Suppose that $X \sim f$ and $U \sim \uni(0,1)$ are independent and put $Y = cUf(X)$. Then $Y| X= x \sim cUf(x) \sim \uni(0, cf(x))$ and we have that for each $x \in \supp X$ and $y \in (0, cf(x))$,
			\begin{align*}
				f_{X, Y}(x,y) 
				&= f_{Y|X}(y| x) f(x) \\
				&= \frac{1}{c f(x)}f(x) \\
				&= \frac{1}{c}
			\end{align*}
			So $(X, Y) \sim \uni(G_c)$\\
			\item Suppose that $(X, V) \sim \uni(G_c)$. Then $f_{X,V}(x,v) = \frac{1}{c} I_{G_c}(x,v)$. So 
			\begin{align*}
				f_X(x) 
				&= \int_{\R} \frac{1}{c}I_{G_c}(x,v) dm(v) \\ 
				&= \int_{0}^{cf(x)} \frac{1}{c}dv \\
				&= f(x) 
			\end{align*}
			So $X \sim f$.
		\end{enumerate}
	\end{proof}

	\begin{ex}
		Let $f, g \in \MD(\R^d)$, $c_f,c_g>0$ and $M > 0$. Put $\tilde{f} = c_f f$ and $\tilde{g} = c_g g$. Suppose that $\tilde{f} \leq M \tilde{g}$. If $Y \sim g$ and $U \sim \uni(0,1)$ are independent, then $Y|U \leq \frac{\tilde{f}(Y)}{M\tilde{g}(Y)} \sim f$ and $P ( U \leq \frac{\tilde{f}(Y)}{M\tilde{g}(Y)} ) = \frac{c_f}{c_gM}$ 
	\end{ex}

	\begin{proof}
		Put $$G_g = \{(y,v) \in \R^{d+1}: 0< v < M\tilde{g}(y)\} $$ and $$ G_f = \{(y,v) \in \R^{d+1}: 0< v < \tilde{f}(y)\} $$ 
		Then $G_f \subset G_g$, $m^d(G_g) = c_gM$ and $m^d(G_f) =c_f$. By the first part of the fundamental theorem of simulation, we know that $$(Y, MUc_gg(Y)) \sim \uni(G_g)$$ 
		Since $\{(Y, MUc_gg(Y)) \in G_f \} = \{U \leq \frac{c_ff(Y)}{Mc_gg(Y)}\}$, a previous exercise tells us that $$(Y, MUc_gg(Y))|U \leq \frac{c_ff(Y)}{Mc_gg(Y)} \sim \uni(G_f)$$
		Then the second part of the fundamental theorem of simulation tells us that $$Y|U \leq \frac{c_ff(Y)}{Mc_gg(Y)} \sim f$$
		Finally we have that
		\begin{align*}
			P \bigg( U \leq \frac{c_ff(Y)}{Mc_gg(Y)} \bigg) 
			&= P [(Y, MUc_gg(Y)) \in G_f ]\\
			&= \frac{c_f}{c_gM}
		\end{align*}
	\end{proof}

	\begin{defn}\textbf{(Rejection Sampling Algorithm):} \\
		Let $f, g \in \MD(\R^d)$, $c_f,c_g>0$ and $M > 0$. Put $\tilde{f} = c_f f$ and $\tilde{g} = c_g g$. Suppose that $\tilde{f} \leq M \tilde{g}$. We define the \textbf{rejection sampling algorithm} as follows:
		\begin{enumerate}
			\item sample $Y \sim g$ and $U \sim \uni(0,1)$ independently
			\item if $U \leq \frac{\tilde{f}(Y)}{M\tilde{g}(Y)} $, accept $Y$, else return to $(1)$.
		\end{enumerate}
		If we sample $(X_n)_{n \in \N}$ independently using the rejection sampler, then the previous exercises imply that $(X_n)_{n \in \N} \iid f$ and the acceptance rate is $\frac{c_f}{c_gM}$.
	\end{defn}

	\begin{note}
		Phrasing the rejection sampler in terms of $\tilde{f}$ and $\tilde{g}$ instead of $f$ and $g$ is usefule because we may not always be able to solve for the normalizing constants.
	\end{note}
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
		\newpage
	\section{Decision Theory}
	
	\subsection{Introduction}
	
	\begin{note} We employ the following notation and conventions:
		\begin{itemize}
			\item data space: a measurable space $(\MX, \MF_{\MX})$
			\item parameter space: a measurable space $(\Theta, \MF_{\Theta})$
			\item distribution familiy: $(P_{\theta})_{\theta \in \Theta} \subset \MP(\MX, \MF_{\MX})$
			\item estimation space: a measurable space $(\ME, \MF_{\ME})$
		\end{itemize}
	\end{note}

	\begin{defn}
		Let $\eta: \Theta \rightarrow \ME$. Then $\eta$ is said to be an \textbf{estimand} if $\eta$ is $(\MF_{\Theta}, \MF_{\ME})$-measurable. 
	\end{defn}
	
	\begin{defn}
		Let $\eta: \Theta \rightarrow \ME$ be an estimand and $\del: \MX \rightarrow \ME$. Then $\del$ is said to be an \textbf{estimator of $\eta$} if $\del$ is $(\MF_{\MX}, \MF_{\ME})$-measurable. We denote the set of estimators for $\eta$ by $\Del_{\eta}$.
	\end{defn}
	
	\begin{defn}
		Let $\eta: \Theta \rightarrow \ME$ be an estimand and $L: \Theta \times \ME \rightarrow \Rg$. Then $L$ is said to be a \textbf{loss function for $\eta$} if 
		\begin{enumerate}
			\item $L(\theta, \cdot)$ is $(\MF_{\ME}, \MB(\R))$-measurable
			\item for each $\theta \in \Theta$, $L(\theta, \eta(\theta)) = 0 $
		\end{enumerate}
	\end{defn}
	
	\begin{defn}
		Let $\eta: \Theta \rightarrow \ME$ be an estimand and $L: \Theta \times \ME \rightarrow \Rg$ be a loss function for $\eta$. We define the \textbf{risk function associated to $L$}, denoted $R_L: \Theta \times \Del_{\eta} \rightarrow \Rg$, by 
		\begin{align*}
			R_L(\theta, \del) 
			& = \int_{\MX} L(\theta, \del(x)) \dP_{\theta}(x)
		\end{align*}
	\end{defn}
	
	\begin{defn}
		Let $\eta: \Theta \rightarrow \ME$ be an estimand, $L: \Theta \times \ME \rightarrow \Rg$ be a loss function for $\eta$ and $\Pi \in \MP(\Theta, \MF_{\Theta})$. 
	\end{defn}
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	\subsection{Bayes Risk}
	
	\begin{defn}
		Let $\eta: \Theta \rightarrow \ME$ be an estimand, $L: \Theta \times \ME \rightarrow \Rg$ be a loss function for $\eta$ and $\Pi \in \MP(\Theta, \MF_{\Theta})$. We define the \textbf{Bayes risk for $L$ and $\Pi$}, denoted $r_{L, \Pi}: \Del_{\eta} \rightarrow \Rg$, by 
		$$r_{L, \Pi}(\del) = \int_{\Theta} R_L(\theta, \del) \, d \Pi(\theta)$$
	\end{defn}

	\begin{defn}
		Let $\eta: \Theta \rightarrow \ME$ be an estimand, $L: \Theta \times \ME \rightarrow \Rg$ be a loss function for $\eta$, $\Pi \in \MP(\Theta, \MF_{\Theta})$ and $\del^* \in \Del_{\eta}$. Then $\del^*$ is said to be a \textbf{Bayes estimator for $L$ and $\Pi$} if 
		$$r_{L, \Pi}(\del^*) = \inf_{\del \in \Del_{\eta}}r_{L, \Pi}(\del)$$
	\end{defn}
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	\newpage
	\subsection{Minimax Estimation}
	
	\begin{defn}
		Let $\eta: \Theta \rightarrow \ME$ be an estimand, $L: \Theta \times \ME \rightarrow \Rg$ be a loss function for $\eta$ and $\del^* \in \Del_{\eta}$. Then $\del^*$ is said to be a \textbf{minimax estimator for $\eta$ and $L$} if 
		$$\sup_{\theta \in \Theta}R(\theta, \del^*) = \inf_{\del \in \Del_{\eta}} \sup_{\theta \in \Theta}R(\theta, \del)  $$
	\end{defn}
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	\newpage
	\section{Posterior Consistency}
	
	\subsection{Introduction}
	
	\begin{defn}
	Let $(\MX, \MF)$ and $\Theta$ be
	\end{defn}	






















	\newpage
	\section{Frechet Mean}
	
	\subsection{Introduction}
	
	\begin{defn}
		Let $(\MM, d)$ be a metric space and $\mu \in M_+(\MM)$. Suppose that $d \in L^2(\mu)$. 
		\begin{itemize}
			\item We define the \tbf{Frechet function of $\mu$}, denoted $F^d_{\mu}: \MM \rightarrow \R$ by 
			$$F_{\mu}^d(x) \defeq \int_{\MM} d(x, y)^2 \dmu(y).$$
			\item We define the \tbf{Fechet mean set of $\mu$ with respect to $d$}, denoted $\MF(\mu, d) \subset \MM$, by $\MF(\mu, d) \defeq \argmin\limits_{x \in \MM} F^d_{\mu}(x)$.  
			\item Let $x_0 \in \MM$. Then $x_0$ is said to be a \tbf{Frechet mean of $\mu$ with respect to $d$} if $x_0 \in \MF(\mu, d)$. 
		\end{itemize}
	\end{defn}	

	\begin{defn}
		Let $(\MM, d)$ be a metric space and $\mu \in M_1(\MM)$. Suppose that $d \in L^2(\mu)$. Let $(x_n)_{n \in \N} \subset \MM$. Let $n \in \N$. Define $\mu_n \in M_1(\MM)$ by $\mu_n \defeq \frac{1}{n} \sum\limits_{j=1}^n \del_{x_j}$.
		\begin{itemize}
			\item We define the \tbf{$n$-th sample Frechet function of $\mu_n$}, denoted $F^d_{\mu_n}: \MM \rightarrow \R$ by 
			$$ F^d_{\mu_n}(x) \defeq \int_{\MM} d(x, y)^2 \dmu_n(y).$$
			\item Let $x_0 \in \MM$. Then $x_0$ is said to be an \tbf{$n$-th sample Frechet mean of $(x_j)_{j \in \N}$ with respect to $d$} if $x_0 \in \MF(\mu_n, d)$. 
		\end{itemize}
	\end{defn}

	\begin{note}
		We recall the projection map $\pi: U(n,k) \rightarrow U(n,k) / U(k)$ that \tcr{make and cite exercise in section about steifel and grassmann manifolds} implies that $f: U(n,k) \rightarrow G(n,k)$ defined by and $f(U) \defeq UU^*$ is a quotient map and $\Im f \cong U(n,k) / U(k)$.  
	\end{note}

	\begin{ex}
		Let $(\Sig_j)_{j \in \N} \subset G(n,k)$. Let $N \in \N$. Define $\mu^{(N)} \in M_1(G(n,k))$ by $\mu^{(N)} \defeq \frac{1}{N} \sum\limits_{j =1}^N \del_{\Sig_j}$. Define $\bar{\Sig}^{(N)} \in G(n,k)$ by $\bar{\Sig}^{(N)} \defeq \frac{1}{N} \sum\limits_{j=1}^N \Sig_j$. There exists $\bar{V}^{(N)} \in U(n)$ and $\bar{\Lam}^{(N)} \in S(n)$ such that $\bar{\Lam}^{(N)}$ is diagonal and $\bar{\Sig}^{(N)} = \bar{V}^{(N)} \bar{\Lam}^{(N)} (\bar{V}^{(N)})^*$. Suppose that $\bar{\Lam}^{(N)} = \diag(\bar{\lam}^{(N)}_1, \ldots, \bar{\lam}^{(N)}_n)$ and $\bar{\lam}^{(N)}_1 \geq \cdots \geq \bar{\lam}^{(N)}_1 \geq 0$. Define $U^{(N)} \in U(n,k)$ and $\Sig^{(N)} \in G(n,k)$ by $U^{(N)} \defeq \bar{V}^{(N)} I_{n,k}$ and $\Sig^{(N)} \defeq U^{(N)}(U^{(N)})^*$. Then $\Sig^{(N)} \in \MF(\mu^{(N)}, d_{G(n, k)})$. 
	\end{ex}

	\begin{proof}
		We note that for each $\Sig \in G(n,k)$,
		\begin{align*}
			F^{d_{G(n,k)}}_{\mu^{(N)}}(\Sig)
			& = \int_{\MM} d_{G(n,k)}(\Sig, \Pi)^2 \dmu^{(N)}(\Pi) \\
			& = \frac{1}{N} \sum_{j =1}^N d_{G(n,k)}(\Sig, \Sig_j)^2 \\
			& = \frac{1}{N} \sum_{j =1}^N \| \Sig - \Sig_j \|_F^2 \\
			& = \frac{1}{N} \sum_{j =1}^N \| (\Sig - \bar{\Sig}^{(N)}) + (\bar{\Sig}^{(N)} - \Sig_j) \|_F^2 \\
			& = \frac{1}{N} \sum_{j =1}^N \bigg[ \| \Sig - \bar{\Sig}^{(N)} \|_F^2 + \| \bar{\Sig}^{(N)} - \Sig_j \|_F^2 + 2\tr[(\Sig - \bar{\Sig}^{(N)})^*(\bar{\Sig}^{(N)} - \Sig_j)] \bigg] \\
			& = \frac{1}{N} \sum_{j =1}^N \bigg[ \| \Sig - \bar{\Sig}^{(N)} \|_F^2 + \| \bar{\Sig}^{(N)} - \Sig_j \|_F^2 + 2\tr[ \Sig^*\bar{\Sig}^{(N)} - \Sig^* \Sig_j + (\bar{\Sig}^{(N)})^*\bar{\Sig}^{(N)} - (\bar{\Sig}^{(N)})^*\Sig_j ] \bigg] \\
			& = \frac{1}{N}  \sum_{j =1}^N \bigg[ \| \Sig - \bar{\Sig}^{(N)} \|_F^2 + \| \bar{\Sig}^{(N)} - \Sig_j \|_F^2 \bigg] + 2\tr[ \Sig^*\bar{\Sig}^{(N)} -  \Sig^*\bar{\Sig}^{(N)} + (\bar{\Sig}^{(N)})^*\bar{\Sig}^{(N)} - (\bar{\Sig}^{(N)})^*\bar{\Sig}^{(N)}] \\
			& = \frac{1}{N}  \sum_{j =1}^N \bigg[ \| \Sig - \bar{\Sig}^{(N)} \|_F^2 + \| \bar{\Sig}^{(N)} - \Sig_j \|_F^2 \bigg] \\
			& = \frac{1}{N} \sum_{j =1}^N  \| \Sig - \bar{\Sig}^{(N)} \|_F^2 + \frac{1}{N} \sum_{j =1}^N  \| \bar{\Sig}^{(N)} - \Sig_j \|_F^2  \\
			& = \| \Sig - \bar{\Sig}^{(N)} \|_F^2 + \frac{1}{N} \sum_{j =1}^N  \| \bar{\Sig}^{(N)} - \Sig_j \|_F^2  \\
			& = \bigg[ \| \Sig \|_F^2 + \| \bar{\Sig}^{(N)} \|_F^2 + 2 \tr(\Sig^*\bar{\Sig}^{(N)}) \bigg] + \frac{1}{N} \sum_{j =1}^N  \| \bar{\Sig}^{(N)} - \Sig_j \|_F^2 \\
			& = k + \| \bar{\Sig}^{(N)} \|_F^2 + 2 \tr(\Sig^*\bar{\Sig}^{(N)}) + \frac{1}{N} \sum_{j =1}^N  \| \bar{\Sig}^{(N)} - \Sig_j \|_F^2.
		\end{align*}
		Then
		\begin{align*}
			\argmax_{\Sig \in G(n,k)} F^{d_{G(n,k)}}_{\mu^{(N)}}(\Sig)
			& = \argmax_{\Sig \in G(n,k)} \bigg[ k + \| \bar{\Sig}^{(N)} \|_F^2 + 2 \tr(\Sig^*\bar{\Sig}^{(N)}) + \frac{1}{N} \sum_{j =1}^N  \| \bar{\Sig}^{(N)} - \Sig_j \|_F^2 \bigg] \\
			& = \argmax_{\Sig \in G(n,k)} \tr(\Sig^*\bar{\Sig}^{(N)}).
		\end{align*}
		Since 
		\begin{align*}
			\max_{\Sig \in G(n,k)} \tr(\Sig^*\bar{\Sig}^{(N)})
			& = \max_{U \in U(n,k)} \tr(UU^*\bar{\Sig}^{(N)}),
		\end{align*}
		\tcr{an exercise in the analysis notes section on optimizing over compact groups} implies that 
		\begin{align*}
			\Sig^{(N)} 
			& \in \argmax_{\Sig \in G(n,k)} \tr(\Sig^*\bar{\Sig}^{(N)}) \\
			& = \argmax_{\Sig \in G(n,k)} F^{d_{G(n,k)}}_{\mu^{(N)}}(\Sig) \\
			& = \MF(\mu^{(N)}, d_{G(n,k)}).
		\end{align*}
	\end{proof}

	

	
\end{document}