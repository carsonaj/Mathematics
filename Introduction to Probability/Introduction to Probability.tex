\documentclass[12pt]{amsart}
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb,setspace, mathtools}
\usepackage{color}   %May be necessary if you want to color links
\usepackage{hyperref}
\hypersetup{
	colorlinks=true, %set true if you want colored links
	linktoc=all,     %set to all if you want both sections and subsections linked
	linkcolor=black,  %choose some color if you want links to stand out
	urlcolor=cyan
}


%
%
%
\newif\ifhideproofs
%\hideproofstrue %uncomment to hide proofs
%
%
%
%
\ifhideproofs
\usepackage{environ}
\NewEnviron{hide}{}
\let\proof\hide
\let\endproof\endhide
\fi

\theoremstyle{definition}
\newtheorem{definition}{Definition}[subsection]
\newtheorem{defn}[definition]{Definition}
\newtheorem{note}[definition]{Note}
\newtheorem{thm}[definition]{Theorem}
\newtheorem{lem}[definition]{Lemma}
\newtheorem{prop}[definition]{Proposition}
\newtheorem{cor}[definition]{Corollary}
\newtheorem{conj}[definition]{Conjecture}
\newtheorem{ex}[definition]{Exercise}






\newcommand{\al}{\alpha}
\newcommand{\Gam}{\Gamma}
\newcommand{\be}{\beta} 
\newcommand{\del}{\delta} 
\newcommand{\Del}{\Delta}
\newcommand{\lam}{\lambda}  
\newcommand{\Lam}{\Lambda} 
\newcommand{\ep}{\epsilon}
\newcommand{\sig}{\sigma} 
\newcommand{\om}{\omega}
\newcommand{\Om}{\Omega}
\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\T}{\mathbb{T}}
\newcommand{\Q}{\mathbb{Q}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\MA}{\mathcal{A}}
\newcommand{\MC}{\mathcal{C}}
\newcommand{\MB}{\mathcal{B}}
\newcommand{\MF}{\mathcal{F}}
\newcommand{\MG}{\mathcal{G}}
\newcommand{\ML}{\mathcal{L}}
\newcommand{\MN}{\mathcal{N}}
\newcommand{\MS}{\mathcal{S}}
\newcommand{\MP}{\mathcal{P}}
\newcommand{\ME}{\mathcal{E}}
\newcommand{\MT}{\mathcal{T}}
\newcommand{\MM}{\mathcal{M}}
\newcommand{\MI}{\mathcal{I}}
\newcommand{\MX}{\mathcal{X}}

\newcommand{\io}{\text{ i.o.}}
\newcommand{\ev}{\text{ ev.}}
\renewcommand{\r}{\rangle}
\renewcommand{\l}{\langle}
\renewcommand{\Re}{\text{Re}}

\newcommand{\RG}{[0,\infty]}
\newcommand{\Rg}{[0,\infty)}
\newcommand{\Ll}{L^1_{\text{loc}}(\R^n)}

\newcommand{\limfn}{\liminf \limits_{n \rightarrow \infty}}
\newcommand{\limpn}{\limsup \limits_{n \rightarrow \infty}}
\newcommand{\limn}{\lim \limits_{n \rightarrow \infty}}
\newcommand{\convt}[1]{\xrightarrow{\text{#1}}}
\newcommand{\conv}[1]{\xrightarrow{#1}} 
\newcommand{\seq}[2]{(#1_{#2})_{#2 \in \N}}

\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\spn}{span}
\DeclareMathOperator{\Img}{Im}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\Aut}{Aut}
\DeclareMathOperator{\Homeo}{Homeo}
\DeclareMathOperator{\Sym}{Sym}


\newcommand{\dm}{\, d m}
\newcommand{\dmu}{\, d \mu}
\newcommand{\dnu}{\, d \nu}
\newcommand{\dlam}{\, d \lambda}
\newcommand{\dP}{\, d P}


\begin{document}
	
	\title{Introduction to Probability}
	\author{Carson James}
	\maketitle
	
	\tableofcontents
	

	
	
	\newpage
	\section{Basic Probability}
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	\section{Kolomogorov Extension Theorem}
	
	Show that for each $x$, $\pi_{st}(x) = \pi_s(x)\pi_t(x)$ and this implies that for each feasible $(s,A), (t, B)$ so that  $\pi_{s t}{-1}(A \times B) = \pi_s(A) \cap \pi_t(B)$.
	
	For $u \subset \C^\N$, set $\pi_u \C^\N = \{x \in \C^\N: \text{for each $j \in u^c$, $x_j = 0$} \}$. Consider lattice of subspaces of $\pi_u \C^\N$, ordered by inclusion indexed by finite subsets $u \subset \N$. 
	
	show that if $u \subset v$, then $\MB(\pi_u \C^\N) = \iota_{u,v}^*(\MB(\pi_v \C^\N))$ where $\iota_{u,v}: \pi_u \C^\N \rightarrow \pi_v \C^\N$ is inclusion.
	
	Equip $(\pi_u \C^\N, \MB(\pi_u \C^\N))$ with measure $\mu_u$ such that $\pi_u$
	
	 with pullback sigma algebra $\iota_{u,v}^*(\MB(\pi_v \C^\N))$ where $u \subset v$ and $\iota_{u,v}: \pi_u \C^\N \rightarrow \pi_v \C^\N$. Equip each object $(\pi_u \C^\N, \MB(\pi_u \C^\N))$ with measures $\mu_u$ satisfying $\mu_u$ of in stoch has maximum 
	
	
	
	
	\begin{defn}
		Let 
	\end{defn}
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	\newpage
	
	\section{Probability}
	\subsection{Distributions}
	
	\begin{defn}
		Let $\Om$ be a set and $\MP \subset \MP(X)$. Then $\MP$ is said to be a \textbf{$\pi$-system} on $\Om$ if for each $A,B \in \MP$, $A \cap B \in \MP$.
	\end{defn}
	
	\begin{defn}
		Let $\Om$ be a set and $\ML \subset \MP(\Om)$. Then $\ML$ is said to be a \textbf{$\lam$-system} on $\Om$ if 
		\begin{enumerate}
			\item $\ML \neq \varnothing$
			\item for each $A \in \ML$, $A^c \in \ML$
			\item for each $(A_n)_{n \in \N} \subset \ML$, if $(A_n)_{n \in \N}$ is disjoint, then $\bigcup\limits_{n \in \N}A_n \in \ML$
		\end{enumerate}
	\end{defn}
	
	\begin{ex}
		Let $\Om$ be a set and $\ML$ a $\lam$-system on $\Om$. Then 
		\begin{enumerate}
			\item $\Om, \varnothing \in \ML$
		\end{enumerate} 
	\end{ex}
	
	\begin{proof}
		Straightforward.
	\end{proof}
	
	\begin{defn}
		Let $\Om$ be a set and $\MC \subset \MP(\Om)$. Put $$\MS = \{\ML \subset \MP(\Om): \ML \text{ is a }\lam\text{-system on }\Om \text{ and } \MC \subset \ML\}$$ We define the \textbf{$\lam$-system on $\Om$ generated by $\MC$}, $\lam(\MC)$, to be $$\lam(\MC) = \bigcap_{\ML \in \MS}\ML$$
	\end{defn}
	
	\begin{ex}
		Let $\Om$ be a set and $\MC \subset \MP(\Om)$. If $\MC$ is a $\lam$-system and $\MC$ is a $\pi$-system, then $\MC$ is a $\sig$-algebra.
	\end{ex}
	
	\begin{proof}
		Suppose that $\MC$ is a $\lam$-system and $\MC$ is a $\pi$-system. Then we need only verify the third axiom in the definition of a $\sig$-algebra. Let $(A_n)_{n \in \N} \subset \MC$. Define $B_1 = A_1$ and for $n \geq 2$, define $B_n = A_n \cap \bigg( \bigcup\limits_{k=1}^{n-1}A_k \bigg)^c = A_n \cap \bigg( \bigcap\limits_{k=1}^{n-1}A_k^c \bigg) \in \MC$. Then $(B_n)_{n \in \N}$ is disjoint and therefore $\bigcup\limits_{n \in \N}A_n = \bigcup\limits_{n \in \N}B_n \in \MC$.
	\end{proof}
	
	\begin{thm}(Dynkin's Theorem) \\
		Let $\Om$ be a set.
		\begin{enumerate}
			\item Let $\MP$ be a $\pi$-system on $\Om$ and $\ML$ a $\lam$-system on $\Om$. If $\MP \subset \ML$, then $\sig(\MP) \subset \ML$.
			\item Let $\MP$ be a $\pi$-system on $\Om$. Then $\sig(\MP) = \lam(\MP)$
		\end{enumerate} 
		
	\end{thm}
	
	\begin{ex}
		Let $(\Om, \MF)$ be a measurable space and $\mu, \nu$ probability measures on $(\Om, \MF)$. Put $\ML_{\mu,\nu} = \{A \in \MF: \mu(A) = \nu(A)\}$. Then $\ML_{\mu, \nu}$ is a $\lam$-system on $\Om$.
	\end{ex}
	
	\begin{proof}\
		\begin{enumerate}
			\item $\varnothing \in \ML_{\mu, \nu}$.
			\item Let $A \in \ML_{\mu, \nu}$. Then $\mu(A) = \nu(A)$. Thus 
			\begin{align*}
				\mu(A^c) 
				&= 1-\mu(A) \\
				&= 1 -\nu(A) \\
				&= \nu(A^c)
			\end{align*}
			So $A^c \in \ML_{\mu, \nu}$. 
			\item Let $(A_n)_{n \in \N} \subset \ML_{\mu, \nu}$. So for each $n \in \N$, $\mu(A_n) = \nu(A_n)$.  Suppose that $(A_n)_{n \in \N}$ is disjoint. Then 
			\begin{align*}
				\mu\bigg(\bigcup_{n \in \N} A_n\bigg) 
				&= \sum_{n \in \N} \mu(A_n) \\
				&= \sum_{n \in \N} \nu(A_n) \\
				&= \nu\bigg(\bigcup_{n \in \N} A_n\bigg) 
			\end{align*}
			Hence $\bigcup_{n \in \N} A_n \in \ML_{\mu, \nu}$.
		\end{enumerate}
	\end{proof}
	
	\begin{ex}
		Let $(\Om, \MF)$ be a measurable space, $\mu, \nu$ probability measures on $(\Om, \MF)$ and $\MP \subset \MF$ a $\pi$-system on $\Om$. Suppose that for each $A \in \MP$, $\mu(A) = \nu(A)$. Then for each $A \in \sig(\MP)$, $\mu(A) = \nu(A)$.
	\end{ex}
	
	\begin{proof}
		Using the previous exercise, we see that $\MP \subset \ML_{\mu, \nu}$. Dynkin's theorem implies that $\sig(\MP) \subset \ML_{\mu, \nu}$. So for each $A \in \sig(\MP)$, $\mu(A) = \nu(A)$.
	\end{proof}
	
	
	
	
	
	
	
	
	\begin{defn}
		Let $F: \R \rightarrow \R$. Then $F$ is said to be a \textbf{probability distribution function} if 
		\begin{enumerate}
			\item $F$ is right continuous
			\item $F$ is increasing
			\item $F(-\infty)  = 0$ and $F(\infty)  = 1$
		\end{enumerate}
	\end{defn}
	
	\begin{defn}
		Let $P $ be a probability measure on $(\R, \MB(\R))$. We define $F_P: \R \rightarrow \R$, by $$F_P(x) = P((-\infty, x])$$ We call $F_P$ the \textbf{probability distribution function of $P$}.
	\end{defn}
	
	\begin{ex}
		Let $(\Om, \MF, P)$ be a probability measure. Then $F_P$ is a probability distribution function.
	\end{ex}
	
	\begin{proof}
		\begin{enumerate}
			\item Let $x \in \R$ and $(x_n)_{n \in \N} \subset [x, \infty)$. Suppose that $x_n \rightarrow x$. Then $(x, x_n] \rightarrow \varnothing$ because $\limsup\limits_{n \rightarrow \infty} (x,x_n] = \varnothing$. Thus $$F(x_n) - F(x) = P((x, x_n]) \rightarrow P(\varnothing) = 0$$This implies that $$F(x_n) \rightarrow F(x)$$ So $F$ is right continuous.
			\item Clearly $F_P$ is increasing.
			\item Continuity from below tells us that $$F(-\infty) = \lim_{n \rightarrow -\infty}F(n) = \lim_{n \rightarrow -\infty}P((-\infty,n]) = 0$$ and continuity from above tell us that $$F(\infty)  = \lim_{n \rightarrow \infty}F(n) = \lim_{n \rightarrow \infty}P((-\infty, n]) = 1$$ 
		\end{enumerate}
	\end{proof}
	
	\begin{ex}
		Let $\mu, \nu$ be probability measures on $(\R, \MB(\R))$. Then $F_{\mu} = F_{\nu}$ iff $\mu = \nu$.  
	\end{ex}
	
	\begin{proof}
		Clearly if $\mu = \nu$, then $F_{\mu} = F_{\nu}$. Conversely, suppose that $F_{\mu} = F_{\nu}$. Then for each $x \in \R$, 
		\begin{align*}
			\mu((-\infty,x]) 
			&= F_{\mu}(x) \\
			&= F_{\nu}(x)  \\
			&= \nu((-\infty,x])
		\end{align*}
		Put $\MC = \{(-\infty,x]:x \in \R\} $. Then $\MC$ is a $\pi$-system and for each $A \in \MC$, $\mu(A) = \nu(A)$. Hence for each $A \in \sig(C) = \MB(\R)$, $\mu(A) = \nu(A)$. So $\mu = \nu$. 
	\end{proof}
	
	\begin{defn}
		Let $(\Om, \MF, P)$ be a probability space and $X:\Om \rightarrow \R^n$. Then $X$ is said to be a \textbf{random vector} on $(\Om, \MF)$ if $X$ is $\MF$-$\MB(\R^n)$ measurable. If $n = 1$, then $X$ is said to be a \textbf{random variable}. We define $$L_n^0(\Om, \MF, P) = \{X:\Om \rightarrow \R^n: X \text{ is a random vector}\}$$ and $$L_n^p(\Om, \MF, P) = \bigg \{X \in L_n^0: \int \|X\|^p dP < \infty \bigg \} $$ 
	\end{defn}
	
	\begin{defn}
		Let $(\Om, \MF, P)$ be a probability space and $X$ a random variable on $(\Om,\MF)$. We define the \textbf{probability distribution} of $X$, $P_X:\MB(R) \rightarrow [0,1]$, to be the measure $$P_X = X_*P$$ That is, for each $A \in \MB(\R)$, $$P_X(A) = P(X^{-1}(F))$$ \\ We define the \textbf{probability distribution function} of $X$, $F_X:\R \rightarrow [0,1]$, to be $$F_X = F_{P_X}$$
	\end{defn}
	
	\begin{defn}
		Let $(\Om, \MF, P)$ be a probability space and $X$ a random variable on $(\Om,\MF)$. If $P_X \ll m$, we define the \textbf{probability density} of $X$, $f_X: \R \rightarrow \R$, by $$f_X = \frac{dP_X}{dm}$$ 
	\end{defn}
	
	\begin{ex}
		Let $(\Om, \MF, P)$ be a probability space and $(X_n)_{n \in \N}$ be a sequence of random variables on $(\Om, \MF)$. Then for each $x \in \R$, $$\P\bigg(\liminf_{n \rightarrow \infty}X_n > x\bigg) \leq \liminf_{n \rightarrow \infty} P(X_n > x)$$  
	\end{ex}
	
	\begin{proof}
		Let $\om \in \bigg \{ \liminf\limits_{n \rightarrow \infty} X_n > x \bigg \}$. Then $x< \liminf\limits_{n \rightarrow \infty} X_n (\om) = \sup\limits_{n \in \N} \bigg( \inf\limits_{k \geq n} X_k(\om)\bigg)$. So there exists $n^* \in \N$ such that $x< \inf\limits_{k \geq n^*} X_k(\om)$. Then for each $k \in \N$, $k \geq n^*$ implies that $x < X_k(\om)$. So there exists $n^* \in \N$ such that for each $k \in \N$, $k \geq n^*$ implies that $\mathbf{1}_{\{X_k > x\}}(\om) =1$. Hence $\inf\limits_{k \geq n^*} \mathbf{1}_{\{X_k > x\}}(\om)  = 1$. Thus  $\liminf\limits_{n \rightarrow \infty} \mathbf{1}_{\{X_k > x\}}(\om) = \sup\limits_{n \in \N} \bigg( \inf_{k \geq n} \mathbf{1}_{\{X_k > x\}}(\om) \bigg) = 1$. Therefore $\om \in \liminf\limits_{n \rightarrow \infty} \{X_k > x\}$ and we have shown that $$\bigg \{ \liminf\limits_{n \rightarrow \infty} X_n > x \bigg \} \subset \liminf\limits_{n \rightarrow \infty} \{X_k > x\}$$ Then 
		\begin{align*}
			P \bigg( \liminf\limits_{n \rightarrow \infty} X_n > x \bigg)
			& \leq P \bigg( \liminf\limits_{n \rightarrow \infty} \{X_k > x\} \bigg) \\
			& \leq \liminf_{n \rightarrow \infty} P(\{X_k > x\})
		\end{align*}
	\end{proof}
	
	
	\begin{defn}
		Let $(\Om, \MF, P)$ be a probability space and $X \in L^+(\Om) \cup L^1$. Define the \textbf{expectation of X}, $E(X)$, to be $$E(X) = \int X \dP$$.
	\end{defn}
	
	
	\subsection{Independence}
	
	\begin{defn}
		Let $(\Om, \MF, P)$ be a probability space and $\MC \subset \MF$. Then $\MC$ is said to be \textbf{independent} if for each $(A_i)_{i=1}^n \subset \MC$, $$P \bigg( \bigcap_{k=1}^nA_k\bigg) = \prod_{k=1}^{n}P(A_k)$$
	\end{defn}
	
	\begin{defn}
		Let $(\Om, \MF, P)$ be a probability space and $\MC_1, \cdots, \MC_n \subset \MF$. Then $\MC_1, \cdots, \MC_n $ are said to be \textbf{independent} if for each $A_1 \in \MC_1,  \cdots, A_n \in \MC_n$, $A_1, \cdots, A_n$ are independent. 
	\end{defn}
	
	\begin{note}
		We will explicitely say that for each $i=1, \cdots , n$, $\MC_i$ is independent when talking about the independence of the elements of $\MC_i$ to avoid ambiguity.
	\end{note}
	
	\begin{defn}
		Let $(\Om, \MF, P)$ be a probability space and $X_1, \cdots, X_2$ random variables on $(\Om, \MF)$. Then $X_1, \cdots, X_n$ are said to be \textbf{independent} if for each $B_1, \cdots, B_n \in \MB(\R)$, $X_1^{-1}B_1, \cdots, X_n^{-1}B_n$ are independent.
	\end{defn}
	
	\begin{ex}
		Let $(\Om, \MF, P)$ be a probability space and $X_1, \cdots, X_n$ random variables on $(\Om, \MF)$. Then $X_1, \cdots, X_n$ are independent iff $\sig(X_1), \cdots, \sig(X_n)$ are independent.
	\end{ex}
	
	\begin{proof}
		Suppose that $X_1, \cdots, X_n$ are independent. Let $A_1, \in \sig(X_1), \cdots, A_n \in \sig(A_n)$. Then for each $i = 1, \cdots, n$, there exists $B_i \in \MB(\R)$ such that $A_i = X_i^{-1}(B_i)$. Then $A_1, \cdots, A_n$ are independent. Hence $\sig(X_1), \cdots, \sig(X_n)$ are independent. Conversely, suppose that $\sig(X_1), \cdots, \sig(X_n)$ are independent. Let $B_1, \cdots, B_n \in \MB(\R)$. Then for each $i = 1, \cdots, n$, $X_i^{-1}B_i \in \sig(X_i)$. Then $X_1^{-1}B_1, \cdots, X_n^{-1}B_n$ are independent. Hence $X_1, \cdots, X_n$ are independent.
	\end{proof}
	
	\begin{ex}
		Let $(\Om, \MF, P)$ be a probability space, $X_1, \cdots, X_n$ random variables on $(\Om, \MF)$ and $\MF_1, \cdots, \MF_n \subset \MF$ a collection of $\sig$-algebras on $\Om$. Suppose that for each $i = 1, \cdots, n$, $X_i$ is $\MF_i$-measurable. If $\MF_1, \cdots, \MF_n$ are independent, then $X_1, \cdots, X_n$ are independent. 
	\end{ex}
	
	\begin{proof}
		For each $i =1, \cdots, n$, $\sig(X_i) \subset \MF_i$. So $\sig(X_1), \cdots, \sig(X_n)$ are independent. Hence $X_1, \cdots, X_n$ are independent.
	\end{proof}
	
	\begin{ex}
		Let $(\Om, \MF, P)$ be a probability space and $\MC_1, \cdots, \MC_n \subset \MF$. Suppose that for each $i = 1, \cdots, n$, $\MC_i$ is a $\pi$-system and $\MC_1, \cdots, \MC_n$ are independent, then $\sig(\MC_1), \cdots, \sig(\MC_n)$ are independent.
	\end{ex}
	
	\begin{proof}
		Let $A_2 \in \MC_2$. Define $\ML = \{A \in \MF: P(A\cap A_2) = P(A)P(A_2)\}$. Then 
		\begin{enumerate}
			\item $\Om \in \ML$
			\item If $A \in \ML$, then 
			\begin{align*}
				P(A^c \cap A_2) 
				&= P(A_2) - P(A_2 \cap A) \\
				&= P(A_2) - P(A_2) P(A) \\
				&= (1- P(A))P(A_2) \\
				&= P(A^c)P(A_2)
			\end{align*}
			So $A^c \in \ML$
			\item If $(B_n)_{n \in \N} \subset \ML$ is disjoint, then 
			\begin{align*}
				P\bigg( \bigg[\bigcup_{n \in \N}B_n \bigg] \cap A_2\bigg) 
				&= P \bigg( \bigcup_{n \in \N}B_n \cap A_2 \bigg) \\
				&= \sum_{n \in \N}P(B_n \cap A_2) \\
				&= \sum_{n \in \N}P(B_n) P(A_2) \\
				&=   \bigg[\sum_{n \in \N}P(B_n)\bigg]P(A_2)  \\
				&=  P\bigg( \bigcup_{n \in \N} A_n\bigg) P(A_2) 
			\end{align*} 
			So $\bigcup\limits_{n \in \N}B_n \in \ML$. 
		\end{enumerate}
		Thus $\ML$ is a $\lam$-system. Since $\MC_1 \subset \ML$ is a $\pi$-system, Dynkin's theorem tells us that $\sig(\MC_1) \subset \ML$. Since $A_2 \in \MC_2$ is arbitrary $\sig(\MC_1)$ and $\MC_2$ are independent. The same reasoning implies that $\sig(\MC_1)$ and $\sig(\MC_2)$ are independent. Let $A_2 \in \MC_1, \cdots, A_n \in \MC_n$ We may do the same process with $$\ML = \bigg \{A \in \MF: P\bigg(A \cap \bigg(\bigcap_{i=2}^n A_i\bigg) \bigg) = P(A)\prod_{i=2}^n P(A_i)\bigg\}$$ and conclude that $\sig(\MC_1), \MC_2, \cdots, \MC_n$ are independent. Which, using the same reasoning would imply that $\sig(\MC_1), \cdots, \sig(\MC_n)$ are independent.
	\end{proof}
	
	\begin{ex}
		Let $(\Om, \MF, P)$ be a probability space, $X_1, \cdots, X_n$ random variables on $(\Om, \MF)$. Then $X_1, \cdots, X_n$ are independent iff for each $x_1, \cdots, x_n \in \R$, $$P(X_1 \leq x_1, \cdots, X_n \leq x_n ) = \prod_{i=1}^nP(X_i \leq x_i)$$
	\end{ex}
	
	\begin{proof}
		Suppose that $X_1, \cdots, X_n$ are independent. Then $\sig(X_1), \cdots, \sig(X_n)$ are independent. Let $x_1, \cdots, x_n \in \R$. Then for each $i=1, \cdots, n$, $\{X_i \leq x_i\} \in \sigma(X_i)$. Hence \\$P(X_1 \leq x_1, \cdots, X_n \leq x_n ) = \prod\limits_{i=1}^nP(X_i \leq x_i)$. Conversely, suppose that for each \\$x_1, \cdots, x_n \in \R$, $P(X_1 \leq x_1, \cdots, X_n \leq x_n ) = \prod\limits_{i=1}^nP(X_i \leq x_i)$. Define $\MC = \{ (-\infty, x]: x \in \R \}$. Then $\MB(\R) = \sig(\MC)$. For each $i =1, \cdots, n$, define $\MC_i = X_i^{-1}\MC$. Then for each $i =1, \cdots, n$, $\MC_i$ is a $\pi$-system and 
		\begin{align*}
			\sig(\MC_i) 
			&= \sig(X^{-1}(\MC)) \\
			&= X_i^{-1}(\sig(\MC)) \\
			&= X_i^{-1}(\MB(\R)) \\
			&= \sig(X_i)
		\end{align*}
		By assumption, $\MC_1, \cdots, \MC_n$ are independent. The previous exercies tells us that $\sig(X_1), \cdots, \sig(X_n)$ are independent. Then $X_1, \cdots, X_n$ are independent. 
	\end{proof}
	
	\begin{ex}
		Let Let $(\Om, \MF, P)$ be a probability space and $X_1, \cdots, X_n$ random variables on $(\Om, \MF)$. Define $X = (X_1, \cdots, X_n)$. If $X_1, \cdots, X_n$ are independent, then $$P_{X} = \prod\limits_{i=1}^nP_{X_i}$$.
	\end{ex}
	
	\begin{proof}
		Let $A_1, \cdots, A_n \in \MB(\R)$. Then 
		\begin{align*}
			P_X(A_1 \times \cdots \times A_n) 
			&= P(X \in A_1 \times \cdots \times \in A_n)\\
			&= P(X_1 \in A_1, \cdots, X_n \in A_n) \\
			&= P(X_1 \in A_1) \cdots P(X_n \in A_n) \\
			&= P_{X_1}(A_1) \cdots P_{X_n}(A_n) \\
			&= \prod_{i=1}^nP_{X_i}(A_1 \times \cdots \times  A_n)
		\end{align*}
		Put $$\MP = \{ A_1 \times \cdots \times A_n: A_1 \in \MB(R), \cdots, A_n \in \MB(R) \}$$ Then $\MP$ is a $\pi$-system and $$\sig(\MP) = \MB(R) \otimes \cdots \otimes \MB(R) = \MB(\R^n)$$
		A previous exercise then tells us that $P_X = \prod\limits_{i=1}^nP_{X_i}$ 
	\end{proof}
	
	\begin{ex}
		Let Let $(\Om, \MF, P)$ be a probability space, $X_1, \cdots, X_n$ random variables on $(\Om, \MF)$ and $f_1, \cdots, f_n: \R \rightarrow \R \in L^0$. Suppose that $f_1 \circ X_1, \cdots, f_n \circ X_n\in L^+(\Om)$ or $f_1 \circ X_1, \cdots, f_n \circ X_n \in L^1(\Om)$. If $X_1, \cdots, X_n$ are independent, then $$E(f_1(X_1) \cdots f_n(X_n)) = \prod_{i=1}^n E(f_i(X_i))$$
	\end{ex}
	
	\begin{proof}
		Define the random vector $X : \Om \rightarrow \R^n$ by $X = (X_1, \cdots, X_n)$ and $g:\R^n \rightarrow \R$ by $g(x_1, \cdots, x_n) = f_1(x_1) \cdots f_n(x_n)$. Suppose that for each $i = 1, \cdots, n$, $f_i \in L^+(\R)$. Then $g \in L^+(\R^n)$ and by change of variables,  
		\begin{align*}
			E(f_1(X_1) \cdots f_n(X_n)) 
			&= E(g(X)) \\
			&= \int_{\Om} g \circ X \dP \\
			&= \int_{\R^n} g(x) \dP_X(x) \\
			&= \int_{R^n} g(x) d \prod\limits_{i=1}^nP_{X_i}(x) \\
			&=  \prod_{i=1}^n \int_{\R}f_i(x) d P_{X_i}(x) \\ 
			&= \prod_{i=1}^n \int_{\Om}f_i \circ X d P \\
			&= \prod_{i=1}^n E(f_i(X_i))
		\end{align*}
		If for each $i = 1, \cdots, n$, $f_i \in L^1(\R, P_{X_i})$, then following the above reasoning with $\vert g \vert $ tells us that $g \in L^1(\R^n, P_X)$ and we use change of variables and Fubini's theorem to get the same result.  
	\end{proof}
	
	\subsection{$L^p$ Spaces for Probability}
	
	\begin{note}
		Recall that for a probability space $(\Om, \MF, P)$ and $1 \leq p \leq q \leq \infty$ we have $L^q \subset L^p$ and for each $X \in L^q, \|X \|_p \leq  \|X\|_q$. Also recall that for $X,Y \in \L^2$, we have that $\|XY \|_1 \leq \|X\|_2 \|X\|_2$.
	\end{note}
	
	\begin{defn}
		Let $(\Om, \MF, P)$ be a probability space and $X \in L^2$. Define the \textbf{variance of X}, $Var(X)$, to be $$Var(X) = E(\big[\big(X-E(X)\big)^2\big)$$.
	\end{defn}
	
	\begin{defn}
		Let $(\Om, \MF, P)$ be a probability space and $X,Y \in L^2$. Define the  
	\end{defn}
	
	
	\begin{defn}
		Let $(\Om, \MF, P)$ be a probability space and $X,Y \in L^2$. Define the \textbf{covariance of $X$ and $Y$}, $Cov(X,Y)$, to be $$Cov(X,Y) = E([X-E(X)][Y-E(Y)])$$
	\end{defn}
	
	\begin{ex}
		Let $(\Om, \MF, P)$ be a probability space and $X,Y \in L^2$. Then the covariance is well defined and $Cov(X,Y) ^2 \leq Var(X)Var(Y)$
		
		\begin{proof}
			By Holder's inequality, 
			\begin{align*}
				| Cov(X,Y) |
				&= \bigg | \int (X-E(X))(Y-E(Y)) \dP \bigg | \\
				&\leq \int | (X-E(X))(Y-E(Y)) | \dP \\
				&= \|(X-E(X))(Y-E(Y)) \|_1 \\
				&\leq \|X-E(X) \|_2 \|(Y-E(Y) \|_2  \\
				&= \bigg(\int | X - E(X) |^2 \dP\bigg)^{\frac{1}{2}} \bigg( | Y - E(Y) |^2 \bigg)^{\frac{1}{2}} \\
				&= Var(X)^{\frac{1}{2}}Var(Y)^{\frac{1}{2}}
			\end{align*}
			
			So $Cov(X,Y) ^2 \leq Var(X)Var(Y)$.
		\end{proof}
		
	\end{ex}
	
	\begin{ex}
		Let $(\Om, \MF, P)$ be a measure space and $X,Y \in L^2$. Then
		\begin{enumerate}
			\item $Cov(X,Y) = E(XY)-E(X)E(Y)$
			\item If $X,Y$ are independent, then $Cov(X,Y) = 0$
			\item $Var(X) = E(X^2) - E(X)^2$
			\item for each $a,b \in \R$, $Var(aX + b) = a^2Var(X)$.
			\item $Var(X+Y) = Var(X) + Var(Y) + 2 Cov(X,Y)$
		\end{enumerate}
	\end{ex}
	
	\begin{proof}\
		\begin{enumerate}
			\item We have that
			\begin{align*}
				Cov(X,Y) 
				&= E\bigg[ (X-E(X))(Y-E(Y)) \bigg] \\
				&= E(XY -E(Y)X - E(X)Y + E(X)E(Y)) \\
				&= E(XY) - E(X)E(Y) - E(X)E(Y) + E(X)E(Y) \\ 
				&= E(XY) - E(X)E(Y)
			\end{align*}
			\item Suppose that $X,Y$ are independent. Then $E(XY) = E(X)E(Y)$. Hence 
			\begin{align*}
				Cov(X,Y) 
				&= E(XY) - E(X)E(Y) \\
				&= E(X)E(Y) - E(X)E(Y) \\
				&= 0
			\end{align*}
			\item Part (1) implies that 
			\begin{align*}
				Var(X) 
				&= Cov(X, X) \\
				&= E(X^2) -E(X)^2
			\end{align*}
			\item Let $a,b \in \R$. Then
			\begin{align*}
				Var(aX+b)
				&= E[(aX+b)^2] - E(aX+b)^2 \\
				&= E[a^2X^2 + 2abX +b^2] - (aE(X)+b)^2 \\
				&= a^2E(X^2)+2abE(X) + b^2 - (a^2E(X)^2 +2abE(X)+b^2) \\
				&= a^2(E(X^2)-E(X)^2) \\
				&= a^2Var(X)
			\end{align*} 
			\item We have that 
			\begin{align*}
				Var(X+Y) 
				&= E[(X+Y)^2] - E[X+Y]^2 \\
				&= E[X^2 +2XY + Y^2] -(E(X)+E[Y])^2 \\
				&= E(X^2) + 2E[XY] + E[Y^2] - (E(X)^2 + 2E(X)E[Y] + E[Y]^2) \\
				&= (E(X^2) - E(X)^2) + (E[Y^2]- E[Y]^2) + 2(E[XY] - E(X)E[Y]) \\
				&= Var(X) + Var(Y) + 2Cov(X,Y)
			\end{align*}
		\end{enumerate}
	\end{proof}
	
	\begin{defn}
		Let $(\Om, \MF, P)$ be a probability space and $X,Y \in L^2$. The \textbf{correlation of X and Y}, $Cor(X,Y)$, is defined to be $$Cor(X,Y) = \frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}}$$
	\end{defn}
	
	\begin{ex}
		
	\end{ex}
	
	\begin{ex} \textbf{Jensen's Inequality:} \\
		Let $(\Om, \MF, P)$ be a probability space, $X \in L^1$ and $\phi:\R \rightarrow \R$. If $\phi$ is convex, then $$\phi(E(X)) \leq E[\phi(X)]$$
	\end{ex}
	
	\begin{proof}
		Put $x_0 = E(X)$. Since $\phi$ is convex, there exist $a,b \in \R$ such that $\phi(x_0) = ax_0+b$ and for each $x \in \R$, $\phi(x) \geq ax+b$. Then \begin{align*}
			E[\phi(X)] 
			&= \int\phi(X) \dP \\
			&\geq \int[ aX+b ]\dP \\
			&= a\int X \dP +b \\
			&= aE(X) +b \\
			&= ax_0+b \\
			&= \phi(x_0) \\
			&= \phi(E(X))
		\end{align*}
	\end{proof}
	
	\begin{ex}{Markov's Inequality:}
		Let $(\Om, \MF, P)$ be a probability space and $X \in L^+$. Then for each $a \in (0,\infty)$, $$P(X \geq a) \leq \frac{E(X)}{a}$$
	\end{ex}
	
	\begin{proof}
		Let $a \in (0,\infty)$. Then $a \mathbf{1}_{\{X \geq a\}} \leq X \mathbf{1}_{\{X \geq a\}}$. Thus 
		\begin{align*}
			a P(X \geq a) 
			&= \int a \mathbf{1}_{\{X \geq a\}} \dP\\
			&= \int X \mathbf{1}_{\{X \geq a\}} \dP\\
			&\leq \int X \dP \\
			&= E(X)
		\end{align*}
		Therefore $$P(X \geq a) \leq \frac{E(X)}{a}$$. 
	\end{proof}
	
	\begin{ex}{Chebychev's Inequality:}
		Let $(\Om, \MF, P)$ be a probability space and $X \in L^2$. Then for each $a \in (0, \infty)$, $$P(| X - E(X) | \geq a) \leq \frac{Var(X)}{a^2}$$
	\end{ex}
	
	\begin{proof}
		Let $a \in (0, \infty)$. Then 
		\begin{align*}
			P(| X - E(X) | \geq a) 
			&= P((X - E(X))^2 \geq a^2) \\
			&\leq \frac{E[(X - E(X))^2]}{a^2} \\
			&= \frac{Var(X)}{a^2}  
		\end{align*}
	\end{proof}
	
	\begin{ex}{Chernoff's Bound:}
		Let $(\Om, \MF, P)$ be a probability space and $X \in L^2$. Then for each $a, t \in (0, \infty)$, $$P( X \geq a) \leq e^{-ta}E[e^{tX}]$$
	\end{ex}
	
	\begin{proof}
		Let $a, t \in (0, \infty)$. Then 
		\begin{align*}
			P( X \geq a)
			&= P(tX \geq ta) \\
			&= P(e^{tX} \geq e^{ta}) \\
			& \leq e^{-ta}E[e^{tX}]
		\end{align*}
	\end{proof}
	
	\begin{ex}{Weak Law of Large Numbers:}
		Let $(\Om, \MF, P)$ be a probability space $(X_i)_{i\in \N} \subset L^2$. Suppose that $(X_i)_{i\in \N}$ are iid. Then $$\frac{1}{n}\sum_{i=1}^n X_i \conv{P} E[X_1]$$
	\end{ex}
	
	\begin{proof}
		Put $\mu = E[X_1]$ and $\sig^2 = Var(X_1)$. Then 
		\begin{align*}
			E[\frac{1}{n}\sum_{i=1}^n X_i] 
			&= \frac{1}{n} \sum_{i=1}^nE[X_i] \\
			&= \frac{1}{n} \sum_{i=1}^n \mu \\
			&= \mu
		\end{align*} and 
		\begin{align*}
			Var(\frac{1}{n}\sum_{i=1}^n X_i) 
			&= \frac{1}{n^2} Var(\sum_{i=1}^n X_i) \\
			&= \frac{1}{n^2} \sum_{i=1}^n Var(X_i) \\
			&= \frac{1}{n^2} \sum_{i=1}^n \sig^2 \\
			&= \frac{\sig^2}{n}
		\end{align*}
		Let $\ep >0$. Then 
		\begin{align*}
			P\bigg(\bigg | \frac{1}{n} \sum_{i=1}^n X_i - E[X_1] \bigg | \geq \ep \bigg) 
			& = P\bigg(\bigg| \frac{1}{n} \sum_{i=1}^n X_i - \mu \bigg| \geq \ep\bigg) \\
			& = P\bigg(\bigg| \frac{1}{n} \sum_{i=1}^n X_i - E(\bigg[\frac{1}{n}\sum_{i=1}^n X_i \bigg] \bigg| \geq \ep\bigg) \\ 
			&\leq \frac{Var(\frac{1}{n} \sum_{i=1}^n X_i)}{\ep^2} \\
			& =  \frac{\sig^2 / n}{\ep^2} \\
			&= \frac{\sig^2}{n\ep^2} \rightarrow 0
		\end{align*}
		
		So $$\frac{1}{n}\sum_{i=1}^n X_i \conv{P} E[X_1]$$
	\end{proof}
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	\newpage
	\subsection{Borel Cantelli Lemma} 
	
	\begin{ex}\textbf{Borel Cantelli Lemma:}\\
		Let $(\Om, \MF, P)$ be a probability space and $(A_n)_{n \in \N} \subset \MF$.
		\begin{enumerate}
			\item If $\sum\limits_{n \in \N}P(A_n) < \infty$, then $P(\limpn A_n) = 0$.
			\item If $(A_n)_{n \in \N}$ are independent and $\sum\limits_{n \in \N} P(A_n) = \infty$, then $P( \limpn A_n) = 1$  
		\end{enumerate}
	\end{ex}
	
	\begin{proof}\
		\begin{enumerate}
			\item Suppose that $\sum\limits_{n \in \N}P(A_n) < \infty$. Recall that $$\limsup\limits_{n \rightarrow \infty}A_n = \bigg \{\om \in \Om: \sum\limits_{n \in \N}1_{A_n}(\om) = \infty \bigg \}$$ Then \begin{align*}
				\infty 
				&> \sum_{n \in \N}P(A_n) \\
				&= \sum_{n \in \N} \int 1_{A_n}\dP \\
				&= \int \sum_{n \in \N} 1_{A_n}\dP \\
			\end{align*}
			Thus $\sum \limits_{n \in \N} 1_{A_n} < \infty$ a.e. and $P(\limpn A_n) = 0$.
			\item Suppose that $(A_n)_{n \in \N}$ are independent and $\sum\limits_{n \in \N} P(A_n) = \infty$.
		\end{enumerate}
	\end{proof}
	
	\begin{ex}
		Let $(\Om, \MF, P)$ be a probability space and $(X_n)_{n \in \N} \subset L^0$ and $X \in L^0$. 
		\begin{enumerate}
			\item If for each $\ep >0 $, $\sum\limits_{n \in \N} P(|X_n -X| \geq \ep) < \infty$, then $X_n \rightarrow X$ a.e.
			\item If $(X_n)_{n \in \N}$ are independent and there exists $\ep >0$ such that $\sum\limits_{n \in \N} P(|X_n -X| \geq \ep) = \infty$, then $X_n \not \rightarrow X$ a.e.
		\end{enumerate}
	\end{ex}
	
	\begin{proof}\
		\begin{enumerate}
			\item For $\ep>0$ and $n \in \N$, set $A_n(\ep) = \{\om \in \Om: |X_n(\om) - X(\om)| \geq \ep\}$. Suppose that for each $\ep >0 $, $\sum\limits_{n \in \N} P(|X_n - X| \geq \ep) < \infty$.  The Borel-Cantelli lemma implies that for each $m \in \N$,  $$P(\limpn A_n(1/m)) = 0$$
			 Let $\om \in \Om$. Then $X_n(\om) \not \rightarrow X(\om)$ iff $$\om \in \bigcup_{m \in \N} \limpn A_n(1/m)$$ 
		So 
		\begin{align*}
		P(X_n \not \rightarrow X)
		&= P\bigg( \bigcup_{m \in \N} \limpn A_n(1/m) \bigg) \\
		&\leq \sum_{m \in \N}P(\limpn A_n(1/m)) \\
		&= 0
\end{align*}		 
	Hence $X_n \rightarrow X$ a.e.
	\item 
		\end{enumerate}
	\end{proof}
	
	
	
	
	
	
	
	
	
	








	\newpage	
	\section{Probability on locally compact Groups}
	
	\begin{note}
	In this section, familiarity with Haar measure will be assumed. This section is intended as a continuation of section $7$ of \cite{measure}.
	\end{note}
	
	
	\subsection{Action on Probability Measures}
	
	\begin{note}
	We recall some notation from section $7.1$ of \cite{measure}.
	\begin{itemize}
	\item $l_g \in \Homeo(G)$, $l_g(x) = gc$
	\item $L_g \in \Sym(L_0(G))$, $L_g f = f \circ l_g^{-1}$
	We continue from section $7$
	\end{itemize}
	\end{note}
	
	\begin{note}
	The next exercise generalizes the notion of a scale-family.
	\end{note}
	
	\begin{ex}
	Let $(\Om, \MF, P)$ be a probability space, $G$ a locally compact group, $\mu$ a left Haar measure on $G$, $X \in L^0_G$ and $g \in G$. If $P_X \ll \mu$, then $f_{gX} = L_g f_X$.
	\end{ex}
	
	\begin{proof}
	Suppose that $P_X \ll \mu$. Let $A \in \MB(G)$. Then 
	\begin{align*}
	P_{gX}(A) 
	&= P(gX \in A) \\
	&= P(X \in g^{-1}A) \\
	&= P_X(g^{-1}A) \\
	&= P_X(l_g^{-1}(A)) \\
	&= {l_g}_*P_X(A) \\
	&= g \cdot P_X(A)
	\end{align*}
	The previous exercise tells us that $f_{gX} = L_g f_X$.
	\end{proof}
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	\newpage
	\section{Weak Convergence of Measures}
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	\newpage
	\section{Concentration Inequalities}
	
	\subsection{Introduction}
	
	\begin{ex}
		Let $(\Om, \MF, P)$ be a probability space and $X,Y \in L^0_{\R}(\Om, \MF, P)$. Then for each $s, t \in \R$, 
		$$P(X + Y \geq s + t) \leq P(X \geq s) + P(Y \geq t) $$ 
	\end{ex}

	\begin{proof}
		For $Z \in L^0_{\R}(\Om, \MF, P)$ and $t \in \R$, define $A_Z^t \in \MF$ by 
		$$A_Z^t = \{\om \in \Om : Z(\om) \geq t\}$$ 
		Let $s,t \in \R$. Since $(A_X^s)^c \cap (A_Y^t)^c \subset (A_{X+Y}^{s+t})^c$, we have that $A_{X+Y}^{s+t} \subset A_X^s \cup A_Y^t$. Then 
		\begin{align*}
			P(X + Y \geq s + t)
			& = P(A_{X+Y}^{s+t}) \\
			& \leq P(A_X^s) + P(A_Y^t) \\
			& = P(X \geq s) + P(Y \geq t)
		\end{align*} 
	\end{proof}

	\begin{ex}
		Let $(\Om, \MF, P)$ be a probability space and $X,Y \in L^+(\Om, \MF, P)$. Then for each $s, t \geq 0$, 
		$$P(XY \geq st) \leq P(X \geq s) + P(Y \geq t) $$ 
	\end{ex}
	
	\begin{proof}
		For $Z \in L^0_{\R}(\Om, \MF, P)$ and $t \in \R$, define define $A_Z^t \in \MF$ by 
		$$A_Z^t = \{\om \in \Om : Z(\om) \geq t\}$$ 
		Let $s,t \in \R$. Since $(A_X^s)^c \cap (A_Y^t)^c \subset (A_{XY}^{st})^c$, we have that $A_{XY}^{st} \subset A_X^s \cup A_Y^t$. Then 
		\begin{align*}
			P(XY \geq st)
			& = P(A_{XY}^{st}) \\
			& \leq P(A_X^s) + P(A_Y^t) \\
			& = P(X \geq s) + P(Y \geq t)
		\end{align*} 
	\end{proof}

	
	
	\subsection{Sub $\al$-Exponential Random Variables}
	
	\begin{defn}
		Let $(\Om, \MF, P)$ be a probability space, $X \in L^0(\Om, \MF, P)$ and $\al >0$. Then $X$ is said to be \textbf{sub $\al$-exponential} if there exist $M, K > 0$ such that for each $t \geq 0$, $$P(|X| \geq t) \leq Me^{-Kt^{\al}}$$
	\end{defn}
	
	\begin{ex}
		Let $(\Om, \MF, P)$ be a probability space, $X \in L^0(\Om, \MF, P)$ and $\al > 0$. Then the following are equivalent:
		\begin{enumerate}
			\item $X$ is sub $\al$-exponential
			\item there exists $K > 0$ such that for each $p \geq 1$, $\|X\|_p \leq K p^{1/\al}$ 
			\item 
			\item 
		\end{enumerate} 
	\end{ex}

	\begin{proof}\
		\begin{itemize}
			\item $(1) \implies (2)$: \\
			Choose $C_{\al} > 0$ such that for each $x \geq \al^{-1}$, $\Gamma(x) \leq C_{\al} x^x$. Since $X$ is sub $\al$-exponential, there exist $M, K_0 >0$ such that for each $t \geq 0$, $P(|X| \geq t) \leq Me^{-Kt^{\al}}$. Set $K = \max(M\al^{-1}C_{\al}, 1) 2 K_0^{-1/\al} \al^{-1/\al}$. Let $p \geq 1$. Then $p \al^{-1} \geq \al^{-1}$
			\begin{align*}
				\|X\|_p^p 
				&= E(|X|^p) \\
				&= \int_0^{\infty} P(|X|^p \geq t)dt \\
				&= \int_0^{\infty} P(|X| \geq t^{1/p})dt \\
				& \leq \int_0^{\infty} M e^{-K_0 t^{\al / p}}dt \\
				&= Mp \al^{-1} \int_0^{\infty} u^{p/\al - 1} e^{-K_0 u} du \\
				&=  Mp \al^{-1} \Gamma(p / \al)K_0^{-p/\al} \\
				& \leq Mp \al^{-1} C_{\al} (p \al^{-1})^{p  /\al}K_0^{-p/\al} \\
			\end{align*} 
		Therefore 
		\begin{align*}
			\|X\|_p 
			& \leq (M\al^{-1}C_{\al})^{1/p} p^{1/p} K_0^{-1/\al} \al^{-1/\al} p^{1/\al}  \\
			& \leq \max(M\al^{-1}C_{\al}, 1) 2 K_0^{-1/\al} \al^{-1/\al} p^{1/\al} \\
			&= K p^{1/\al} 
		\end{align*}
			\item $(2) \implies (3)$: \\
			
		\end{itemize}
	\end{proof}
	
	\begin{defn}
		Let $\psi: \Rg \rightarrow \Rg$. Then $\psi$ is said to be an \textbf{Orlicz function} if 
		\begin{enumerate}
			\item $\psi$ is convex
			\item $\psi$ is increasing
			\item $\psi(x) \rightarrow \infty$ as $x \rightarrow \infty$
			\item $\psi(0) = 0$
		\end{enumerate}
	\end{defn}

	\begin{defn}
		Let $(\Om, \MF, P)$ be a probability space and $\psi: \Rg \rightarrow \Rg$ and Orlicz function. We define the \textbf{Orlicz $\psi$-norm}, denoted $\|\cdot\|_{\psi}: L^0(\Om, \MF, P) \rightarrow \RG$, by 
		$$\|X\|_{\psi} = \inf \{t >0: E [\psi(|X|/t)] \leq 1\}$$
		We define the \textbf{Orlicz $\psi$-space}, denoted $L^{\psi}(\Om, \MF, P)$, by 
		$$L^{\psi}(\Om, \MF, P) = \{X \in L^1(\Om, \MF, P): \|X\|_{\psi} < \infty\}$$
	\end{defn}

	\begin{ex}
		Let $(\Om, \MF, P)$ be a probability space and $\psi: \Rg \rightarrow \Rg$ an Orlicz function. Then $L^{\psi}$ is a vector space and $\|\cdot\|_{\psi}:L^{\psi} \rightarrow \Rg$ is a norm.\\
		\textbf{Hint:} note that
		\begin{itemize}
			\item for $s,t > 0$,
			$$\frac{|X|}{s + t} + \frac{|Y|}{s + t} = \frac{s}{s + t}\frac{|X|}{s} + \frac{t}{s + t}\frac{|Y|}{t}$$
			\item $\psi$ is star-shaped, i.e. for each $x, t \geq 0$ and $f(tx) \leq tf(x)$.
		\end{itemize}
	\end{ex}

	\begin{proof}
		For $X \in L^0(\Om, \MF, P)$, define $A_X \in \MF$ by 
		$$A_X = \{t > 0 : E [\psi(|X|/t)] \leq 1\}$$
		Let $X, Y \in L^{\psi}(\Om, \MF, P)$ and $\lam \in \C$. Since $\|X\|_{\psi} < \infty$ and $\|Y\|_{\psi} < \infty$, we have that $A_X \neq \varnothing$ and $A_Y \neq \varnothing$,
		\begin{enumerate}
			\item \textbf{subadditivity:} Let $\ep > 0$. Then there exists $s \in A_X$ and $t \in A_Y$ such that $s < \inf A_X + \ep/2$ and $t < \inf A_Y + \ep/2$. Since is convex and increasing, we have that
			\begin{align*}
				\psi \bigg( \frac{|X+Y|}{s +t} \bigg) 
				& \leq \psi \bigg( \frac{|X|}{s +t} +  \frac{|Y|}{s +t} \bigg) \\
				& = \psi \bigg( \frac{s}{s + t}\frac{|X|}{s} + \frac{t}{s + t}\frac{|Y|}{t} \bigg) \\
				& \leq \frac{s}{s + t} \psi \bigg( \frac{|X|}{s} \bigg) + \frac{t}{s + t} \psi \bigg( \frac{|Y|}{t} \bigg) \\
			\end{align*}
			Therefore, 
			\begin{align*}
				E\bigg[ \psi \bigg( \frac{|X+Y|}{s +t} \bigg)  \bigg]
				& \leq  \frac{s}{s + t} E \bigg[ \psi \bigg( \frac{|X|}{s} \bigg) \bigg] +  \frac{t}{s + t} E \bigg[ \psi \bigg( \frac{|Y|}{t} \bigg) \bigg] \\
				& \leq  \frac{s}{s + t} +  \frac{t}{s + t} \\
				& \leq 1
			\end{align*}
			Hence $s+t \in A_{X+Y}$. Thus $A_{X+Y} \neq \varnothing$. Since $s < \inf A_X + \ep/2$ and $t < \inf A_Y + \ep/2$, we have that 
			\begin{align*}
				\|X + Y\|_{\psi}
				& = \inf A_{X+Y} \\
				& \leq s + t \\ 
				& < \inf A_X + \inf A_Y + \ep \\
				& = \|X\|_{\psi} + \|Y \|_{\psi} + \ep
			\end{align*}
			Since $\ep >0$ is arbitrary, $	\|X + Y\|_{\psi} \leq \|X\|_{\psi} + \|Y \|_{\psi}$. So $X+Y \in L^{\psi}(\Om, \MF, P)$
			\item \textbf{absolute homogeneity:} Suppose that $\lam = 0$. Then for each $t >0$, 
			\begin{align*}
				E [\psi(|\lam X|/t)] 
				& = E[\psi(0)]\\
				& = E[0] \\
				& = 0 \\
				& \leq 1
			\end{align*}
			Thus
			\begin{align*}
				\|\lam X\|_{\psi} 
				& = 0 \\
				& = |\lam|\|X\|_{\psi}
			\end{align*}
			Suppose that $\lam \neq 0$. Since for each $t > 0$,
			$$\psi \bigg( \frac{|X|}{t} \bigg) = \psi \bigg( \frac{|\lam X|}{|\lam| t} \bigg)$$ 
			we have that for each $t > 0$, $t \in A_{X}$ iff $|\lam| t \in A_{\lam X}$. Therefore $A_{\lam X} = |\lam| A_{X}$ and 
			\begin{align*}
				\|\lam X\|_{\psi} 
				& = \inf A_{\lam X} \\
				& = |\lam| \inf A_X \\
				& = |\lam| \|X\|_{\psi} \\
			\end{align*}  
			So $|\lam X\| \in L^{\psi}(\Om, \MF, P)$.
 			\item \textbf{positive definiteness:} 
			Note that since $\psi$ is increasing, for each $t_0 >0$, $t_0 \in A_X$ implies that $[t_0, \infty) \subset A_X$. Suppose that $\|X\|_{\psi} = 0$. Then $(0, \infty) \subset A_X$. Jensen's inequality implies that for each $t > 0$,
			\begin{align*}
				\psi \bigg( \frac{E|X|}{t} \bigg) 
				& \leq E \bigg[\psi \bigg( \frac{|X|}{t} \bigg) \bigg] \\
				& \leq 1 
			\end{align*} 
			For the sake of contradiction, suppose that $E|X| > 0$. Since $\psi(x) \rightarrow \infty$ as $x \rightarrow \infty$, Choose $x > 0$ such that $\psi(x) > 1$. Set $t = E|X| /x$. Then $t > 0$ and 
			\begin{align*}
				1 
				& < \psi(x) \\
				& = \psi \bigg( \frac{E|X|}{t} \bigg) \\
				& \leq 1
			\end{align*}
			which is a contradiction. Hence $E|X| = 0$. Thus $X = 0$ a.s.  
		\end{enumerate}
	\end{proof}
	
	
	\begin{ex}
			Let $(\Om, \MF, P)$ be a probability space and $\psi: \Rg \rightarrow \Rg$ an Orlicz function. Then $L^{\psi}(\Om, \MF, P)$ is a Banach space.
	\end{ex}

	\begin{proof}
		
	\end{proof}
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	\newpage
	\section{Conditional Expectation and Probability}
	
	\subsection{Conditional Expectation}
	
	\begin{ex}
		Let $(\Om, \MF, P)$ be a probability space, $\MG$ a sub $\sig$-algebra of $\MF$ and $X \in L^1(\Om, \MF, P)$. Define $P_{\MG} = P|_{\MG}$ and $Q: \MG \rightarrow [0,\infty)$ by $Q(G) = \int_G X d P $. Then $Q \ll P_{\MG}$. 
	\end{ex}	
	
	\begin{proof}
		Let $G \in \MG$. Suppose that $P_{\MG}(G) = 0$. By definition, $P(G) = 0$. So $Q(G) = 0$ and $Q \ll P_{\MG}$.
	\end{proof}
	
	\begin{defn} 
	Let $(\Om, \MF, P)$ be a probability space, $\MG$ a sub $\sig$-algebra of $\MF$ and $X, Y \in L^1(\Om, \MF, P)$. Then $Y $ is said to be a \textbf{conditional expectation of $X$ given $\MG$} if 
	\begin{enumerate}
	\item $Y$ is $\MG$-measurable
	\item for each $G \in \MG$, 
	$$\int_G Y \dP = \int_G X \dP$$ 
	\end{enumerate}	 
	Since $(2)$ implies that conditional expectations of $X$ given $\MG$ are equal $P_{\MG}$-a.e., we write $Y = E(X|\MG)$.
	\end{defn}		

	\begin{note}
		Let $(\Om, \MF, P)$ be a probability space, $(S, \MS)$ a measurable space, $X \in L^1(\Om, \MF, P)$ and $Y \in L^0_S(\Om, \MF)$. We typically write $E(X|Y)$ instead of $E(X| Y^*\MS)$.
	\end{note}
	
	\begin{ex}\textbf{Existence of Conditional Expectation:}\\
	Let $(\Om, \MF, P)$ be a probability space, $\MG$ a sub $\sig$-algebra of $\MF$ and $X \in L^1(\Om, \MF, P)$. Define $Q$ and $P_{\MG}$ as in the previous exercise. Define $Y = dQ/dP_{\MG}$. Then $Y$ is a conditional expectation of $X$ given $\MG$.
	\end{ex}
	
	\begin{proof}
		The Radon-Nikodym theorem implies that $Y$ is $\MG$-measurable. Since $Q$ is finite, so is $|Q|$. Since $d|Q| = |Y|\dP_{\MG}$, we have that $Y \in L^1(\Om, \MG, P_{\MG})$. An exercise in section $3.3$ of \cite{measure}, implies that for each $G \in \MG$ 
		\begin{align*}
		\int_G Y \dP 
		&= \int_G Y \dP_{\MG} \\
		&= Q(G) \\
		&= \int_G X \dP
\end{align*}					
	\end{proof}	
	
	\begin{defn}
	Let $(\Om, \MF, P)$ be a probability space, $(S, \MS)$ a measurable space, $X \in L^1(\Om, \MF, P)$ and $Y \in L^0_S(\Om, \MF)$. Let $\phi \in L^0(Y(\Om), \MS \cap Y(\Om))$. Then $\phi$ is said to be a \textbf{conditional expectation function of $X$ given $Y$} if for each $B \in \MS \cap Y(\Om)$, $$\int_{Y^{-1}(B)} X \dP = \int_{B} \phi \dP_Y$$
	To denote this, we write $\phi(y) = E[X|Y = y]$.
	\end{defn}
	
	
	
	\begin{ex} \textbf{Existence of Conditional Expectation Function:} \\
	Let $(\Om, \MF, P)$ be a probability space, $(S, \MS)$ a measurable space, $X \in L^1(\Om, \MF, P)$ and $Y \in L^0_S(\Om, \MF)$. Suppose that for each $y \in S$, $\{y\} \in \MS$. Then there exists $\phi \in L^0(Y(S), \MS \cap Y(\Om))$ such that $\phi$ is a conditional expectation function of $X$ given $Y$.\\
	\textbf{Hint:} Doob-Dynkin lemma
	\end{ex}	
	
	\begin{proof}
	Since $E[X| Y] \in L^0(\Om, Y^*\MS)$, the Doob-Dynkin lemma implies that there exists $\phi \in L^0(Y(\Om), \MS \cap Y(\Om))$ such that $\phi \circ Y = E(X|Y)$. Let $B \in \MS \cap Y(\Om)$. Then 
	\begin{align*}
	\int_B \phi \dP_Y
	&= \int_{Y^{-1}(B)} \phi \circ Y \dP \\
	&= \int_{Y^{-1}(B)} E(X|Y) \dP \\
	&= \int_{Y^{-1}(B)} X \dP \\
	\end{align*}
	\end{proof}
	
	
	
	\subsection{Conditional Probability}
	
	\begin{defn}
	Let $(A, \MA)$ be a measurable space, $(B, \MB, P_Y)$ a probability space and $Q:B \times \MA \rightarrow [0,1]$. Then $Q$ is said to be a \textbf{stochastic transition kernel from $(B, \MB, P)$ to $(A, \MA)$} if 
	\begin{enumerate}
	\item for each $E \in \MA$, $Q(\cdot, E)$ is $\MB$-measurable
	\item for $P$-a.e. $b \in B$, $Q(b, \cdot)$ is a probability measure on $(A, \MA)$
	\end{enumerate}
	\end{defn}	
	
	\begin{defn}
	Let $(\Om, \MF, P)$ be a probability space, $X,Y \in L_n^0(\Om, \MF, P)$ and $Q: \R^n  \times \MF \rightarrow [0,1]$. Then $Q$ is said to be a \textbf{conditional probability distribution of $X$ given $Y$} if 
	\begin{enumerate}
	\item $Q$ is a stochastic transition kernel from $(\R^n, \MB(\R^n), P_Y)$ to $(\R^n, \MB(\R^n))$
	\item for each $A,B \in \MF$, $$\int_B Q(y, A) dP_Y(y) = P(X \in A, Y \in B)$$ 
	\end{enumerate}
	\end{defn}
	
	\begin{note}
	It is helpful to connect this notion of conditional probability with the elementary one by writing $Q(y, A) = P(X \in A| Y = y)$. If $P_Y \ll \mu$, then property $(2)$ in the definition becomes 
	\begin{align*}
	P(X \in A, Y \in B)  
	&=  \int_B Q(y, A) dP_Y(y) \\
	&= \int_B P(X \in A|Y=y) f_Y(y)d \mu(y) \\
\end{align*}	 
	as in a first course on probability.
	\end{note}	
	
	\begin{ex}
	Let $(\Om, \MF, P)$ be a probability space, $X,Y \in L_n^0$ and $Q: \R^n \times \MF \rightarrow [0,1]$. Suppose that for each $A \in \MF$, $Q(\cdot, A)$ is $\MB(\R^n)$-measurable, for $P_Y$-a.e. $y \in \R^n$, $P_{X|Y}(y, \cdot)$ is a probability measure on $(\Om, \MF)$ and $Q(Y, A) = P(X \in A|Y)$ a.e. Then $Q$ is a conditional probability of $X$ given $Y$.
	\end{ex}	
	
	\begin{proof}
	By assumption, for each $A \in \MF$, $Q(\cdot, A)$ is $\MB(\R^n)$-measurable and for $P_Y$-a.e. $y \in \R^n$, $Q(y, \cdot)$ is a probability measure on $(\Om, \MF)$. Let $A,B \in \MF$. Then
	\begin{align*}
	\int_B Q(y, A) dP_Y(y)
	&= \int_{Y^{-1}(B)} Q(Y(\om), A) dP(\om) \\
	&= \int_{Y^{-1}(B)}  P(X \in A|Y) dP \\
	&= \int_{Y^{-1}(B)}  E[1_{X^{-1}(A)}|Y] dP \\
	&= \int_{Y^{-1}(B)}  1_{X^{-1}(A)} dP \\
	&= \int 1_{X^{-1}(A)}  1_{Y^{-1}(B)} dP \\
	&= \int 1_{X^{-1}(A) \cap Y^{-1}(B)} dP \\
	&= P(X \in A, Y \in B)
	\end{align*}
	So $Q$ is a conditional probability distribution of $X$ given $Y$.
	\end{proof}
	
	\begin{defn}
	Let $(\Om, \MF, P)$ be a probability space, $X,Y \in L_n^0$ and $\mu$ a $\sig$-finite measure on $(\R^n, \MB(\R^n))$. Suppose that $P_X, P_Y \ll \mu$. Then $P_{X,Y} \ll \mu^2$. Let $f_X = dP_X/d \mu$, $f_Y = dP_Y/d \mu$ and $f_{X,Y} = dP_{X,Y}/d \mu^2$. Define $f_{X|Y}: \R^n \times \R^n$ by 
	\[
	f_{X|Y}(x,y) = 
	\begin{cases}
	\frac{f_{X,Y}(x,y)}{f_Y(y)},& y \in \supp Y \\
	0, &y \not \in \supp Y 
	\end{cases}
	\] 
	Then $f_{X|Y}$ is called the \textbf{conditional probability density of $X$ given $Y$}.
	\end{defn}	
	
	\begin{ex}
	Let $(\Om, \MF, P)$ be a probability space, $X,Y \in L_n^0$ and $\mu$ a $\sig$-finite measure on $(\R^n, \MB(\R^n))$. Suppose that $P_X, P_Y \ll \mu$. Define $Q: \R^n \times \MF \rightarrow [0,1]$ by 
	$$Q(y, A) = \int_A f_{X|Y}(x,y) d\mu(x)$$ 
	Then $Q$ is a conditional probability distribution of $X$ given $Y$.
	\end{ex}	
	
	\begin{proof}
	By the Fubini-Tonelli Theorem, for each $A \in \MF$, $Q(\cdot, A)$ is $\MB(\R^n)$-measurable and for $P_Y$-a.e. $y \in \R^n$, $Q(y, \cdot)$ is a probability measure on $(\Om, \MF)$. Let $A, B \in \MF$. Then 
	\begin{align*}
	\int_B Q(y, A) dP_Y(y)
	&= \int_B \bigg[ \int_A f_{X|Y}(x,y) d\mu(x) \bigg] dP_Y(y) \\
	&= \int_{B \cap \supp Y} \bigg[ \int_{A \cap \supp Y} \frac{f_{X,Y}(x,y)}{f_Y(y)} d\mu(x) f_Y(y) \bigg] d \mu(y) \\
	&= \int_{B \cap \supp Y}  \bigg[\int_A f_{X,Y}(x,y) d\mu(x) \bigg] d \mu(y) \\
	&= P(X \in A, Y \in B \cap \supp Y) \\
	&= P(X \in A, Y \in B) \\	
	\end{align*}
	\end{proof}
	
	\begin{thm}
	Let $(\Om, \MF, P)$ be a probability space, $X,Y \in L_n^1(\Om, \MF, P)$. Suppose that $\Img X \in \MB(\R^n)$. Then there exists a conditional probability distribution of $Y$ given $X$. 
	\end{thm}
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	\newpage
	\section{Markov Chains}	

	
	\begin{defn}
	Let $(\Om, \MF, P)$ be a probability space and $(X_n)_{n \in \N_0} \in L_n^0$. Then $(X_n)_{n \in \N_0}$ is said to be a \textbf{homogeneous Markov chain} if for each $A \in \MF$ and $n \in \N$, $P(X_n \in A| X_1, \cdots, X_{n-1}) = P(X_1 \in A| X_{0})$ a.e. 
	\end{defn}
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	\newpage
	\section{Probabilities Induced by Isometric Group Actions}
	
	\subsection{Applications to Bayesian Statistics}
	
	\begin{ex}
	Let $(\MX, \MA)$ be a measurable space $(\Theta, d)$ a metric space, $G$ a group, $\phi: G \times \Theta \rightarrow \Theta$ an isometric group action. Suppose that $\bar{d}$ is a metric on $\Theta / G$. Let 
	\begin{itemize}
	\item $H_p^{\Theta}$ be the Hausdorff measure on $\Theta$, $\mu_{\MX}$ a measure on $\MX$, 
	\item $p$ a denisty on $\Theta$ and for each $\theta \in \Theta$, $p(\cdot|\theta)$ a density on $\MX$. 
	\item $\theta_0 \in \Theta$ and for $j \in \N$, $X_j \sim p(x|\theta_0)$
	\end{itemize}
	Suppose that $\mu_{\Theta}$ is $G$-invariant, $p$ is $G$-invariant and continuous on $\Theta$ and for each $x \in \MX$, $p(x| \cdot)$ is $G$-invariant and continuous on $\Theta$. For $n \in \N$, set $p(\cdot|X^{(n)}) \propto f(X_1, \ldots, X_n| \cdot) p(\cdot)$. Define the posterior measure $P_{\Theta|X^{(n)}}: \MB(\Theta) \rightarrow [0, 1]$ by 
	\begin{equation*}
	d P_{\Theta|X^{(n)}} (\theta) = p(\theta |X^{(n)}) \, dH_p^{\Theta} (\theta)
	\end{equation*}
	Then there exists a density $\bar{p}(\cdot|X^{(n)})$ on $\Theta / G$ such that 
	\begin{equation*}
	d \bar{P}_{\Theta|X^{(n)}}(\theta) = \bar{p}(\theta |X^{(n)}) \, d\bar{H}^{\Theta} (\theta)
	\end{equation*}
	\end{ex}
	
	\begin{proof}
	Clear from previous work.
	\end{proof}
	
	\begin{ex}
	Let $(\MX, \MA)$ be a measurable space $(\Theta, d)$ a metric space, $G$ a group, $\phi: G \times \Theta \rightarrow \Theta$ an isometric group action. Suppose that $\bar{d}$ is a metric on $\Theta / G$. Let 
	\begin{itemize}
	\item $H_p^{\Theta}$ be the Hausdorff measure on $\Theta$, $\mu_{\MX}$ a measure on $\MX$, 
	\item $p$ a denisty on $\Theta$ and for each $\theta \in \Theta$, $p(\cdot|\theta)$ a density on $\MX$. 
	\item $\theta_0 \in \Theta$ and for $j \in \N$, $X_j \sim p(x|\theta_0)$
	\end{itemize}
	Suppose $p$ is $G$-invariant and continuous on $\Theta$ and for each $x \in \MX$, $p(x| \cdot)$ is $G$-invariant and continuous on $\Theta$. For $n \in \N$, set $p(\cdot|X^{(n)}) \propto f(X_1, \ldots, X_n| \cdot) p(\cdot)$. Define the posterior measure $P_{\Theta|X^{(n)}}: \MB(\Theta) \rightarrow [0, 1]$ by 
	\begin{equation*}
	d P_{\Theta|X^{(n)}} (\theta) = p(\theta |X^{(n)}) \, dH_p^{\Theta} (\theta)
	\end{equation*}
	Suppose that $(P_{\Theta|X^{(n)}})_{n \in \N}$ concentrates on $\bar{\theta}_0 \subset \Theta$ a.s. or in probability. Then $(\bar{P}_{\Theta|X^{(n)}})_{n \in \N}$ concentrates a.s. or in probability on $\{\bar{\theta_0}\} \subset \Theta / G$ (i.e. is consistent a.s. or in probability)
	\end{ex}
	
	\begin{proof}
	Let $V \in \MN_{\bar{\theta}_0}$. Then $\pi^{-1}(V) \in \MN_{\bar{\theta}_0}$. By definition, 
	\begin{align*}
	\bar{P}_{\Theta|X^{(n)}}(V^c)
	&= P_{\Theta|X^{(n)}}(\pi^{-1}(V^c)) \\
	&= P_{\Theta|X^{(n)}}(\pi^{-1}(V)^c) \\
	&\convt{a.s./$P$} 0 
	\end{align*}
	\end{proof}
	
	\begin{note}
	Some examples of $G$-invariant priors would be the uniform distribution, or $N_n(0, \sig^2I)$ on $\R^n$ when acted on by $O(n)$. An example of a $G$-invariant likelihood would be $f(A|Z) \sim \text{Ber}(ZZ^T)$ as in a latent position random graph model where $Z \in \R^{n \times d}$ is the parameter is invariant under right multiplication by $U \in O_d$.
	\end{note}
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	\newpage
	\section{Projective Systems}
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	\newpage
	\section{Stochastic Integration}
	\begin{ex}
	Let $(\Om, \MF, P)$ be a probability space, $X$ a set $\MA_0$ an algebra, $\mu_0:\MA_0 \rightarrow \C$ and $B:\MA_0 \rightarrow L^2(\Om, \MF, P)$. 
	Suppose that 
	\begin{enumerate}
	\item $B(\varnothing) = 0$
	\item for each $E, F \in \MA_0$, if $E \cap F = \varnothing$, then $B(E \cup F) = B(E) + B(F)$
	\item $E(B(E)B(F)^*) = \mu_0(E \cap F)$
	\end{enumerate}
	Then 
	\begin{enumerate}
	\item for each $E \in \MA_0$, $\mu_0(E) = E(|B(E)|^2)$.
	\item for each $E \in \MA_0$, $0 \leq \mu_0(E) < \infty$ 
	\item for each $E, F \in \MA_0$, if $E \cap F = \varnothing$, then $\mu_0(E \cup F) = \mu_0(E) + \mu_0(F)$
	\end{enumerate}
	\end{ex}
	
	\begin{proof}\
	\begin{enumerate}
	\item Clear 
	\item Clear
	\item Let $E, F \in \MA_0$. Suppose that $E \cap F = \varnothing$. Then 
	\begin{align*}
	E(B(E)B(F)^*) 
	&= \mu_0(E \cap F) \\
	&= \mu_0(\varnothing) \\
	&= E(|B(\varnothing)|^2) \\
	&= E(0) \\
	&= 0
	\end{align*}
	This implies that 
	\begin{align*}
	\mu_0(E \cup F) 
	&= E(|B(E \cup F)|^2) \\
	&= E(|B(E) + B(F)|^2) \\
	&= E(|B(E)|^2) + E(|B(F)|^2) + 2 \Re E(B(E)B(F)^*) \\
	&= \mu_0(E) + \mu_0(F) +0  \\
	&=  \mu_0(E) + \mu_0(F) 
	\end{align*}
	\end{enumerate}
	\end{proof}
	
	\begin{defn}
	Let $(\Om, \MF, P)$ be a probability space, $X$ a set $\MA_0$ an algebra, $\mu_0:\MA_0 \rightarrow \Rg$ a premeasure and $B:\MA_0 \rightarrow L^2(\Om, \MF, P)$. 
	Suppose that 
	\begin{enumerate}
	\item $B(\varnothing) = 0$
	\item for each $E, F \in \MA_0$, if $E \cap F = \varnothing$, then $B(E \cup F) = B(E) + B(F)$
	\item $E (B(E)B(F)^*) = \mu_0(E \cap F)$
	\end{enumerate}
	Then $B$ is said to be a \textbf{stochastic premeasure with sturcture $\mu_0$}
	
	\end{defn}
	





















	\newpage
		\section{TODO}
	Incorporate vector measures to have conditional expectations of Banach space valued functions given a $\sig$-algebra
	

	
	
	
	
	
	\newpage
	\begin{thebibliography}{4}
\bibitem{analysis}  \href{https://github.com/carsonaj/Mathematics/blob/master/Introduction\%20to\%20Analysis/Introduction\%20to\%20Analysis.pdf}{Introduction to Analysis}	
	
\bibitem{groups} \href{https://github.com/carsonaj/Mathematics/tree/master/Introduction\%20to\%20Group\%20Theory}{Introduction to Group Theory}

\bibitem{measure}  \href{https://github.com/carsonaj/Mathematics/blob/master/Introduction\%20to\%20Measure\%20and\%20Integration/Introduction\%20to\%20Measure\%20and\%20Integration.pdf}{Introduction to Measure and Integration}


\end{thebibliography}
	
	
\end{document}