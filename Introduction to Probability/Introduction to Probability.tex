\documentclass[12pt]{amsart}
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb,setspace, mathtools, enumitem}
\usepackage{color}   %May be necessary if you want to color links
\usepackage{hyperref}
\hypersetup{
	colorlinks=true, %set true if you want colored links
	linktoc=all,     %set to all if you want both sections and subsections linked
	linkcolor=black,  %choose some color if you want links to stand out
}


%
%
%
\newif\ifhideproofs
%\hideproofstrue %uncomment to hide proofs
%
%
%
%
\ifhideproofs
\usepackage{environ}
\NewEnviron{hide}{}
\let\proof\hide
\let\endproof\endhide
\fi

\theoremstyle{definition}
\newtheorem{definition}{Definition}[subsection]
\newtheorem{defn}[definition]{Definition}
\newtheorem{note}[definition]{Note}
\newtheorem{thm}[definition]{Theorem}
\newtheorem{lem}[definition]{Lemma}
\newtheorem{prop}[definition]{Proposition}
\newtheorem{cor}[definition]{Corollary}
\newtheorem{conj}[definition]{Conjecture}
\newtheorem{ex}[definition]{Exercise}



\DeclareMathOperator{\supp}{supp}

\renewcommand{\Re}{\text{Re}}

\newcommand{\al}{\alpha}
\newcommand{\Gam}{\Gamma}
\newcommand{\be}{\beta} 
\newcommand{\del}{\delta} 
\newcommand{\Del}{\Delta}
\newcommand{\lam}{\lambda}  
\newcommand{\Lam}{\Lambda} 
\newcommand{\ep}{\epsilon}
\newcommand{\sig}{\sigma} 
\newcommand{\om}{\omega}
\newcommand{\Om}{\Omega}
\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\T}{\mathbb{T}}
\newcommand{\Q}{\mathbb{Q}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\MA}{\mathcal{A}}
\newcommand{\MC}{\mathcal{C}}
\newcommand{\MB}{\mathcal{B}}
\newcommand{\MF}{\mathcal{F}}
\newcommand{\MG}{\mathcal{G}}
\newcommand{\ML}{\mathcal{L}}
\newcommand{\MN}{\mathcal{N}}
\newcommand{\MS}{\mathcal{S}}
\newcommand{\MP}{\mathcal{P}}
\newcommand{\ME}{\mathcal{E}}
\newcommand{\MT}{\mathcal{T}}
\newcommand{\MM}{\mathcal{M}}
\newcommand{\MI}{\mathcal{I}}

\newcommand{\E}{\text{E}}

\newcommand{\io}{\text{ i.o.}}
\newcommand{\ev}{\text{ ev.}}
\renewcommand{\r}{\rangle}
\renewcommand{\l}{\langle}

\newcommand{\RG}{[0,\infty]}
\newcommand{\Rg}{[0,\infty)}
\newcommand{\Ll}{L^1_{\text{loc}}(\R^n)}

\newcommand{\limfn}{\liminf \limits_{n \rightarrow \infty}}
\newcommand{\limpn}{\limsup \limits_{n \rightarrow \infty}}
\newcommand{\limn}{\lim \limits_{n \rightarrow \infty}}
\newcommand{\convt}[1]{\xrightarrow{\text{#1}}}
\newcommand{\conv}[1]{\xrightarrow{#1}} 
\newcommand{\seq}[2]{(#1_{#2})_{#2 \in \N}}

\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\spn}{span}
\DeclareMathOperator{\Img}{Im}



\begin{document}
	
	\title{Introduction to Probability}
	\author{Carson James}
	\maketitle
	
	\tableofcontents
	
	\section{Introduction}
	
	\subsection{Purpose}
	
	\section{Probability Framework}
	
	\section{Probability}
	\subsection{Distributions}
	
	\begin{defn}
		Let $\Om$ be a set and $\MP \subset \MP(X)$. Then $\MP$ is said to be a \textbf{$\pi$-system} on $\Om$ if for each $A,B \in \MP$, $A \cap B \in \MP$.
	\end{defn}
	
	\begin{defn}
		Let $Om$ be a set and $\ML \subset \MP(\Om)$. Then $\ML$ is said to be a \textbf{$\lam$-system} on $\Om$ if 
		\begin{enumerate}
			\item $\ML \neq \varnothing$
			\item for each $A \in \ML$, $A^c \in \ML$
			\item for each $(A_n)_{n \in \N} \subset \ML$, if $(A_n)_{n \in \N}$ is disjoint, then $\bigcup\limits_{n \in \N}A_n \in \ML$
		\end{enumerate}
	\end{defn}
	
	\begin{ex}
		Let $\Om$ be a set and $\ML$ a $\lam$-system on $\Om$. Then 
		\begin{enumerate}
			\item $\Om, \varnothing \in \ML$
		\end{enumerate} 
	\end{ex}
	
	\begin{proof}
		Straightforward.
	\end{proof}
	
	\begin{defn}
		Let $\Om$ be a set and $\MC \subset \MP(\Om)$. Put $$\MS = \{\ML \subset \MP(\Om): \ML \text{ is a }\lam\text{-system on }\Om \text{ and } \MC \subset \ML\}$$ We define the \textbf{$\lam$-system on $\Om$ generated by $\MC$}, $\lam(\MC)$, to be $$\lam(\MC) = \bigcap_{\ML \in \MS}\ML$$
	\end{defn}
	
	\begin{ex}
		Let $\Om$ be a set and $\MC \subset \MP(\Om)$. If $\MC$ is a $\lam$-system and $\MC$ is a $\pi$-system, then $\MC$ is a $\sig$-algebra.
	\end{ex}
	
	\begin{proof}
		Suppose that $\MC$ is a $\lam$-system and $\MC$ is a $\pi$-system. Then we need only verify the third axiom in the definition of a $\sig$-algebra. Let $(A_n)_{n \in \N} \subset \MC$. Define $B_1 = A_1$ and for $n \geq 2$, define $B_n = A_n \cap \bigg( \bigcup\limits_{k=1}^{n-1}A_k \bigg)^c = A_n \cap \bigg( \bigcap\limits_{k=1}^{n-1}A_k^c \bigg) \in \MC$. Then $(B_n)_{n \in \N}$ is disjoint and therefore $\bigcup\limits_{n \in \N}A_n = \bigcup\limits_{n \in \N}B_n \in \MC$.
	\end{proof}
	
	\begin{thm}(Dynkin's Theorem) \\
		Let $\Om$ be a set.
		\begin{enumerate}
			\item Let $\MP$ be a $\pi$-system on $\Om$ and $\ML$ a $\lam$-system on $\Om$. If $\MP \subset \ML$, then $\sig(\MP) \subset \ML$.
			\item Let $\MP$ be a $\pi$-system on $\Om$. Then $\sig(\MP) = \lam(\MP)$
		\end{enumerate} 
		
	\end{thm}
	
	\begin{ex}
		Let $(\Om, \MF)$ be a measurable space and $\mu, \nu$ probability measures on $(\Om, \MF)$. Put $\ML_{\mu,\nu} = \{A \in \MF: \mu(A) = \nu(A)\}$. Then $\ML_{\mu, \nu}$ is a $\lam$-system on $\Om$.
	\end{ex}
	
	\begin{proof}\
		\begin{enumerate}
			\item $\varnothing \in \ML_{\mu, \nu}$.
			\item Let $A \in \ML_{\mu, \nu}$. Then $\mu(A) = \nu(A)$. Thus 
			\begin{align*}
				\mu(A^c) 
				&= 1-\mu(A) \\
				&= 1 -\nu(A) \\
				&= \nu(A^c)
			\end{align*}
			So $A^c \in \ML_{\mu, \nu}$. 
			\item Let $(A_n)_{n \in \N} \subset \ML_{\mu, \nu}$. So for each $n \in \N$, $\mu(A_n) = \nu(A_n)$.  Suppose that $(A_n)_{n \in \N}$ is disjoint. Then 
			\begin{align*}
				\mu\bigg(\bigcup_{n \in \N} A_n\bigg) 
				&= \sum_{n \in \N} \mu(A_n) \\
				&= \sum_{n \in \N} \nu(A_n) \\
				&= \nu\bigg(\bigcup_{n \in \N} A_n\bigg) 
			\end{align*}
			Hence $\bigcup_{n \in \N} A_n \in \ML_{\mu, \nu}$.
		\end{enumerate}
	\end{proof}
	
	\begin{ex}
		Let $(\Om, \MF)$ be a measurable space, $\mu, \nu$ probability measures on $(\Om, \MF)$ and $\MP \subset \MA$ a $\pi$-system on $\Om$. Suppose that for each $A \in \MP$, $\mu(A) = \nu(A)$. Then for each $A \in \sig(\MP)$, $\mu(A) = \nu(A)$.
	\end{ex}
	
	\begin{proof}
		Using the previous exercise, we see that $\MP \subset \ML_{\mu, \nu}$. Dynkin's theorem implies that $\sig(\MP) \subset \ML_{\mu, \nu}$. So for each $A \in \sig(\MP)$, $\mu(A) = \nu(A)$.
	\end{proof}
	
	
	
	
	
	
	
	
	\begin{defn}
		Let $F: \R \rightarrow \R$. Then $F$ is said to be a \textbf{probability distribution function} if 
		\begin{enumerate}
			\item $F$ is right continuous
			\item $F$ is increasing
			\item $F(-\infty)  = 0$ and $F(\infty)  = 1$
		\end{enumerate}
	\end{defn}
	
	\begin{defn}
		Let $P $ be a probability measure on $(\R, \MB(\R))$. We define $F_P: \R \rightarrow \R$, by $$F_P(x) = P((-\infty, x])$$ We call $F_P$ the \textbf{probability distribution function of $P$}.
	\end{defn}
	
	\begin{ex}
		Let $(\Om, \MF, P)$ be a probability measure. Then $F_P$ is a probability distribution function.
	\end{ex}
	
	\begin{proof}
		\begin{enumerate}
			\item Let $x \in \R$ and $(x_n)_{n \in \N} \subset [x, \infty)$. Suppose that $x_n \rightarrow x$. Then $(x, x_n] \rightarrow \varnothing$ because $\limsup\limits_{n \rightarrow \infty} (x,x_n] = \varnothing$. Thus $$F(x_n) - F(x) = P((x, x_n]) \rightarrow P(\varnothing) = 0$$This implies that $$F(x_n) \rightarrow F(x)$$. So $F$ is right continuous.
			\item Clearly $F_P$ is increasing.
			\item Continuity from below tells us that $$F(-\infty) = \lim_{n \rightarrow -\infty}F(n) = \lim_{n \rightarrow -\infty}P((-\infty,n]) = 0$$ and continuity from above tell us that $$F(\infty)  = \lim_{n \rightarrow \infty}F(n) = \lim_{n \rightarrow \infty}P((-\infty, n]) = 1$$ 
		\end{enumerate}
	\end{proof}
	
	\begin{ex}
		Let $\mu, \nu$ be probability measures on $(\R, \MB(\R))$. Then $F_{\mu} = F_{\nu}$ iff $\mu = \nu$.  
	\end{ex}
	
	\begin{proof}
		Clearly if $\mu = \nu$, then $F_{\mu} = F_{\nu}$. Conversely, suppose that $F_{\mu} = F_{\nu}$. Then for each $x \in \R$, 
		\begin{align*}
			\mu((-\infty,x]) 
			&= F_{\mu}(x) \\
			&= F_{\nu}(x)  \\
			&= \nu((-\infty,x])
		\end{align*}
		Put $\MC = \{(-\infty,x]:x \in \R\} $. Then $\MC$ is a $\pi$-system and for each $A \in \MC$, $\mu(A) = \nu(A)$. Hence for each $A \in \sig(C) = \MB(\R)$, $\mu(A) = \nu(A)$. So $\mu = \nu$. 
	\end{proof}
	
	\begin{defn}
		Let $(\Om, \MF, P)$ be a probability space and $X:\Om \rightarrow \R^n$. Then $X$ is said to be a \textbf{random vector} on $(\Om, \MF)$ if $X$ is $\MF$-$\MB(\R^n)$ measurable. If $n = 1$, then $X$ is said to be a \textbf{random variable}. We define $$L_n^0(\Om, \MF, P) = \{X:\Om \rightarrow \R^n: X \text{ is a random vector}\}$$ and $$L_n^p(\Om, \MF, P) = \bigg \{X \in L_n^0: \int \|X\|^p dP < \infty \bigg \} $$ 
	\end{defn}
	
	\begin{defn}
		Let $(\Om, \MF, P)$ be a probability space and $X$ a random variable on $(\Om,\MF)$. We define the \textbf{probability distribution} of $X$, $P_X:\MB(R) \rightarrow [0,1]$, to be the measure $$P_X = X_*P$$ That is, for each $A \in \MB(\R)$, $$P_X(A) = P(X^{-1}(F))$$ \\ We define the \textbf{probability distribution function} of $X$, $F_X:\R \rightarrow [0,1]$, to be $$F_X = F_{P_X}$$
	\end{defn}
	
	\begin{defn}
		Let $(\Om, \MF, P)$ be a probability space and $X$ a random variable on $(\Om,\MF)$. If $P_X \ll m$, we define the \textbf{probability density} of $X$, $f_X: \R \rightarrow \R$, by $$f_X = \frac{dP_X}{dm}$$ 
	\end{defn}
	
	\begin{ex}
		Let $(\Om, \MF, P)$ be a probability space and $(X_n)_{n \in \N}$ be a sequence of random variables on $(\Om, \MF)$. Then for each $x \in \R$, $$\P\bigg(\liminf_{n \rightarrow \infty}X_n > x\bigg) \leq \liminf_{n \rightarrow \infty} P(X_n > x)$$  
	\end{ex}
	
	\begin{proof}
		Let $\om \in \bigg \{ \liminf\limits_{n \rightarrow \infty} X_n > x \bigg \}$. Then $x< \liminf\limits_{n \rightarrow \infty} X_n (\om) = \sup\limits_{n \in \N} \bigg( \inf\limits_{k \geq n} X_k(\om)\bigg)$. So there exists $n^* \in \N$ such that $x< \inf\limits_{k \geq n^*} X_k(\om)$. Then for each $k \in \N$, $k \geq n^*$ implies that $x < X_k(\om)$. So there exists $n^* \in \N$ such that for each $k \in \N$, $k \geq n^*$ implies that $\mathbf{1}_{\{X_k > x\}}(\om) =1$. Hence $\inf\limits_{k \geq n^*} \mathbf{1}_{\{X_k > x\}}(\om)  = 1$. Thus  $\liminf\limits_{n \rightarrow \infty} \mathbf{1}_{\{X_k > x\}}(\om) = \sup\limits_{n \in \N} \bigg( \inf_{k \geq n} \mathbf{1}_{\{X_k > x\}}(\om) \bigg) = 1$. Therefore $\om \in \liminf\limits_{n \rightarrow \infty} \{X_k > x\}$ and we have shown that $$\bigg \{ \liminf\limits_{n \rightarrow \infty} X_n > x \bigg \} \subset \liminf\limits_{n \rightarrow \infty} \{X_k > x\}$$ Then 
		\begin{align*}
			P \bigg( \liminf\limits_{n \rightarrow \infty} X_n > x \bigg)
			& \leq P \bigg( \liminf\limits_{n \rightarrow \infty} \{X_k > x\} \bigg) \\
			& \leq \liminf_{n \rightarrow \infty} P(\{X_k > x\})
		\end{align*}
	\end{proof}
	
	
	\begin{defn}
		Let $(\Om, \MF, P)$ be a probability space and $X \in L^+(\Om) \cup L^1$. Define the \textbf{expectation of X}, $E[X]$, to be $$\E[X] = \int X dP$$.
	\end{defn}
	
	
	\subsection{Independence}
	
	\begin{defn}
		Let $(\Om, \MF, P)$ be a probability space and $\MC \subset \MF$. Then $\MC$ is said to be \textbf{independent} if for each $(A_i)_{i=1}^n \subset \MC$, $$P \bigg( \bigcap_{k=1}^nA_k\bigg) = \prod_{k=1}^{n}P(A_k)$$
	\end{defn}
	
	\begin{defn}
		Let $(\Om, \MF, P)$ be a probability space and $\MC_1, \cdots, \MC_n \subset \MF$. Then $\MC_1, \cdots, \MC_n $ are said to be \textbf{independent} if for each $A_1 \in \MC_1,  \cdots, A_n \in \MC_n$, $A_1, \cdots, A_n$ are independent. 
	\end{defn}
	
	\begin{note}
		We will explicitely say that for each $i=1, \cdots , n$, $\MC_i$ is independent when talking about the independence of the elements of $\MC_i$ to avoid ambiguity.
	\end{note}
	
	\begin{defn}
		Let $(\Om, \MF, P)$ be a probability space and $X_1, \cdots, X_2$ random variables on $(\Om, \MF)$. Then $X_1, \cdots, X_n$ are said to be \textbf{independent} if for each $B_1, \cdots, B_n \in \MB(\R)$, $X_1^{-1}B_1, \cdots, X_n^{-1}B_n$ are independent.
	\end{defn}
	
	\begin{ex}
		Let $(\Om, \MF, P)$ be a probability space and $X_1, \cdots, X_n$ random variables on $(\Om, \MF)$. Then $X_1, \cdots, X_n$ are independent iff $\sig(X_1), \cdots, \sig(X_n)$ are independent.
	\end{ex}
	
	\begin{proof}
		Suppose that $X_1, \cdots, X_n$ are independent. Let $A_1, \in \sig(X_1), \cdots, A_n \in \sig(A_n)$. Then for each $i = 1, \cdots, n$, there exists $B_i \in \MB(\R)$ such that $A_i = X_i^{-1}(B_i)$. Then $A_1, \cdots, A_n$ are independent. Hence $\sig(X_1), \cdots, \sig(X_n)$ are independent. Conversely, suppose that $\sig(X_1), \cdots, \sig(X_n)$ are independent. Let $B_1, \cdots, B_n \in \MB(\R)$. Then for each $i = 1, \cdots, n$, $X_i^{-1}B_i \in \sig(X_i)$. Then $X_1^{-1}B_1, \cdots, X_n^{-1}B_n$ are independent. Hence $X_1, \cdots, X_n$ are independent.
	\end{proof}
	
	\begin{ex}
		Let $(\Om, \MF, P)$ be a probability space, $X_1, \cdots, X_n$ random variables on $(\Om, \MF)$ and $\MF_1, \cdots, \MF_n \subset \MF$ a collection of $\sig$-algebras on $\Om$. Suppose that for each $i = 1, \cdots, n$, $X_i$ is $\MF_i$-measurable. If $\MF_1, \cdots, \MF_n$ are independent, then $X_1, \cdots, X_n$ are independent. 
	\end{ex}
	
	\begin{proof}
		For each $i =1, \cdots, n$, $\sig(X_i) \subset \MF_i$. So $\sig(X_1), \cdots, \sig(X_n)$ are independent. Hence $X_1, \cdots, X_n$ are independent.
	\end{proof}
	
	\begin{ex}
		Let $(\Om, \MF, P)$ be a probability space and $\MC_1, \cdots, \MC_n \subset \MF$. Suppose that for each $i = 1, \cdots, n$, $\MC_i$ is a $\pi$-system and $\MC_1, \cdots, \MC_n$ are independent, then $\sig(\MC_1), \cdots, \sig(\MC_n)$ are independent.
	\end{ex}
	
	\begin{proof}
		Let $A_2 \in \MC_2$. Define $\ML = \{A \in \MF: P(A\cap A_2) = P(A)P(A_2)\}$. Then 
		\begin{enumerate}
			\item $\Om \in \ML$
			\item If $A \in \ML$, then 
			\begin{align*}
				P(A^c \cap A_2) 
				&= P(A_2) - P(A_2 \cap A) \\
				&= P(A_2) - P(A_2) P(A) \\
				&= (1- P(A))P(A_2) \\
				&= P(A^c)P(A_2)
			\end{align*}
			So $A^c \in \ML$
			\item If $(B_n)_{n \in \N} \subset \ML$ is disjoint, then 
			\begin{align*}
				P\bigg( \bigg[\bigcup_{n \in \N}B_n \bigg] \cap A_2\bigg) 
				&= P \bigg( \bigcup_{n \in \N}B_n \cap A_2 \bigg) \\
				&= \sum_{n \in \N}P(B_n \cap A_2) \\
				&= \sum_{n \in \N}P(B_n) P(A_2) \\
				&=   \bigg[\sum_{n \in \N}P(B_n)\bigg]P(A_2)  \\
				&=  P\bigg( \bigcup_{n \in \N} A_n\bigg) P(A_2) 
			\end{align*} 
			So $\bigcup\limits_{n \in \N}B_n \in \ML$. 
		\end{enumerate}
		Thus $\ML$ is a $\lam$-system. Since $\MC_1 \subset \ML$ is a $\pi$-system, Dynkin's theorem tells us that $\sig(\MC_1) \subset \ML$. Since $A_2 \in \MC_2$ is arbitrary $\sig(\MC_1)$ and $\MC_2$ are independent. The same reasoning implies that $\sig(\MC_1)$ and $\sig(\MC_2)$ are independent. Let $A_2 \in \MC_1, \cdots, A_n \in \MC_n$ We may do the same process with $$\ML = \bigg \{A \in \MF: P\bigg(A \cap \bigg(\bigcap_{i=2}^n A_i\bigg) \bigg) = P(A)\prod_{i=2}^n P(A_i)\bigg\}$$ and conclude that $\sig(\MC_1), \MC_2, \cdots, \MC_n$ are independent. Which, using the same reasoning would imply that $\sig(\MC_1), \cdots, \sig(\MC_n)$ are independent.
	\end{proof}
	
	\begin{ex}
		Let $(\Om, \MF, P)$ be a probability space, $X_1, \cdots, X_n$ random variables on $(\Om, \MF)$. Then $X_1, \cdots, X_n$ are independent iff for each $x_1, \cdots, x_n \in \R$, $$P(X_1 \leq x_1, \cdots, X_n \leq x_n ) = \prod_{i=1}^nP(X_i \leq x_i)$$
	\end{ex}
	
	\begin{proof}
		Suppose that $X_1, \cdots, X_n$ are independent. Then $\sig(X_1), \cdots, \sig(X_n)$ are independent. Let $x_1, \cdots, x_n \in \R$. Then for each $i=1, \cdots, n$, $\{X_i \leq x_i\} \in \sigma(X_i)$. Hence \\$P(X_1 \leq x_1, \cdots, X_n \leq x_n ) = \prod\limits_{i=1}^nP(X_i \leq x_i)$. Conversely, suppose that for each \\$x_1, \cdots, x_n \in \R$, $P(X_1 \leq x_1, \cdots, X_n \leq x_n ) = \prod\limits_{i=1}^nP(X_i \leq x_i)$. Define $\MC = \{ (-\infty, x]: x \in \R \}$. Then $\MB(\R) = \sig(\MC)$. For each $i =1, \cdots, n$, define $\MC_i = X_i^{-1}\MC$. Then for each $i =1, \cdots, n$, $\MC_i$ is a $\pi$-system and 
		\begin{align*}
			\sig(\MC_i) 
			&= \sig(X^{-1}(\MC)) \\
			&= X_i^{-1}(\sig(\MC)) \\
			&= X_i^{-1}(\MB(\R)) \\
			&= \sig(X_i)
		\end{align*}
		By assumption, $\MC_1, \cdots, \MC_n$ are independent. The previous exercies tells us that $\sig(X_1), \cdots, \sig(X_n)$ are independent. Then $X_1, \cdots, X_n$ are independent. 
	\end{proof}
	
	\begin{ex}
		Let Let $(\Om, \MF, P)$ be a probability space and $X_1, \cdots, X_n$ random variables on $(\Om, \MF)$. Define $X = (X_1, \cdots, X_n)$. If $X_1, \cdots, X_n$ are independent, then $$P_{X} = \prod\limits_{i=1}^nP_{X_i}$$.
	\end{ex}
	
	\begin{proof}
		Let $A_1, \cdots, A_n \in \MB(\R)$. Then 
		\begin{align*}
			P_X(A_1 \times \cdots \times A_n) 
			&= P(X \in A_1 \times \cdots \times \in A_n)\\
			&= P(X_1 \in A_1, \cdots, X_n \in A_n) \\
			&= P(X_1 \in A_1) \cdots P(X_n \in A_n) \\
			&= P_{X_1}(A_1) \cdots P_{X_n}(A_n) \\
			&= \prod_{i=1}^nP_{X_i}(A_1 \times \cdots \times  A_n)
		\end{align*}
		Put $$\MP = \{ A_1 \times \cdots \times A_n: A_1 \in \MB(R), \cdots, A_n \in \MB(R) \}$$ Then $\MP$ is a $\pi$-system and $$\sig(\MP) = \MB(R) \otimes \cdots \otimes \MB(R) = \MB(\R^n)$$
		A previous exercise then tells us that $P_X = \prod\limits_{i=1}^nP_{X_i}$ 
	\end{proof}
	
	\begin{ex}
		Let Let $(\Om, \MF, P)$ be a probability space, $X_1, \cdots, X_n$ random variables on $(\Om, \MF)$ and $f_1, \cdots, f_n: \R \rightarrow \R \in L^0$. Suppose that $f_1 \circ X_1, \cdots, f_n \circ X_n\in L^+(\Om)$ or $f_1 \circ X_1, \cdots, f_n \circ X_n \in L^1(\Om)$. If $X_1, \cdots, X_n$ are independent, then $$E[f_1(X_1) \cdots f_n(X_n)] = \prod_{i=1}^n E[f_i(X_i)]$$
	\end{ex}
	
	\begin{proof}
		Define the random vector $X : \Om \rightarrow \R^n$ by $X = (X_1, \cdots, X_n)$ and $g:\R^n \rightarrow \R$ by $g(x_1, \cdots, x_n) = f_1(x_1) \cdots f_n(x_n)$. Suppose that for each $i = 1, \cdots, n$, $f_i \in L^+(\R)$. Then $g \in L^+(\R^n)$ and by change of variables,  
		\begin{align*}
			E[f_1(X_1) \cdots f_n(X_n)] 
			&= E[g(X)] \\
			&= \int_{\Om} g \circ X dP \\
			&= \int_{\R^n} g(x) dP_X(x) \\
			&= \int_{R^n} g(x) d \prod\limits_{i=1}^nP_{X_i}(x) \\
			&=  \prod_{i=1}^n \int_{\R}f_i(x) d P_{X_i}(x) \\ 
			&= \prod_{i=1}^n \int_{\Om}f_i \circ X d P \\
			&= \prod_{i=1}^n E[f_i(X_i)]
		\end{align*}
		If for each $i = 1, \cdots, n$, $f_i \in L^1(\R, P_{X_i})$, then following the above reasoning with $\vert g \vert $ tells us that $g \in L^1(\R^n, P_X)$ and we use change of variables and Fubini's theorem to get the same result.  
	\end{proof}
	
	\subsection{$L^p$ Spaces for Probability}
	
	\begin{note}
		Recall that for a probability space $(\Om, \MF, P)$ and $1 \leq p \leq q \leq \infty$ we have $L^q \subset L^p$ and for each $X \in L^q, \|X \|_p \leq  \|X\|_q$. Also recall that for $X,Y \in \L^2$, we have that $\|XY \|_1 \leq \|X\|_2 \|X\|_2$.
	\end{note}
	
	\begin{defn}
		Let $(\Om, \MF, P)$ be a probability space and $X \in L^2$. Define the \textbf{variance of X}, $Var(X)$, to be $$Var(X) = \E\big[\big(X-E[X]\big)^2\big]$$.
	\end{defn}
	
	\begin{defn}
		Let $(\Om, \MF, P)$ be a probability space and $X,Y \in L^2$. Define the  
	\end{defn}
	
	
	\begin{defn}
		Let $(\Om, \MF, P)$ be a probability space and $X,Y \in L^2$. Define the \textbf{covariance of $X$ and $Y$}, $Cov(X,Y)$, to be $$Cov(X,Y) = E[(X-E[X])(Y-E[Y])]$$
	\end{defn}
	
	\begin{ex}
		Let $(\Om, \MF, P)$ be a probability space and $X,Y \in L^2$. Then the covariance is well defined and $Cov(X,Y) ^2 \leq Var(X)Var(Y)$
		
		\begin{proof}
			By Holder's inequality, 
			\begin{align*}
				\vert Cov(X,Y) \vert 
				&= \bigg \vert \int (X-E[X])(Y-E[Y]) dP \bigg \vert \\
				&\leq \int \vert (X-E[X])(Y-E[Y]) \vert dP \\
				&= \|(X-E[X])(Y-E[Y]) \|_1 \\
				&\leq \|X-E[X] \|_2 \|(Y-E[Y] \|_2  \\
				&= \bigg(\int \vert X - E[X] \vert^2 dP\bigg)^{\frac{1}{2}} \bigg( \vert Y - E[Y] \vert^2 \bigg)^{\frac{1}{2}} \\
				&= Var(X)^{\frac{1}{2}}Var(Y)^{\frac{1}{2}}
			\end{align*}
			
			So $Cov(X,Y) ^2 \leq Var(X)Var(Y)$.
		\end{proof}
		
	\end{ex}
	
	\begin{ex}
		Let $(\Om, \MF, P)$ be a measure space and $X,Y \in L^2$. Then
		\begin{enumerate}
			\item $Cov(X,Y) = E[XY]-E[X]E[Y]$
			\item If $X,Y$ are independent, then $Cov(X,Y) = 0$
			\item $Var(X) = E[X^2] - E[X]^2$
			\item for each $a,b \in \R$, $Var(aX + b) = a^2Var(X)$.
			\item $Var(X+Y) = Var(X) + Var(Y) + 2 Cov(X,Y)$
		\end{enumerate}
	\end{ex}
	
	\begin{proof}\
		\begin{enumerate}
			\item We have that
			\begin{align*}
				Cov(X,Y) 
				&= E[(X-E[X])(Y-E[Y])] \\
				&= E[XY -E[Y]X - E[X]Y + E[X]E[Y]] \\
				&= E[XY] - E[X]E[Y] - E[X]E[Y] + E[X]E[Y] \\ 
				&= E[XY] - E[X]E[Y]
			\end{align*}
			\item Suppose that $X,Y$ are independent. Then $E[XY] = E[X]E[Y]$. Hence 
			\begin{align*}
				Cov(X,Y) 
				&= E[XY] - E[X]E[Y] \\
				&= E[X]E[Y] - E[X]E[Y] \\
				&= 0
			\end{align*}
			\item Part (1) implies that 
			\begin{align*}
				Var(X) 
				&= Cov(X, X) \\
				&= E[X^2] -E[X]^2
			\end{align*}
			\item Let $a,b \in \R$. Then
			\begin{align*}
				Var(aX+b)
				&= E[(aX+b)^2] - E[aX+b]^2 \\
				&= E[a^2X^2 + 2abX +b^2] - (aE[X]+b)^2 \\
				&= a^2E[X^2]+2abE[X] + b^2 - (a^2E[X]^2 +2abE[X]+b^2) \\
				&= a^2(E[X^2]-E[X]^2) \\
				&= a^2Var(X)
			\end{align*} 
			\item We have that 
			\begin{align*}
				Var(X+Y) 
				&= E[(X+Y)^2] - E[X+Y]^2 \\
				&= E[X^2 +2XY + Y^2] -(E[X]+E[Y])^2 \\
				&= E[X^2] + 2E[XY] + E[Y^2] - (E[X]^2 + 2E[X]E[Y] + E[Y]^2) \\
				&= (E[X^2] - E[X]^2) + (E[Y^2]- E[Y]^2) + 2(E[XY] - E[X]E[Y]) \\
				&= Var(X) + Var(Y) + 2Cov(X,Y)
			\end{align*}
		\end{enumerate}
	\end{proof}
	
	\begin{defn}
		Let $(\Om, \MF, P)$ be a probability space and $X,Y \in L^2$. The \textbf{correlation of X and Y}, $Cor(X,Y)$, is defined to be $$Cor(X,Y) = \frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}}$$
	\end{defn}
	
	\begin{ex}
		
	\end{ex}
	
	\begin{ex}{Jensen's Inequality}
		Let $(\Om, \MF, P)$ be a probability space, $X \in L^1$ and $\phi:\R \rightarrow \R$. If $\phi$ is convex, then $$\phi(E[X]) \leq E[\phi(X)]$$
	\end{ex}
	
	\begin{proof}
		Put $x_0 = E[X]$. Since $\phi$ is convex, there exist $a,b \in \R$ such that $\phi(x_0) = ax_0+b$ and for each $x \in \R$, $\phi(x) \geq ax+b$. Then \begin{align*}
			E[\phi(X)] 
			&= \int\phi(X) dP \\
			&\geq \int[ aX+b ]dP \\
			&= a\int X dP +b \\
			&= aE[X] +b \\
			&= ax_0+b \\
			&= \phi(x_0) \\
			&= \phi(E[X])
		\end{align*}
	\end{proof}
	
	\begin{ex}{Markov's Inequality:}
		Let $(\Om, \MF, P)$ be a probability space and $X \in L^+$. Then for each $a \in (0,\infty)$, $$P(X \geq a) \leq \frac{E[X]}{a}$$
	\end{ex}
	
	\begin{proof}
		Let $a \in (0,\infty)$. Then $a \mathbf{1}_{\{X \geq a\}} \leq X \mathbf{1}_{\{X \geq a\}}$. Thus 
		\begin{align*}
			a P(X \geq a) 
			&= \int a \mathbf{1}_{\{X \geq a\}} dP\\
			&= \int X \mathbf{1}_{\{X \geq a\}} dP\\
			&\leq \int X dP \\
			&= E[X]
		\end{align*}
		Therefore $$P(X \geq a) \leq \frac{E[X]}{a}$$. 
	\end{proof}
	
	\begin{ex}{Chebychev's Inequality:}
		Let $(\Om, \MF, P)$ be a probability space and $X \in L^2$. Then for each $a \in (0, \infty)$, $$P(\vert X - E[X] \vert \geq a) \leq \frac{Var(X)}{a^2}$$
	\end{ex}
	
	\begin{proof}
		Let $a \in (0, \infty)$. Then 
		\begin{align*}
			P(\vert X - E[X] \vert \geq a) 
			&= P((X - E[X])^2 \geq a^2) \\
			&\leq \frac{E[(X - E[X])^2]}{a^2} \\
			&= \frac{Var(X)}{a^2}  
		\end{align*}
	\end{proof}
	
	\begin{ex}{Chernoff's Bound:}
		Let $(\Om, \MF, P)$ be a probability space and $X \in L^2$. Then for each $a, t \in (0, \infty)$, $$P( X \geq a) \leq e^{-ta}E[e^{tX}]$$
	\end{ex}
	
	\begin{proof}
		Let $a, t \in (0, \infty)$. Then 
		\begin{align*}
			P( X \geq a)
			&= P(tX \geq ta) \\
			&= P(e^{tX} \geq e^{ta}) \\
			& \leq e^{-ta}E[e^{tX}]
		\end{align*}
	\end{proof}
	
	\begin{ex}{Weak Law of Large Numbers:}
		Let $(\Om, \MF, P)$ be a probability space $(X_i)_{i\in \N} \subset L^2$. Suppose that $(X_i)_{i\in \N}$ are iid. Then $$\frac{1}{n}\sum_{i=1}^n X_i \conv{P} E[X_1]$$
	\end{ex}
	
	\begin{proof}
		Put $\mu = E[X_1]$ and $\sig^2 = Var(X_1)$. Then 
		\begin{align*}
			E[\frac{1}{n}\sum_{i=1}^n X_i] 
			&= \frac{1}{n} \sum_{i=1}^nE[X_i] \\
			&= \frac{1}{n} \sum_{i=1}^n \mu \\
			&= \mu
		\end{align*} and 
		\begin{align*}
			Var(\frac{1}{n}\sum_{i=1}^n X_i) 
			&= \frac{1}{n^2} Var(\sum_{i=1}^n X_i) \\
			&= \frac{1}{n^2} \sum_{i=1}^n Var(X_i) \\
			&= \frac{1}{n^2} \sum_{i=1}^n \sig^2 \\
			&= \frac{\sig^2}{n}
		\end{align*}
		Let $\ep >0$. Then 
		\begin{align*}
			P\bigg(\bigg \vert \frac{1}{n} \sum_{i=1}^n X_i - E[X_1] \bigg \vert \geq \ep \bigg) 
			& = P\bigg(\bigg\vert \frac{1}{n} \sum_{i=1}^n X_i - \mu \bigg\vert \geq \ep\bigg) \\
			& = P\bigg(\bigg\vert \frac{1}{n} \sum_{i=1}^n X_i - \E\bigg[\frac{1}{n}\sum_{i=1}^n X_i \bigg] \bigg\vert \geq \ep\bigg) \\ 
			&\leq \frac{Var(\frac{1}{n} \sum_{i=1}^n X_i)}{\ep^2} \\
			& =  \frac{\sig^2 / n}{\ep^2} \\
			&= \frac{\sig^2}{n\ep^2} \rightarrow 0
		\end{align*}
		
		So $$\frac{1}{n}\sum_{i=1}^n X_i \conv{P} E[X_1]$$
	\end{proof}
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	\newpage
	\subsection{Borel Cantelli Lemma} 
	
	\begin{ex}\textbf{Borel Cantelli Lemma:}
		Let $(\Om, \MF, P)$ be a probability space and $(A_n)_{n \in \N} \subset \MF$.
		\begin{enumerate}
			\item If $\sum\limits_{n \in \N}P(A_n) < \infty$, then $P(\limpn A_n) = 0$.
			\item If $(A_n)_{n \in \N}$ are independent and $\sum\limits_{n \in \N} P(A_n) = \infty$, then $P( \limpn A_n) = 1$  
		\end{enumerate}
	\end{ex}
	
	\begin{proof}\
		\begin{enumerate}
			\item Suppose that $\sum\limits_{n \in \N}P(A_n) < \infty$. Recall that $$\limsup\limits_{n \rightarrow \infty}A_n = \bigg \{\om \in \Om: \sum\limits_{n \in \N}1_{A_n}(\om) = \infty \bigg \}$$ Then \begin{align*}
				\infty 
				&> \sum_{n \in \N}P(A_n) \\
				&= \sum_{n \in \N} \int 1_{A_n}dP \\
				&= \int \sum_{n \in \N} 1_{A_n}dP \\
			\end{align*}
			Thus $\sum \limits_{n \in \N} 1_{A_n} < \infty$ a.e. and $P(\limpn A_n) = 0$.
			\item Suppose that $(A_n)_{n \in \N}$ are independent and $\sum\limits_{n \in \N} P(A_n) = \infty$.
		\end{enumerate}
	\end{proof}
	
	\begin{ex}
		Let $(\Om, \MF, P)$ be a probability space and $(X_n)_{n \in \N} \subset L^0$ and $X \in L^0$. 
		\begin{enumerate}
			\item If for each $\ep >0 $, $\sum\limits_{n \in \N} P(|X_n -X| \geq \ep) < \infty$, then $X_n \rightarrow X$ a.e.
			\item If $(X_n)_{n \in \N}$ are independent and there exists $\ep >0$ such that $\sum\limits_{n \in \N} P(|X_n -X| \geq \ep) = \infty$, then $X_n \not \rightarrow X$ a.e.
		\end{enumerate}
	\end{ex}
	
	\begin{proof}\
		\begin{enumerate}
			\item For $\ep>0$ and $n \in \N$, set $A_n(\ep) = \{\om \in \Om: |X_n(\om) - X(\om)| \geq \ep\}$. Suppose that for each $\ep >0 $, $\sum\limits_{n \in \N} P(|X_n - X| \geq \ep) < \infty$.  The Borel-Cantelli lemma implies that for each $m \in \N$,  $$P(\limpn A_n(1/m)) = 0$$
			 Let $\om \in \Om$. Then $X_n(\om) \not \rightarrow X(\om)$ iff $$\om \in \bigcup_{m \in \N} \limpn A_n(1/m)$$ 
		So 
		\begin{align*}
		P(X_n \not \rightarrow X)
		&= P\bigg( \bigcup_{m \in \N} \limpn A_n(1/m) \bigg) \\
		&\leq \sum_{m \in \N}P(\limpn A_n(1/m)) \\
		&= 0
\end{align*}		 
	Hence $X_n \rightarrow X$ a.e.
	\item 
		\end{enumerate}
	\end{proof}
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	\newpage
	\section{Weak Convergence of Measures}
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	\newpage
	\section{Conditional Expectation and Probability}
	\subsection{Conditional Expectation}
	
	\begin{ex}
	Let $(\Om, \MF, P)$ be a probability space, $\MG$ a sub $\sig$-alg of $\MF$ and $X \in L^1(\Om, \MF, P)$. Define $P_{\MG} = P|_{\MG}$ and $Q: \MG \rightarrow [0,\infty)$ by $Q(G) = \int_G X d P $. Then $Q$ is finite. and $Q \ll P_{\MG}$. 
	\end{ex}	
	
	\begin{proof}
	Since $X \in L^1$, for each $G \in \MG$, 
	\begin{align*}
	|Q(G)|
	&= \bigg| \int_G X dP \bigg| \\
	& \leq \int_G |X| dP \\
	& < \infty
	\end{align*}
	So $Q$ is finite. Let $G \in \MG$. Suppose that $P_{\MG}(G) = 0$. By definition then, $P(G) = 0$. So $Q(G) = 0$ and $Q \ll P_{\MG}$.
	\end{proof}
	
	\begin{defn}
	Let $(\Om, \MF, P)$ be a probability space, $\MG$ a sub $\sig$-alg of $\MF$ and $X, Y \in L^1(\Om, \MF, P)$. Then $Y $ is said to be a \textbf{conditional expectation of $X$ given $\MG$} if 
	\begin{enumerate}
	\item $Y$ is $\MG$-measurable
	\item for each $G \in \MG$, 
	$$\int_G Y dP = \int_G X dP$$ 
	\end{enumerate}	 
	To denote this, we write $Y = E[X|\MG]$
	\end{defn}		
	
	\begin{ex}
	Let $(\Om, \MF, P)$ be a probability space, $\MG$ a sub $\sig$-alg of $\MF$ and $X \in L^1(\Om, \MF, P)$. Define $Q$ and $P_{\MG}$ as in the previous exercise. Define $Y = dQ/dP_{\MG}$. Then $Y$ is a conditional expectation of $X$ given $\MG$.
	\end{ex}
	
	\begin{proof}
		By definition of the Radon-Nikodym derivative, $Y$ is $\MG$-measurable and by the Radon-Nikodym theorem, $X \in L^1(\Om, \MF, P)$ imples that $Y \in L^1(\Om, \MG, P_{\MG})$. An exercies in section $3.3$ of \cite{imi}, implies that for each $G \in \MG$ $$\int_G Y dP = \int_G X dP$$
	\end{proof}	
	
	
	
	\begin{ex}\textbf{(Doob–Dynkin Lemma)} \\
	Let $\Om$ be a nonempty set, $(\Om', \MF')$ a measurable space $X: \Om \rightarrow \Om'$ and $Z: \Om \rightarrow \R^n$. Suppose that $\Img X \in \MF'$. Then $Z$ is $\sig(X)$-$\MB(\R^n)$ measurable iff there exists $\phi:\Om' \rightarrow \R^n$ such that $\phi$ is $\MF'$-$\MB(\R^n)$ measurable and $Z = \phi \circ X$.
	\end{ex}
	
	\begin{proof}
	Suppose that there exists $\phi:\Om' \rightarrow \R^n$ such that $\phi$ is $\MF'$-$\MB(\R^n)$ measurable and $Z = Y \circ X$. Since $X$ is $\sig(X)$-$\MF'$ measurable, $Z = \phi \circ X$ is $\sig(X)$-$\MB(\R^n)$ measurable.
	Conversely, suppose that $Z$ is $\sig(X)$-$\MB(\R^n)$ measurable. For now, suppose that $n =1$ and $Z$ is simple. Then there exists a partition $(A_i)_{i=1}^k \subset \sig(X)$ of $\Om$ and $(a_i)_{i =1}^k \in \R$ such that 
	$$Z = \sum_{i=1}^k a_i 1_{A_i}$$ 
	By definition of $\sig(X)$, there exists a partition $(B_i)_{i=1}^k \subset \MF'$ such that for each $i = 1, \cdots, k$, $A_i = X^{-1}(B_i)$. Define 
	$$\phi = \sum_{i=1}^k a_i 1_{B_i}$$ 
	Since $(B_i)_{i=1}^k$ partitions $\Om'$, 
	\begin{align*}
	\phi \circ X 
	&= \sum_{i=1}^k a_i 1_{X^{-1}(B_i)} \\
	&= \sum_{i=1}^k a_i 1_{A_i} \\
	&= Z
	\end{align*}
	More generally, if $Z$ is $\sig(X)$-$\MB(\R)$ measurable, there exits a sequence $(Z_j)_{j \in \N}$ of simple $\sig(X)$-$\MB(\R)$ measurable functions such that for each $j \in N$ $0 \leq |Z_j| \leq |Z_{j+1}| \leq |Z| $ and $Z_j \convt{p.w.} Z$. \vspace{0cm} \\ Therefore, as shown previously, there exists a sequence $(\phi_j)_{j \in \N}$ of $\MF'$-$\MB(\R)$-measurable simple functions such that for each $j \in \N$, $Z_j = \phi_j \circ X$. Let $b \in \Img X$. Then there exists $a \in \Om$ such that $X(a) = b$. So 
	\begin{align*}
	\phi_j(b) 
	&= \phi_j \circ X(a) \\
	&= Z_j(a) \\
	& \rightarrow Z(a) 
	\end{align*}
	Thus we may define $\phi:\Om' \rightarrow \R$ by $$\phi = \lim_{j \rightarrow \infty} \phi_j 1_{\Img X}$$
	Then $\phi$ is measurable since $\Img X \in \MF'$ and $Z = \phi \circ X$. For $n \geq 1$, we may write $Z = (Z_1, \cdots, Z_n)$ where for each $i =1, \cdots, n$, $Z_i$ is $\sig(X)$-$\MB(\R)$ measurable and apply the result from above to obtain $ \phi = (\phi_1, \cdots, \phi_n)$ where for each $i =1, \cdots, n$, $\phi_i$ is $\MF'$-$\MB(\R)$ measurable and $Z_i = \phi_i \circ X$. Then $Z = \phi \circ X$.
	\end{proof}		
	
	\subsection{Conditional Probability}
	
	\begin{defn}
	Let $(A, \MA)$ be a measurable space, $(B, \MB, P_Y)$ a probability space and $Q:B \times \MA \rightarrow [0,1]$. Then $Q$ is said to be a \textbf{stochastic transition kernel from $(B, \MB, P)$ to $(A, \MA)$} if 
	\begin{enumerate}
	\item for each $E \in \MA$, $Q(\cdot, E)$ is $\MB$-measurable
	\item for $P$-a.e. $b \in B$, $Q(b, \cdot)$ is a probability measure on $(A, \MA)$
	\end{enumerate}
	\end{defn}	
	
	\begin{defn}
	Let $(\Om, \MF, P)$ be a probability space, $X,Y \in L_n^0(\Om, \MF, P)$ and $Q: \R^n  \times \MF \rightarrow [0,1]$. Then $Q$ is said to be a \textbf{conditional probability distribution of $X$ given $Y$} if 
	\begin{enumerate}
	\item $Q$ is a stochastic transition kernel from $(\R^n, \MB(\R^n), P_Y)$ to $(\R^n, \MB(\R^n))$
	\item for each $A,B \in \MF$, $$\int_B Q(y, A) dP_Y(y) = P(X \in A, Y \in B)$$ 
	\end{enumerate}
	\end{defn}
	
	\begin{note}
	It is helpful to connect this notion of conditional probability with the elementary one by writing $Q(y, A) = P(X \in A| Y = y)$. If $P_Y \ll \mu$, then property $(2)$ in the definition becomes 
	\begin{align*}
	P(X \in A, Y \in B)  
	&=  \int_B Q(y, A) dP_Y(y) \\
	&= \int_B P(X \in A|Y=y) f_Y(y)d \mu(y) \\
\end{align*}	 
	as in a first course on probability.
	\end{note}	
	
	\begin{ex}
	Let $(\Om, \MF, P)$ be a probability space, $X,Y \in L_n^0$ and $Q: \R^n \times \MF \rightarrow [0,1]$. Suppose that for each $A \in \MF$, $Q(\cdot, A)$ is $\MB(\R^n)$-measurable, for $P_Y$-a.e. $y \in \R^n$, $P_{X|Y}(y, \cdot)$ is a probability measure on $(\Om, \MF)$ and $Q(Y, A) = P(X \in A|Y)$ a.e. Then $Q$ is a conditional probability of $X$ given $Y$.
	\end{ex}	
	
	\begin{proof}
	By assumption, for each $A \in \MF$, $Q(\cdot, A)$ is $\MB(\R^n)$-measurable and for $P_Y$-a.e. $y \in \R^n$, $Q(y, \cdot)$ is a probability measure on $(\Om, \MF)$. Let $A,B \in \MF$. Then
	\begin{align*}
	\int_B Q(y, A) dP_Y(y)
	&= \int_{Y^{-1}(B)} Q(Y(\om), A) dP(\om) \\
	&= \int_{Y^{-1}(B)}  P(X \in A|Y) dP \\
	&= \int_{Y^{-1}(B)}  E[1_{X^{-1}(A)}|Y] dP \\
	&= \int_{Y^{-1}(B)}  1_{X^{-1}(A)} dP \\
	&= \int 1_{X^{-1}(A)}  1_{Y^{-1}(B)} dP \\
	&= \int 1_{X^{-1}(A) \cap Y^{-1}(B)} dP \\
	&= P(X \in A, Y \in B)
	\end{align*}
	So $Q$ is a conditional probability distribution of $X$ given $Y$.
	\end{proof}
	
	\begin{defn}
	Let $(\Om, \MF, P)$ be a probability space, $X,Y \in L_n^0$ and $\mu$ a $\sig$-finite measure on $(\R^n, \MB(\R^n))$. Suppose that $P_X, P_Y \ll \mu$. Then $P_{X,Y} \ll \mu^2$. Let $f_X = dP_X/d \mu$, $f_Y = dP_Y/d \mu$ and $f_{X,Y} = dP_{X,Y}/d \mu^2$. Define $f_{X|Y}: \R^n \times \R^n$ by 
	\[
	f_{X|Y}(x,y) = 
	\begin{cases}
	\frac{f_{X,Y}(x,y)}{f_Y(y)},& y \in \supp Y \\
	0, &y \not \in \supp Y 
	\end{cases}
	\] 
	Then $f_{X|Y}$ is called the \textbf{conditional probability density of $X$ given $Y$}.
	\end{defn}	
	
	\begin{ex}
	Let $(\Om, \MF, P)$ be a probability space, $X,Y \in L_n^0$ and $\mu$ a $\sig$-finite measure on $(\R^n, \MB(\R^n))$. Suppose that $P_X, P_Y \ll \mu$. Define $Q: \R^n \times \MF \rightarrow [0,1]$ by 
	$$Q(y, A) = \int_A f_{X|Y}(x,y) d\mu(x)$$ 
	Then $Q$ is a conditional probability distribution of $X$ given $Y$.
	\end{ex}	
	
	\begin{proof}
	By the Fubini-Tonelli Theorem, for each $A \in \MF$, $Q(\cdot, A)$ is $\MB(\R^n)$-measurable and for $P_Y$-a.e. $y \in \R^n$, $Q(y, \cdot)$ is a probability measure on $(\Om, \MF)$. Let $A, B \in \MF$. Then 
	\begin{align*}
	\int_B Q(y, A) dP_Y(y)
	&= \int_B \bigg[ \int_A f_{X|Y}(x,y) d\mu(x) \bigg] dP_Y(y) \\
	&= \int_{B \cap \supp Y} \bigg[ \int_{A \cap \supp Y} \frac{f_{X,Y}(x,y)}{f_Y(y)} d\mu(x) f_Y(y) \bigg] d \mu(y) \\
	&= \int_{B \cap \supp Y}  \bigg[\int_A f_{X,Y}(x,y) d\mu(x) \bigg] d \mu(y) \\
	&= P(X \in A, Y \in B \cap \supp Y) \\
	&= P(X \in A, Y \in B) \\	
	\end{align*}
	\end{proof}
	
	\begin{thm}
	Let $(\Om, \MF, P)$ be a probability space, $X,Y \in L_n^1(\Om, \MF, P)$. Suppose that $\Img X \in \MB(\R^n)$. Then there exists a conditional probability distribution of $Y$ given $X$. 
	\end{thm}
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	\newpage
	\section{Markov Chains}	

	
	\begin{defn}
	Let $(\Om, \MF, P)$ be a probability space and $(X_n)_{n \in \N_0} \in L_n^0$. Then $(X_n)_{n \in \N_0}$ is said to be a \textbf{homogeneous Markov chain} if for each $A \in \MF$ and $n \in \N$, $P(X_n \in A| X_1, \cdots, X_{n-1}) = P(X_1 \in A| X_{0})$ a.e. 
	\end{defn}
	
	
	
	
	
	
	
	
	
	
	
	
	\newpage
	\section{Stochastic Integration}
	\begin{ex}
	Let $(\Om, \MF, P)$ be a probability space, $X$ a set $\MA_0$ an algebra, $\mu_0:\MA_0 \rightarrow \C$ and $B:\MA_0 \rightarrow L^2(\Om, \MF, P)$. 
	Suppose that 
	\begin{enumerate}
	\item $B(\varnothing) = 0$
	\item for each $E, F \in \MA_0$, if $E \cap F = \varnothing$, then $B(E \cup F) = B(E) + B(F)$
	\item $\E [B(E)B(F)^*] = \mu_0(E \cap F)$
	\end{enumerate}
	Then 
	\begin{enumerate}
	\item for each $E \in \MA_0$, $\mu_0(E) = \E[|B(E)|^2]$.
	\item for each $E \in \MA_0$, $0 \leq \mu_0(E) < \infty$ 
	\item for each $E, F \in \MA_0$, if $E \cap F = \varnothing$, then $\mu_0(E \cup F) = \mu_0(E) + \mu_0(F)$
	\end{enumerate}
	\end{ex}
	
	\begin{proof}\
	\begin{enumerate}
	\item Clear 
	\item Clear
	\item Let $E, F \in \MA_0$. Suppose that $E \cap F = \varnothing$. Then 
	\begin{align*}
	\E[B(E)B(F)^*] 
	&= \mu_0(E \cap F) \\
	&= \mu_0(\varnothing) \\
	&= \E[|B(\varnothing)|^2] \\
	&= \E[0] \\
	&= 0
	\end{align*}
	This implies that 
	\begin{align*}
	\mu_0(E \cup F) 
	&= \E[|B(E \cup F)|^2] \\
	&= \E[|B(E) + B(F)|^2] \\
	&= \E[|B(E)|^2] + \E[|B(F)|^2] + 2 \Re \E[B(E)B(F)^*] \\
	&= \mu_0(E) + \mu_0(F) +0  \\
	&=  \mu_0(E) + \mu_0(F) 
	\end{align*}
	\end{enumerate}
	\end{proof}
	
	\begin{defn}
	Let $(\Om, \MF, P)$ be a probability space, $X$ a set $\MA_0$ an algebra, $\mu_0:\MA_0 \rightarrow \Rg$ a premeasure and $B:\MA_0 \rightarrow L^2(\Om, \MF, P)$. 
	Suppose that 
	\begin{enumerate}
	\item $B(\varnothing) = 0$
	\item for each $E, F \in \MA_0$, if $E \cap F = \varnothing$, then $B(E \cup F) = B(E) + B(F)$
	\item $\E [B(E)B(F)^*] = \mu_0(E \cap F)$
	\end{enumerate}
	Then $B$ is said to be a \textbf{stochastic premeasure with sturcture $\mu_0$}
	
	\end{defn}
	
\end{document}